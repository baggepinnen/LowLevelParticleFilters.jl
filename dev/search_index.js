var documenterSearchIndex = {"docs":
[{"location":"distributions/#Performance-tips","page":"Performance tips","title":"Performance tips","text":"","category":"section"},{"location":"distributions/","page":"Performance tips","title":"Performance tips","text":"Use of StaticArrays.jl is recommended for optimal performance when the state dimension is small, e.g., less than about 10-15 for Kalman filters and less than about 100 for particle filters. In the section Parameter optimization we demonstrate one workflow that makes use of StaticArrays everywhere it is needed for an UnscentedKalmanFilter in order to get a completely allocation free filter. The following arrays must be static for this to hold","category":"page"},{"location":"distributions/","page":"Performance tips","title":"Performance tips","text":"The initial state distribution (the vector and matrix passed to d0 = MvNormal(μ, Σ) for Kalman filters). If you are performing parameter optimization with gradients derived using ForwardDiff.jl, these must further have the correct element type. How to achieve this is demonstrated in the liked example above.\nInputs u measured outputs y.\nIn case of Kalman filters, the dynamic model matrices A, B, C, D and the covariance matrices R1, R2.\nThe dynamics functions for UnscentedKalmanFilter and particle filters must further return static arrays when passed static arrays as inputs.","category":"page"},{"location":"distributions/#Analysis-using-JET","page":"Performance tips","title":"Analysis using JET","text":"","category":"section"},{"location":"distributions/","page":"Performance tips","title":"Performance tips","text":"All flavors of Kalman filters are analyzed for potential runtime dispatch using JET.jl. This analysis is performed in the tests and generally requires a completely static filter using static arrays internally. See the tests for an example of how to set a filter up this way.","category":"page"},{"location":"distributions/#High-performance-Distributions","page":"Performance tips","title":"High performance Distributions","text":"","category":"section"},{"location":"distributions/","page":"Performance tips","title":"Performance tips","text":"When using LowLevelParticleFilters, a number of methods related to distributions are defined for static arrays, making logpdf etc. faster. We also provide a new kind of distribution: TupleProduct <: MultivariateDistribution that behaves similarly to the Product distribution. The TupleProduct however stores the individual distributions in a tuple, has compile-time known length and supports Mixed <: ValueSupport, meaning that it can be a product of both Continuous and Discrete dimensions, something not supported by the standard Product. Example","category":"page"},{"location":"distributions/","page":"Performance tips","title":"Performance tips","text":"using BenchmarkTools, LowLevelParticleFilters, Distributions, StaticArrays\ndt = TupleProduct((Normal(0,2), Normal(0,2), Binomial())) # Mixed value support","category":"page"},{"location":"distributions/","page":"Performance tips","title":"Performance tips","text":"A small benchmark","category":"page"},{"location":"distributions/","page":"Performance tips","title":"Performance tips","text":"sv = @SVector randn(2)\nd = Distributions.Product([Normal(0,2), Normal(0,2)])\ndt = TupleProduct((Normal(0,2), Normal(0,2)))\ndm = MvNormal(2, 2)\n@btime logpdf($d,$(Vector(sv)))  # 19.536 ns (0 allocations: 0 bytes)\n@btime logpdf($dt,$(Vector(sv))) # 13.742 ns (0 allocations: 0 bytes)\n@btime logpdf($dm,$(Vector(sv))) # 11.392 ns (0 allocations: 0 bytes)","category":"page"},{"location":"distributions/","page":"Performance tips","title":"Performance tips","text":"@btime logpdf($d,$sv)  # 13.964 ns (0 allocations: 0 bytes)\n@btime logpdf($dt,$sv) # 12.817 ns (0 allocations: 0 bytes)\n@btime logpdf($dm,$sv) # 8.383  ns (0 allocations: 0 bytes)","category":"page"},{"location":"distributions/","page":"Performance tips","title":"Performance tips","text":"Without loading LowLevelParticleFilters, the timing for the native distributions are the following","category":"page"},{"location":"distributions/","page":"Performance tips","title":"Performance tips","text":"@btime logpdf($d,$sv)  # 18.040 ns (0 allocations: 0 bytes)\n@btime logpdf($dm,$sv) # 9.938  ns (0 allocations: 0 bytes)","category":"page"},{"location":"sample_rate/#Influence-of-sample-rate-on-performance","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"","category":"section"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"Naturally, if we sample more often, we obtain more information about the system and can thus expect better performance. Frequent sampling allows for an averaging effect that can mitigate the influence of measurement noise. Sampling \"frequently enough\" is also important in order to rely on theoretical concepts such as observability when analyzing what modes of the system can be accurately recovered. To understand this, consider an extreme case: a particle subject to a random force where we can measure the position of the particle (this is a double integrator system). If we measure the position of the particle often, we can infer both the position and the velocity of the particle, the system is observable. However, if we measure the position rather infrequently, we can't say much about the velocity of the particle since the noisy force driving the particle will have had a large influence on the velocity, which isn't directly measured, since the last measurement of the position. When analyzing observability traditionally, we may compute","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"The observability matrix. This can tell us the theoretical observable and unobservable subspaces of the system. One may take measurement noise into account by scaling the outputs to have equal variance, but one can not take driving noise properties into account.\nThe observability Gramian. This gives us a measure of how well we can estimate modes of the system in a balanced realization from the available measurements, but there is once again no way to take the driving noise into account.","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"To estimate the practical observability of the system we may instead consider an analysis of the stationary error covariance of a state estimator. For a linear-Gaussian system observed with a Kalman filter, the stationary Kalman gain and error covariance is obtained by solving an algebraic Riccati equation:","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"beginalign\nx^+ = Ax + w\ny = Cx + e \n\nx^+ = Ax + K(y - Cx)  qquad textestimator\n\nε = x - x qquad textprediction error\nε^+ = Ax + w - big(Ax + K(y - Cx) big) qquad textprediction error dynamics\nε^+ = Ax + w - big(Ax + K(Cx + e - Cx) big)\nε^+ = (A - KC)ε + w - Ke \nE(we^T) = 0 Longrightarrow \nE(w - Ke)(w - Ke)^T = R_1 + K R_2 K^T\nendalign","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"The stationary covariance of the prediction error ε(t+1t), R_(t+1t) = E_(ε(t+1t)ε(t+1t)^T), is automatically computed by the solver of the algebraic Riccati equation that computes the stationary Kalman gain K.","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"By incorporating the measurement, we form a filtering estimate ε(tt) and in doing so, reduce the covariance of the prediction error according to R_(tt) = (I - KC)R_(tt-1). ","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"note: Note\nDue to the exact formulation of K returned by the Riccati solver in MatrixEquations.jl, we must either use A^-1K or compute K = RC^T (R_2 + C R C^T)^-1 ourselves. MatrixEquations.ared solves the Riccati equation corresponding to the filter form, but returns the K matrix for the prediction form. ","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"using ControlSystemsBase, LinearAlgebra\nimport ControlSystemsBase.MatrixEquations\n\nfunction kalman_are(sys::AbstractStateSpace{<:Discrete}, R1, R2)\n    A,B,C,D = ssdata(sys)\n    R∞, p, K, args... = MatrixEquations.ared(A', C', R2, R1)\n    K', R∞, args...\nend","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"To perform an analysis of the performance as a function of the sample rate, we will assume that we have a continuous-time LTI system with a continuous-time Gaussian noise process driving the system. This will allow us to discretize both the system dynamics and noise process at varying sample rates and compute the stationary Kalman filter and associated stationary covariance matrix (see Discretization for more details). We assume that the measurement noise is a discrete-time noise process with a fixed covariance, representing a scenario where the sensor equipment is predetermined but the sample rate is not. ","category":"page"},{"location":"sample_rate/#Example-1:-Double-integrator","page":"Influence of sample rate on performance","title":"Example 1: Double integrator","text":"","category":"section"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"A double integrator can be thought of as a particle subject to a force, the continuous-time dynamics are ddot x = w.","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"using ControlSystemsBase, Plots, Test\nsysc = ss([0 1; 0 0], [0; 1], [1 0], 0) # Continuous-time double integrator\nR1c = [0 0; 0 1]                        # Continuous-time process noise covariance\nR2  = [1;;]                             # Measurement noise covariance\n\nTs    = 1                               # Sample interval\nsysd  = c2d(sysc, Ts)                   # Discretize the system\nR1d   = c2d(sysc, R1c, Ts)              # Discretize the process noise covariance\nK, R∞ = kalman_are(sysd, R1d, R2)       # Compute the stationary Kalman gain and covariance\nR∞","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"Does the computed stationary covariance matrix match the expected covariance matrix we derived above? ","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"A,B,C,D = ssdata(sysd)\n@test lyap(Discrete, A-K*C, R1d + K*R2*K') ≈ R∞","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"For this system, we expect the stationary filtering covariance R_(tt) to go to zero for small T_s since the system is observable. For large T_s, we expect the variance of the position estimate to approach the variance of the measurement noise, i.e., we don't expect the model to be of any use if it's forced to predict for too long. The variance of the velocity estimate is expected to go to infinity since the disturbance force has increasingly more time to affect the velocity and we cannot measure this. Let's investigate:","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"Tss = exp10.(LinRange(-3, 3, 30))\nR∞s = map(Tss) do Ts\n    sysd    = c2d(sysc, Ts)\n    A,B,C,D = ssdata(sysd)\n    R1d     = c2d(sysc, R1c, Ts)\n    AK, R∞  = kalman_are(sysd, R1d, R2)\n\n    # diag((I-A\\AK*C)*R∞) # This also works\n\n    K = (R∞*C')/(R2 + C*R∞*C')\n    diag((I-K*C)*R∞)\nend\nplot(Tss, reduce(hcat, R∞s)', label=[\"\\$σ^2 p\\$\" \"\\$σ^2 v\\$\"], xlabel=\"Sample interval [s]\", ylabel=\"Stationary filtering variance\", title=\"Double integrator\", xscale=:log10, yscale=:log10)\nhline!(R2, label=\"\\$R_2\\$\", linestyle=:dash, legend=:bottomright)","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"The plot confirms our expectations. Note: this plot shows the filtering covariance, the prediction-error covariance R_(t+1t) would in this case go to infinity for both state variables.","category":"page"},{"location":"sample_rate/#Example-2:-Double-integrator-with-friction","page":"Influence of sample rate on performance","title":"Example 2: Double integrator with friction","text":"","category":"section"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"If we take the same system as above, but introduce some friction in the system, we expect similar behavior for small sample intervals, but for large sample intervals we expect the stationary variance of the velocity to converge to a finite value due to the dissipation of energy in the system:","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"sysc = ss([0 1; 0 -0.02], [0; 1], [1 0], 0) # Continuous-time double integrator with friction\n\nR∞s = map(Tss) do Ts\n    sysd    = c2d(sysc, Ts)\n    A,B,C,D = ssdata(sysd)\n    R1d     = c2d(sysc, R1c, Ts)\n    AK, R∞  = kalman_are(sysd, R1d, R2)\n    K       = (R∞*C')/(R2 + C*R∞*C')\n    diag((I-K*C)*R∞)\nend\nplot(Tss, reduce(hcat, R∞s)', label=[\"\\$σ^2 p\\$\" \"\\$σ^2 v\\$\"], xlabel=\"Sample interval [s]\", ylabel=\"Stationary filtering variance\", title=\"Double integrator with friction\", xscale=:log10, yscale=:log10)\nhline!(R2, label=\"\\$R_2\\$\", linestyle=:dash, legend=:bottomright)","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"Nice, it did.","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"If we instead look at the prediction-error covariance, we see the opposite behavior","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"R∞s = map(Tss) do Ts\n    sysd   = c2d(sysc, Ts)\n    R1d    = c2d(sysc, R1c, Ts)\n    AK, R∞ = kalman_are(sysd, R1d, R2)\n    diag(R∞)\nend\nplot(Tss, reduce(hcat, R∞s)', label=[\"\\$σ^2 p\\$\" \"\\$σ^2 v\\$\"], xlabel=\"Sample interval [s]\", ylabel=\"Stationary filtering variance\", title=\"Double integrator with friction\", xscale=:log10, yscale=:log10)\nhline!(R2, label=\"\\$R_2\\$\", linestyle=:dash, legend=:bottomright)","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"in this case, the variance of the position prediction goes to infinity for large T_s since the position dynamics still contain a pure integrator. The velocity prediction variance still converges to a finite value, the same finite value as the filtering variance, which also happens to be the same value as we get by computing the stationary covariance of the noise filtered through the system without any Kalman filter. This indicates that the estimator is completely useless for large sample intervals and we can't predict the velocity any better than its long-term average:","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"velocity_dynamics = ss(-0.02, 1, 1, 0)\nR∞s[end][end] ≈ lyap(velocity_dynamics, [1;;])[]","category":"page"},{"location":"sample_rate/#Convergence-of-time-varying-Kalman-filter","page":"Influence of sample rate on performance","title":"Convergence of time-varying Kalman filter","text":"","category":"section"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"When the dynamics is time invariant and the noise process is stationary, the time-varying Kalman filter converges to the stationary Kalman filter irrespective of what the inputs and outputs are. We can confirm this by running a simulation of the time-varying Kalman filter and checking that the stationary covariance converges to the stationary covariance we computed above. ","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"using LowLevelParticleFilters\nTs    = 1\nsysd  = c2d(sysc, Ts)\nR1d   = c2d(sysc, R1c, Ts)\n_, R∞ = kalman_are(sysd, R1d, R2)\nkf    = KalmanFilter(sysd, R1d, R2) # A Kalman filter can be constructed from a discrete-time system statespace model\nfor i = 1:1000 # Perform 1000 prediction and measurement updates to let the filter converge\n    update!(kf, [0], [0])\nend\n@test kf.R ≈ R∞ # The Kalman filter prediction error covariance converges to the stationary covariance","category":"page"},{"location":"sample_rate/","page":"Influence of sample rate on performance","title":"Influence of sample rate on performance","text":"(; K) = correct!(kf, [0], [0])   # Perform a measurement update\n@test K ≈ (R∞*C')/(R2 + C*R∞*C') # The time varying Kalman gain converges to the stationary Kalman gain (direct form)\n@test kf.R ≈ (I-K*C)*R∞          # The time varying Kalman filter filtering error covariance (after measurement update) converges to the stationary covariance","category":"page"},{"location":"parameter_estimation/#Parameter-Estimation","page":"Parameter estimation","title":"Parameter Estimation","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"State estimation is an integral part of many parameter-estimation methods. Below, we will illustrate several different methods of performing parameter estimation. We can roughly divide the methods into two camps","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"Methods that optimize prediction error or likelihood by tweaking model parameters.\nMethods that add the parameters to be estimated as state variables in the model and estimate them using standard state estimation. ","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"From the first camp, we provide som basic functionality for maximum-likelihood estimation and MAP estimation, described below. An example of (2), joint state and parameter estimation, is provided in Joint state and parameter estimation.","category":"page"},{"location":"parameter_estimation/#Maximum-likelihood-estimation","page":"Parameter estimation","title":"Maximum-likelihood estimation","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"Filters calculate the likelihood and prediction errors while performing filtering, this can be used to perform maximum likelihood estimation or prediction-error minimization. One can estimate all kinds of parameters using this method, in the example below, we will estimate the noise covariance. We may for example plot likelihood as function of the variance of the dynamics noise like this:","category":"page"},{"location":"parameter_estimation/#Generate-data-by-simulation","page":"Parameter estimation","title":"Generate data by simulation","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"This simulates the same linear system as on the index page of the documentation","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"using LowLevelParticleFilters, LinearAlgebra, StaticArrays, Distributions, Plots\nnx = 2   # Dimension of state\nnu = 2   # Dimension of input\nny = 2   # Dimension of measurements\nN = 2000 # Number of particles\n\nconst dg = MvNormal(ny,1.0)          # Measurement noise Distribution\nconst df = MvNormal(nx,1.0)          # Dynamics noise Distribution\nconst d0 = MvNormal(@SVector(randn(nx)),2.0)   # Initial state Distribution\n\nconst A = SA[1 0.1; 0 1]\nconst B = @SMatrix [0.0 0.1; 1 0.1]\nconst C = @SMatrix [1.0 0; 0 1]\n\ndynamics(x,u,p,t) = A*x .+ B*u \nmeasurement(x,u,p,t) = C*x\nvecvec_to_mat(x) = copy(reduce(hcat, x)') # Helper function\npf = ParticleFilter(N, dynamics, measurement, df, dg, d0)\nxs,u,y = simulate(pf,300,df)","category":"page"},{"location":"parameter_estimation/#Compute-likelihood-for-various-values-of-the-parameters","page":"Parameter estimation","title":"Compute likelihood for various values of the parameters","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"Since this example looks for a single parameter only, we can plot the likelihood as a function of this parameter. If we had been looking for more than 2 parameters, we typically use an optimizer instead (see further below).","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"p = nothing\nsvec = exp10.(LinRange(-0.8, 1.2, 60))\nllspf = map(svec) do s\n    df = MvNormal(nx,s)\n    pfs = ParticleFilter(N, dynamics, measurement, df, dg, d0)\n    loglik(pfs, u, y, p)\nend\nllspfaux = map(svec) do s\n    df = MvNormal(nx,s)\n    pfs = AuxiliaryParticleFilter(N, dynamics, measurement, df, dg, d0)\n    loglik(pfs, u, y, p)\nend\nplot( svec, llspf,\n    xscale = :log10,\n    title = \"Log-likelihood\",\n    xlabel = \"Dynamics noise standard deviation\",\n    lab = \"PF\",\n)\nplot!(svec, llspfaux, yscale=:identity, xscale=:log10, lab=\"AUX PF\", c=:green)\nvline!([svec[findmax(llspf)[2]]], l=(:dash,:blue), primary=false)","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"the correct value for the simulated data is 1 (the simulated system is the same as on the front page of the docs).","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"We can do the same with a Kalman filter, shown below. When using Kalman-type filters, one may also provide a known state sequence if one is available, such as when the data is obtained from a simulation or in an instrumented lab setting. If the state sequence is provided, state-prediction errors are used for log-likelihood estimation instead of output-prediction errors.","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"eye(n) = SMatrix{n,n}(1.0I(n))\nllskf = map(svec) do s\n    kfs = KalmanFilter(A, B, C, 0, s^2*eye(nx), eye(ny), d0)\n    loglik(kfs, u, y, p)\nend\nllskfx = map(svec) do s # Kalman filter with known state sequence, possible when data is simulated\n    kfs = KalmanFilter(A, B, C, 0, s^2*eye(nx), eye(ny), d0)\n    loglik(kfs, u, y, xs, p)\nend\nplot!(svec, llskf, yscale=:identity, xscale=:log10, lab=\"Kalman\", c=:red)\nvline!([svec[findmax(llskf)[2]]], l=(:dash,:red), primary=false)\nplot!(svec, llskfx, yscale=:identity, xscale=:log10, lab=\"Kalman with known state sequence\", c=:purple)\nvline!([svec[findmax(llskfx)[2]]], l=(:dash,:purple), primary=false)","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"the result can be quite noisy due to the stochastic nature of particle filtering. The particle filter likelihood agrees with the Kalman-filter estimate, which is optimal for the linear example system we are simulating here, apart for when the noise variance is small. Due to particle depletion, particle filters often struggle when dynamics-noise is too small. This problem is mitigated by using a greater number of particles, or simply by not using a too small covariance.","category":"page"},{"location":"parameter_estimation/#MAP-estimation","page":"Parameter estimation","title":"MAP estimation","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"Maximum a posteriori estimation (MAP) is similar to maximum likelihood (ML), but includes also prior knowledge of the distribution of the parameters in a way that is similar to parameter regularization. In this example, we will estimate the variance of the noises in the dynamics and the measurement functions.","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"To solve a MAP estimation problem, we need to define a function that takes a parameter vector and returns a filter, the parameters are used to construct the covariance matrices:","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"filter_from_parameters(θ, pf = nothing) = KalmanFilter(A, B, C, 0, exp(θ[1])^2*eye(nx), exp(θ[2])^2*eye(ny), d0) # Works with particle filters as well\nnothing # hide","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"The call to exp on the parameters is so that we can define log-normal priors","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"priors = [Normal(0,2),Normal(0,2)]","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"Now we call the function log_likelihood_fun that returns a function to be minimized","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"ll = log_likelihood_fun(filter_from_parameters, priors, u, y, p)\nnothing # hide","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"Since this is once again a low-dimensional problem, we can plot the LL on a 2d-grid","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"function meshgrid(a,b)\n    grid_a = [i for i in a, j in b]\n    grid_b = [j for i in a, j in b]\n    grid_a, grid_b\nend\nNv       = 20\nv        = LinRange(-0.7,1,Nv)\nllxy     = (x,y) -> ll([x;y])\nVGx, VGy = meshgrid(v,v)\nVGz      = llxy.(VGx, VGy)\nheatmap(\n    VGz,\n    xticks = (1:Nv, round.(v, digits = 2)),\n    yticks = (1:Nv, round.(v, digits = 2)),\n    xlabel = \"sigma v\",\n    ylabel = \"sigma w\",\n) # Yes, labels are reversed","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"For higher-dimensional problems, we may estimate the parameters using an optimizer, e.g., Optim.jl.","category":"page"},{"location":"parameter_estimation/#Bayesian-inference-using-PMMH","page":"Parameter estimation","title":"Bayesian inference using PMMH","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"We proceed like we did for MAP above, but when calling the function metropolis, we will get the entire posterior distribution of the parameter vector, for the small cost of a massive increase in the amount of computations. metropolis runs the Metropolis Hastings algorithm, or more precisely if a particle filter is used, the \"Particle Marginal Metropolis Hastings\" (PMMH) algorithm. Here we use the Kalman filter simply to have the documentation build a bit faster, it can be quite heavy to run.","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"filter_from_parameters(θ, pf = nothing) = KalmanFilter(A, B, C, 0, exp(θ[1])^2*I(nx), exp(θ[2])^2*I(ny), d0) # Works with particle filters as well\nnothing # hide","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"The call to exp on the parameters is so that we can define log-normal priors","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"priors = [Normal(0,2),Normal(0,2)]\nll     = log_likelihood_fun(filter_from_parameters, priors, u, y, p)\nθ₀     = log.([1.0, 1.0]) # Starting point\nnothing # hide","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"We also need to define a function that suggests a new point from the \"proposal distribution\". This can be pretty much anything, but it has to be symmetric since I was lazy and simplified an equation.","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"draw   = θ -> θ .+ 0.05 .* randn.() # This function dictates how new proposal parameters are being generated. \nburnin = 200 # remove this many initial samples (\"burn-in period\")\n@info \"Starting Metropolis algorithm\"\n@time theta, lls = metropolis(ll, 2200, θ₀, draw) # Run PMMH for 2200  iterations\nthetam = reduce(hcat, theta)'[burnin+1:end,:] # Build a matrix of the output\nhistogram(exp.(thetam), layout=(3,1), lab=[\"R1\" \"R2\"]); plot!(lls[burnin+1:end], subplot=3, lab=\"log likelihood\") # Visualize","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"In this example, we initialize the MH algorithm on the correct value θ₀, in general, you'd see a period in the beginning where the likelihood (bottom plot) is much lower than during the rest of the sampling, this is the reason we remove a number of samples in the beginning, typically referred to as \"burn in\".","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"If you are lucky, you can run the above threaded as well. I tried my best to make particle filters thread safe with their own rngs etc., but your milage may vary. For threading to help, the dynamics must be non-allocating, e.g., by using StaticArrays etc.","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"@time thetalls = LowLevelParticleFilters.metropolis_threaded(burnin, ll, 2200, θ₀, draw, nthreads=2)\nhistogram(exp.(thetalls[:,1:2]), layout=3)\nplot!(thetalls[:,3], subplot=3)","category":"page"},{"location":"parameter_estimation/#Bayesian-inference-using-DynamicHMC.jl","page":"Parameter estimation","title":"Bayesian inference using  DynamicHMC.jl","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"The following snippet of code performs the same estimation as above, but uses the much more sophisticated HMC sampler in DynamicHMC.jl rather than the PMMH sampler above. This package requires the log-likelihood function to be wrapped in a custom struct that implements the LogDensityProblems interface, which is done below. We also indicate that we want to use ForwardDiff.jl to compute the gradients for fast sampling.","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"using DynamicHMC, LogDensityProblemsAD, ForwardDiff, LogDensityProblems, LinearAlgebra, Random\n\nstruct LogTargetDensity{F}\n    ll::F\n    dim::Int\nend\nLogDensityProblems.logdensity(p::LogTargetDensity, θ) = p.ll(θ)\nLogDensityProblems.dimension(p::LogTargetDensity) = p.dim\nLogDensityProblems.capabilities(::Type{LogTargetDensity}) = LogDensityProblems.LogDensityOrder{0}()\n\nfunction filter_from_parameters(θ, pf = nothing)\n    # It's important that the distribution of the initial state has the same\n    # element type as the parameters. DynamicHMC will use Dual numbers for differentiation,\n    # hence, we make sure that d0 has `eltype(d0) = eltype(θ)`\n    T = eltype(θ)\n    d0 = MvNormal(T.(d0.μ), T.(d0.Σ))\n    KalmanFilter(A, B, C, 0, exp(θ[1])^2*eye(nx), exp(θ[2])^2*eye(ny), d0) \nend\nll = log_likelihood_fun(filter_from_parameters, priors, u, y, p)\n\nD = length(θ₀)\nℓπ = LogTargetDensity(ll, D)\n∇P = ADgradient(:ForwardDiff, ℓπ)\n\nresults = mcmc_with_warmup(Random.default_rng(), ∇P, 3000)\nDynamicHMC.Diagnostics.summarize_tree_statistics(results.tree_statistics)\nlls = [ts.π for ts in results.tree_statistics]\n\nhistogram(exp.(results.posterior_matrix)', layout=(3,1), lab=[\"R1\" \"R2\"])\nplot!(lls, subplot=3, lab=\"log likelihood\") # Visualize","category":"page"},{"location":"parameter_estimation/#Joint-state-and-parameter-estimation","page":"Parameter estimation","title":"Joint state and parameter estimation","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"In this example, we'll show how to perform parameter estimation by treating a parameter as a state variable. This method can not only estimate constant parameters, but also time-varying parameters. The system we will consider is a quadruple tank, where two upper tanks feed into two lower tanks. The outlet for tank 1 can vary in size, simulating, e.g., that something partially blocks the outlet. We start by defining the dynamics on a form that changes the outlet area a_1 at time t=500:","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"using LowLevelParticleFilters\nusing SeeToDee\nusing Distributions\nusing StaticArrays\nusing Plots, LinearAlgebra\n\nfunction quadtank(h,u,p,t)\n    k1, k2, g = 1.6, 1.6, 9.81\n    A1 = A3 = A2 = A4 = 4.9\n    a1, a3, a2, a4 = 0.03, 0.03, 0.03, 0.03\n    γ1, γ2 = 0.2, 0.2\n\n    if t > 500\n        a1 *= 2 # Change the parameter at t = 500\n    end\n\n    ssqrt(x) = √(max(x, zero(x)) + 1e-3) # For numerical robustness at x = 0\n    \n    SA[\n        -a1/A1 * ssqrt(2g*h[1]) + a3/A1*ssqrt(2g*h[3]) +     γ1*k1/A1 * u[1]\n        -a2/A2 * ssqrt(2g*h[2]) + a4/A2*ssqrt(2g*h[4]) +     γ2*k2/A2 * u[2]\n        -a3/A3*ssqrt(2g*h[3])                          + (1-γ2)*k2/A3 * u[2]\n        -a4/A4*ssqrt(2g*h[4])                          + (1-γ1)*k1/A4 * u[1]\n    ]\nend\n\nnu = 2 # number of control inputs\nnx = 4 # number of state variables\nny = 2 # number of measured outputs\nTs = 1 # sample time\nnothing # hide","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"We then define a measurement function, we measure the levels of tanks 1 and 2, and discretize the continuous-time dynamics using a Runge-Kutta 4 integrator SeeToDee.Rk4:","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"measurement(x,u,p,t) = SA[x[1], x[2]]\ndiscrete_dynamics = SeeToDee.Rk4(quadtank, Ts)\nnothing # hide","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"We simulate the system using the rollout function and add some noise to the measurements. The inputs in this case are just square waves.","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"Tperiod = 200\nt = 0:Ts:1000\nu = vcat.(0.25 .* sign.(sin.(2pi/Tperiod .* t)) .+ 0.25)\nu = SVector{nu}.(vcat.(u,u))\nx0 = Float64[2,2,3,3]\nx = LowLevelParticleFilters.rollout(discrete_dynamics, x0, u)[1:end-1]\ny = measurement.(x, u, 0, 0)\ny = [y .+ 0.01.*randn.() for y in y]\n\nplot(\n    plot(reduce(hcat, x)', title=\"State\"),\n    plot(reduce(hcat, u)', title=\"Inputs\")\n)","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"To perform the joint state and parameter estimation, we define a version of the dynamics that contains an extra state, corresponding to the unknown or time varying parameter, in this case a_1. We do not have any apriori information about how this parameter changes, so we say that its derivative is 0 and it's thus only driven by noise:","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"function quadtank_paramest(h, u, p, t)\n    k1, k2, g = 1.6, 1.6, 9.81\n    A1 = A3 = A2 = A4 = 4.9\n    a3, a2, a4 = 0.03, 0.03, 0.03\n    γ1, γ2 = 0.2, 0.2\n\n    a1 = h[5] # the a1 parameter is a state\n\n    ssqrt(x) = √(max(x, zero(x)) + 1e-3) # For numerical robustness at x = 0\n    \n    SA[\n        -a1/A1 * ssqrt(2g*h[1]) + a3/A1*ssqrt(2g*h[3]) +     γ1*k1/A1 * u[1]\n        -a2/A2 * ssqrt(2g*h[2]) + a4/A2*ssqrt(2g*h[4]) +     γ2*k2/A2 * u[2]\n        -a3/A3*ssqrt(2g*h[3])                          + (1-γ2)*k2/A3 * u[2]\n        -a4/A4*ssqrt(2g*h[4])                          + (1-γ1)*k1/A4 * u[1]\n        0 # the state is only driven by noise\n    ]\nend\n\ndiscrete_dynamics_params = SeeToDee.Rk4(quadtank_paramest, Ts)\nnothing # hide","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"We then define a nonlinear state estimator, we will use the UnscentedKalmanFilter, and solve the filtering problem. We start by an initial state estimate x_0 that is slightly off for the parameter a_1","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"nx = 5\nnames = SignalNames(x = [\"h1\", \"h2\", \"h3\", \"h4\", \"a1\"], y = [\"h$i\" for i in 1:2], u = [\"p1\", \"p2\"], name=\"UKF\") # For nicer plot labels\n\nR1 = SMatrix{nx,nx}(Diagonal([0.1, 0.1, 0.1, 0.1, 0.0001])) # Use of StaticArrays is generally good for performance\nR2 = SMatrix{ny,ny}(Diagonal((1e-2)^2 * ones(ny)))\nx0 = SA[2, 2, 3, 3, 0.02] # The SA prefix makes the array static, which is good for performance\n\nkf = UnscentedKalmanFilter(discrete_dynamics_params, measurement, R1, R2, MvNormal(x0, R1); ny, nu, Ts, names)\n\nsol = forward_trajectory(kf, u, y)\nplot(sol, plotx=false, plotxt=true, plotu=false, ploty=true, legend=:bottomright)\nplot!([0,500,500,1000], [0.03, 0.03, 0.06, 0.06], l=(:dash, :black), sp=5, lab=\"True param\")","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"as we can see, the correct value of the parameter is quickly found (a_1), and it also adapts at t=500 when the parameter value changes. The speed with which the parameter adapts to changes is determined by the covariance matrix R_1, a higher value results in faster adaptation, but also higher sensitivity to noise. ","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"If adaptive parameter estimation is coupled with a model-based controller, we get an adaptive controller! Note: the state that corresponds to the estimated parameter is typically not controllable, a fact that may require some special care for some control methods.","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"We may ask ourselves, what's the difference between a parameter and a state variable if we can add parameters as state variables? Typically, parameters do not vary with time, and if they do, they vary significantly slower than the state variables. State variables also have dynamics associate with them, whereas we often have no idea about how the parameters vary other than that they vary slowly.","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"Abrupt changes to the dynamics like in the example above can happen in practice, for instance, due to equipment failure or change of operating mode. This can be treated as a scenario with time-varying parameters that are continuously estimated. ","category":"page"},{"location":"parameter_estimation/#Using-an-optimizer","page":"Parameter estimation","title":"Using an optimizer","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"The state estimators in this package are all statistically motivated and thus compute things like the likelihood of the data as a by-product of the estimation. Maximum-likelihood or prediction-error estimation is thus very straight-forward by simply calling a gradient-based optimizer with gradients provided by differentiating through the state estimator using automatic differentiation. In this example, we will continue the example from above, but now estimate all the parameters of the quad-tank process. This time, they will not vary with time. We will first use a standard optimization algorithm from Optim.jl to minimize the cost function based on the prediction error, and then use a Gauss-Newton optimizer.","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"We now define the dynamics function such that it takes its parameters from the p input argument. We also define a variable p_true that contains the true values that we will use to simulate some estimation data","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"function quadtank(h, u, p, t)\n    k1, k2, g = p[1], p[2], 9.81\n    A1 = A3 = A2 = A4 = p[3]\n    a1 = a3 = a2 = a4 = p[4]\n    γ1 = γ2 = p[5]\n\n    ssqrt(x) = √(max(x, zero(x)) + 1e-3) # For numerical robustness at x = 0\n    \n    SA[\n        -a1/A1 * ssqrt(2g*h[1]) + a3/A1*ssqrt(2g*h[3]) +     γ1*k1/A1 * u[1]\n        -a2/A2 * ssqrt(2g*h[2]) + a4/A2*ssqrt(2g*h[4]) +     γ2*k2/A2 * u[2]\n        -a3/A3*ssqrt(2g*h[3])                          + (1-γ2)*k2/A3 * u[2]\n        -a4/A4*ssqrt(2g*h[4])                          + (1-γ1)*k1/A4 * u[1]\n    ]\nend\n\ndiscrete_dynamics = SeeToDee.Rk4(quadtank, Ts) # Discretize the dynamics using a 4:th order Runge-Kutta integrator\np_true = [1.6, 1.6, 4.9, 0.03, 0.2]\nnothing # hide","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"Similar to previous example, we simulate the system, this time using a more exciting input in order to be able to identify several parameters","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"using Random; Random.seed!(1) # hide\nTperiod = 200\nt = 0:Ts:1000\nu1 = vcat.(0.25 .* sign.(sin.(2pi/Tperiod .* (t ./ 40).^2)) .+ 0.25)\nu2 = vcat.(0.25 .* sign.(sin.(2pi/Tperiod .* (t ./ 40).^2 .+ pi/2)) .+ 0.25)\nu  = SVector{nu}.(vcat.(u1,u2))\nx0 = SA[2.0,2,3,3] # Initial condition, static array for performance\nx = LowLevelParticleFilters.rollout(discrete_dynamics, x0, u, p_true)[1:end-1]\ny = measurement.(x, u, 0, 0)\ny = [y .+ 0.01 .* randn.() for y in y]\n\nplot(\n    plot(reduce(hcat, x)', title=\"State\"),\n    plot(reduce(hcat, u)', title=\"Inputs\")\n)","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"This time, we define a cost function for the optimizer to optimize, we'll use the sum of squared errors (sse). It's important to define the UKF with an initial state distribution with the same element type as the parameter vector so that automatic differentiation through the state estimator works, hence the explicit casting T.(x0) and T.(R1). We also make sure to use StaticArrays for the covariance matrices and the initial condition for performance reasons (optional).","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"nx = 4\nR1 = SMatrix{nx,nx}(Diagonal([0.1, 0.1, 0.1, 0.1])) # Use of StaticArrays is generally good for performance\nR2 = SMatrix{ny,ny}(Diagonal((1e-2)^2 * ones(ny)))\nx0 = SA[2.0, 2, 3, 3]\n\nfunction cost(p::Vector{T}) where T\n    kf = UnscentedKalmanFilter(discrete_dynamics, measurement, R1, R2, MvNormal(T.(x0), T.(R1)); ny, nu, Ts)\n    LowLevelParticleFilters.sse(kf, u, y, p) # Sum of squared prediction errors\nend\nnothing # hide","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"We generate a random initial guess for the estimation problem","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"p_guess = p_true .+  0.1*p_true .* randn(length(p_true))","category":"page"},{"location":"parameter_estimation/#Solving-using-Optim","page":"Parameter estimation","title":"Solving using Optim","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"We first minimize the cost using the BFGS optimization algorithm from Optim.jl","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"using Optim\nres = Optim.optimize(\n    cost,\n    p_guess,\n    BFGS(),\n    Optim.Options(\n        show_trace = true,\n        show_every = 5,\n        iterations = 100,\n        time_limit = 30,\n    ),\n    autodiff = :forward, # Indicate that we want to use forward-mode AD to derive gradients\n)","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"We started out with a normalized parameter error of","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"using LinearAlgebra\nnorm(p_true - p_guess) / norm(p_true)","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"and ended with","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"p_opt = res.minimizer\nnorm(p_true - p_opt) / norm(p_true)","category":"page"},{"location":"parameter_estimation/#Solving-using-Gauss-Newton-optimization","page":"Parameter estimation","title":"Solving using Gauss-Newton optimization","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"Below, we optimize the sum of squared residuals again, but this time we do it using a Gauss-Newton style algorithm (Levenberg Marquardt). These algorithms want the entire residual vector rather than the sum of squares of the residuals, so we define an alternative \"cost function\" called residuals that calls the lower-level function LowLevelParticleFilters.prediction_errors!","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"using LeastSquaresOptim\n\nfunction residuals!(res, p::Vector{T}) where T\n    kf = UnscentedKalmanFilter(discrete_dynamics, measurement, R1, R2, MvNormal(T.(x0), T.(R1)); ny, nu, Ts)\n    LowLevelParticleFilters.prediction_errors!(res, kf, u, y, p) \nend\n\nres_gn = optimize!(LeastSquaresProblem(x = copy(p_guess), f! = residuals!, output_length = length(y)*ny, autodiff = :forward), LevenbergMarquardt())\n\np_opt_gn = res_gn.minimizer\nnorm(p_true - p_opt_gn) / norm(p_true)","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"When performing sum-of-squares minimization like here, we can, assuming that we converge to the global optimum, estimate the covariance of the estimated parameters. The precision matrix Λ, which is the inverse of the covariance matrix of the parameters, is given by a scaled Hessian of the cost function. The Gauss-Newton appoximation of the Hessian is given by JJ, where J is the Jacobian of the residuals. ","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"using ForwardDiff\nT = length(y)\nJ = ForwardDiff.jacobian(residuals!, zeros(T * ny), res_gn.minimizer)\nΛ = (T - length(p_guess)) * Symmetric(J' * J) # Precision matrix of the estimated parameters\n# Σ = inv(Λ) # Covariance matrix of the estimated parameters (only compute this if precision matrix is well conditioned)\nsvdvals(Λ)","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"In this case, the precision matrix is singular, indicating that there is at least one diretion in parameter space that yields no increase in cost, and we can thus not determine where along a line in this direction the true parameter lies.","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"Gauss-Newton algorithms are often more efficient at sum-of-squares minimization than the more generic BFGS optimizer. This form of Gauss-Newton optimization of prediction errors is also available through ControlSystemIdentification.jl, which uses this package undernath the hood.","category":"page"},{"location":"parameter_estimation/#Which-method-should-I-use?","page":"Parameter estimation","title":"Which method should I use?","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"The methods demonstrated above have slightly different applicability, here, we try to outline which methods to consider for different problems","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"Method Parameter Estimation Covariance Estimation Time Varying Parameters Online Estimation\nMaximum likelihood 🟢 🟢 🟥 🟥\nJoint state/par estim 🔶 🟥 🟢 🟢\nPrediction-error opt. 🟢 🟥 🟥 🟥","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"When trying to optimize parameters of the noise distributions, most commonly the covariance matrices, maximum-likelihood (or MAP) is the only recommened method. Similarly, when parameters are time varying or you want an online estimate, the method that jointly estimates state and parameter is the only applicable method. When fitting standard parameters, all methods are applicable. In this case the joint state and parameter estimation tends to be inefficient and unneccesarily complex, and it is recommended to opt for maximum likelihood or prediction-error minimization. The prediction-error minimization (PEM) with a Gauss-Newtown optimizer is often the most efficient method for this type of problem.","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"Maximum likelihood estimation tends to yield an estimator with better estimates of posterior covariance since this is explicitly optimized for, while PEM tends to produce the smallest possible prediction errors.","category":"page"},{"location":"parameter_estimation/#Identifiability","page":"Parameter estimation","title":"Identifiability","text":"","category":"section"},{"location":"parameter_estimation/#Polynomial-methods","page":"Parameter estimation","title":"Polynomial methods","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"There is no guarantee that we will recover the true parameters by perfoming parameter estimation, especially not if the input excitation is poor. For the system in this tutorial, we will generally find parameters that results in a good predictor for the system (this is after all what we're optimizing for), but these may not be the \"correct\" parameters. A tool like StructuralIdentifiability.jl may be used to determine the identifiability of parameters and state variables (for rational systems), something that for this system could look like","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"using StructuralIdentifiability\n\node = @ODEmodel(\n    h1'(t) = -a1/A1 * h1(t) + a3/A1*h3(t) +     gam*k1/A1 * u1(t),\n    h2'(t) = -a2/A2 * h2(t) + a4/A2*h4(t) +     gam*k2/A2 * u2(t),\n    h3'(t) = -a3/A3*h3(t)                 + (1-gam)*k2/A3 * u2(t),\n    h4'(t) = -a4/A4*h4(t)                 + (1-gam)*k1/A4 * u1(t),\n\ty1(t) = h1(t),\n    y2(t) = h2(t),\n)\n\nlocal_id = assess_local_identifiability(ode)","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"where we have made the substitution sqrt h rightarrow h due to a limitation of the tool (it currently only handles rational ODEs). The output of the above analysis is ","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"julia> local_id = assess_local_identifiability(ode)\nDict{Nemo.fmpq_mpoly, Bool} with 15 entries:\n  a3  => 0\n  gam => 1\n  k2  => 0\n  A4  => 0\n  h4  => 0\n  h2  => 1\n  A3  => 0\n  a1  => 0\n  A2  => 0\n  k1  => 0\n  a4  => 0\n  h3  => 0\n  h1  => 1\n  A1  => 0\n  a2  => 0","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"indicating that we can not hope to resolve all of the parameters. However, using appropriate regularization from prior information, we might still recover a lot of information about the system. Regularization could easily be added to the function cost above, e.g., using a penalty like (p-p_guess)'Γ*(p-p_guess) for some matrix Gamma, to indicate our confidence in the initial guess.","category":"page"},{"location":"parameter_estimation/#Linear-methods","page":"Parameter estimation","title":"Linear methods","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"This package also contains an interface to ControlSystemsBase, which allows you to call ControlSystemsBase.observability(f, x, u, p, t) on a filter f to linearize (if needed) it in the point x,u,p,t and assess observability using linear methods (the PHB test).","category":"page"},{"location":"parameter_estimation/#Videos","page":"Parameter estimation","title":"Videos","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"Examples of parameter estimation are available here","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"By using an optimizer to optimize the likelihood of an UnscentedKalmanFilter:","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"<iframe style=\"height: 315px; width: 560px\" src=\"https://www.youtube.com/embed/0RxQwepVsoM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"Estimation of time-varying parameters:","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"<iframe style=\"height: 315px; width: 560px\" src=\"https://www.youtube.com/embed/zJcOPPLqv4A?si=XCvpo3WD-4U3PJ2S\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"Adaptive control by means of estimation of time-varying parameters:","category":"page"},{"location":"parameter_estimation/","page":"Parameter estimation","title":"Parameter estimation","text":"<iframe style=\"height: 315px; width: 560px\" src=\"https://www.youtube.com/embed/Ip_prmA7QTU?si=Fat_srMTQw5JtW2d\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","category":"page"},{"location":"disturbance_gallery/#Disturbance-gallery","page":"Disturbance gallery","title":"Disturbance gallery","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Most filters in this package assume that the disturbances acting on the system are comprised of Gaussian white noise. This may at first appear as a severe limitation, but together with a dynamical model, this is a surprisingly flexible combination. Most disturbance models we list are linear, which means that they work for any state estimator, including standard Kalman filters. In the end, we also mention some nonlinear disturbance models that require a nonlinear state estimator, such as an UnscentedKalmanFilter. For each disturbance model, we provide a statespace model and show a number of samples from the model, we also list a number of example scenarios where the model is useful. In many cases, models have an interpretation also in the Laplace domain or as a temporal Gaussian process. ","category":"page"},{"location":"disturbance_gallery/#Stochastic-vs.-deterministic-but-unknown","page":"Disturbance gallery","title":"Stochastic vs. deterministic but unknown","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"While some sources of errors are random, such as sensor noise, other sources of errors are deterministic but unknown. For example, a miscalibrated sensor is affected by a static but unknown error. We may communicate these properties to our state estimator by","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Providing the initial distribution of the state. If this is, e.g., a wide Gaussian distribution, we indicate that we are uncertain about the initial state. If the covariance is zero, we indicate that the initial state is perfectly known. The initial state distribution is usually denoted d_0 in this documentation.\nProviding the covariance of the driving disturbance noise. If this is zero, the disturbance is deterministic and the uncertainty about it comes solely from the initial state distribution. If this is positive, the disturbance is random and the uncertainty about it comes from both the initial state distribution and the disturbance noise. Where distributions are assumed to be Gaussian, we refer to the covariance matrix of the dynamics noise as R_1 and the measurement noise R_2. When noises can take any distribution, we refer to these distributions as df and dg instead.","category":"page"},{"location":"disturbance_gallery/#White-noise","page":"Disturbance gallery","title":"White noise","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"This is the simplest possible disturbance model and require no dynamical system at all, just the driving white-noise input. Most state estimators in this package assume that the noise is Gaussian, but particle filters can also be used with non-Gaussian noise.","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"White noise has a flat spectrum (analogous to how white light contains all colors). ","category":"page"},{"location":"disturbance_gallery/#Samples","page":"Disturbance gallery","title":"Samples","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"using Plots\nw = randn(100)\nplot(\n    plot(w, label=\"Gaussian white noise\"),\n    histogram(w, title=\"Histogram\"),\n)","category":"page"},{"location":"disturbance_gallery/#Integrated-white-noise","page":"Disturbance gallery","title":"Integrated white noise","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"This simplest dynamical disturbance model is white noise integrated once. This is a non-stationary process since the variance grows over time, which means that this model is suitable for disturbances that can have any arbitrary magnitude, but no particular properties of the evolution of the disturbance over time is known. This models is sometimes called a Brownian random walk, or a Wiener process.","category":"page"},{"location":"disturbance_gallery/#Model","page":"Disturbance gallery","title":"Model","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Continuous time","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"dotx = w","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Discrete time","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"x_k+1 = x_k + T_s w_k","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Frequency domain","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"G(s) = frac1s","category":"page"},{"location":"disturbance_gallery/#Samples-2","page":"Disturbance gallery","title":"Samples","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Below, we draw samples from the disturbance model by creating a linear statespace system from ControlSystemsBase.jl and call lsim to simulate the system with white noise w sim mathcalN(0 1) input. We also compute the time-varying covariance of the disturbance process, assuming that the covariance of the initial state is zero. This is done by solving the discrete time-varying Lyapunov equation","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"beginaligned\nR_k+1 = A R_k A^T + BB^T \nR_0 = mathbf0 \nR_y_k = C R_k C^T\nendaligned","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"implemented in the function covariance_dynamics. ","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"using ControlSystemsBase, Plots, LinearAlgebra\n\nfunction covariance_dynamics(sys, N=1000; R1=I)\n    # Calculate the covariance of the dynamics noise\n    iscontinuous(sys) && error(\"Only discrete-time systems are supported\")\n    (; A, B, C) = sys\n    Q = B * R1 * B' # Covariance of the dynamics noise\n    R = 0*I(size(A, 1)) # Initial covariance\n    Ry = [(C*R*C')[]] # Covariance of the output\n    for i = 1:N-1\n        R = A*R*A' + Q # Discrete-time Lyapunov equation\n        push!(Ry, (C*R*C')[])\n    end\n    return 2 .* sqrt.(Ry) # 2σ(t)\nend\n\nTs = 0.01 # Sampling time\nsys = ss(1, Ts, 1, 0, Ts) # Discrete-time integrator\nres = map(1:10) do i\n    w = randn(1, 1000) # White noise input\n    lsim(sys, w)\nend\nfigsim = plot(res)\nplot!(res[1].t, [1 -1] .* covariance_dynamics(sys), lab=\"2σ(t)\", color=:black, linestyle=:dash)\nfigspec = bodeplot(sys, plotphase=false)\nfigimp = plot(impulse(sys, 10), title=\"Impulse response\")\nplot(figsim, figspec, figimp, plot_title=\"Integrated white noise\")","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Note, the samples from this process do not look random and step like, but a random step-like process can nevertheless be well modeled by such a process (this is hinted at by the transfer function 1s which is identical to the Laplace transform of the step function). This model is used in a number of examples that demonstrate this property:","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Joint state and parameter estimation\nFault detection\nLQG control with integral action","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"When the noise w has the variance 1, this leads to a process that has a linearly increasing variance with slope BB^T  T_s = T_s^2  T_s = T_s, that is, after 10 seconds the variance is 10T_s = 01. To make the covariance dynamics invariant to the choice of sample interval, we can use the variance R1 = σ² / Ts, in which case the variance is σ² after 1 second and 10σ² after 10 seconds, irrespective of the choice of sample interval T_s.","category":"page"},{"location":"disturbance_gallery/#Suitable-for","page":"Disturbance gallery","title":"Suitable for","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Random-step like disturbances\nFriction\nUnknown change of operating point or set point\nStatic or slowly varying calibration errors (if the error is completely static but initially unknown, use nonzero initial covariance but zero covariance for w).\nGyroscope drift","category":"page"},{"location":"disturbance_gallery/#Double-integrated-white-noise","page":"Disturbance gallery","title":"Double integrated white noise","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"This is a second-order dynamical disturbance model that is white noise integrated twice. This is a non-stationary process since the variance grows over time, which means that this model is suitable for disturbances that can have any arbitrary magnitude, and where the evolution of the disturbance is subject to inertia, that is, the disturbance is expected to evolve be smoothly. ","category":"page"},{"location":"disturbance_gallery/#Model-2","page":"Disturbance gallery","title":"Model","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Continuous time","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"ddotx = w","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Discrete time","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"beginaligned\nx_k+1 = x_k + T_s v_k + fracT_s^22w_k \nv_k+1 = v_k + T_s w_k\nendaligned","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Frequency domain","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"G(s) = frac1s^2","category":"page"},{"location":"disturbance_gallery/#Samples-3","page":"Disturbance gallery","title":"Samples","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"using ControlSystemsBase, Plots\nTs = 0.01 # Sampling time\nsys = ss([1 Ts; 0 1], [Ts^2/2; Ts], [1 0], 0, Ts) # Discrete-time double integrator\nres = map(1:10) do i\n    w = randn(1, 1000) # White noise input\n    lsim(sys, w)\nend\nfigsim = plot(res)\nplot!(res[1].t, [1 -1] .* covariance_dynamics(sys), lab=\"2σ(t)\", color=:black, linestyle=:dash)\nfigspec = bodeplot(sys, plotphase=false)\nplot(figsim, figspec, plot_title=\"Double integrated white noise\")","category":"page"},{"location":"disturbance_gallery/#Suitable-for-2","page":"Disturbance gallery","title":"Suitable for","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Random ramp-like disturbances\nSmoothly varying disturbances","category":"page"},{"location":"disturbance_gallery/#Low-pass-filtered-white-noise","page":"Disturbance gallery","title":"Low-pass filtered white noise","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"If we pass white noise through a low-pass filter, we get a signal that is random but primarily contains low frequencies. This is a stationary process, which means that the variance does not grow indefinitely over time, and we can calculate the stationary covariance of the process by solving a Lyapunov equation (done by ControlSystemsBase.covar). We do this below in order to indicate the stationary standard deviation of the process in the plot of the samples. This model is associated with a tuning parameter that determines the cutoff frequency of the low-pass filter, `tau. ","category":"page"},{"location":"disturbance_gallery/#Model-3","page":"Disturbance gallery","title":"Model","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Continuous time","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"dotx = frac1tau(-x + w)","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Discrete time","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"x_k+1 = α x_k + (1-α) w_k qquad α = e^-T_stau","category":"page"},{"location":"disturbance_gallery/#Transfer-function","page":"Disturbance gallery","title":"Transfer function","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"G(s) = frac1tau s + 1","category":"page"},{"location":"disturbance_gallery/#Samples-4","page":"Disturbance gallery","title":"Samples","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"using ControlSystemsBase, Plots\nTs = 0.01 # Sampling time\nτ = 1.0\nsys = ss(c2d(tf(1, [τ, 1]), Ts)) # Discrete-time first-order low-pass filter\nres = map(1:10) do i\n    w = randn(1, 1000) # White noise input\n    lsim(sys, w)\nend\nfigsim = plot(res)\nhline!(2*sqrt.(covar(sys, I(1))) .* [1 -1], color=:black, linestyle=:dash, linewidth=2, label=\"2σ(∞)\") # Stationary standard deviation\nplot!(res[1].t, [1 -1] .* covariance_dynamics(sys), lab=\"2σ(t)\", color=:black, linestyle=:dash)\nfigspec = bodeplot(sys, plotphase=false)\nfigimp = plot(impulse(sys, 10), title=\"Impulse response\")\nplot(figsim, figspec, figimp, plot_title=\"Low-pass filtered white noise\")","category":"page"},{"location":"disturbance_gallery/#Suitable-for-3","page":"Disturbance gallery","title":"Suitable for","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Stationary noise dominated by low frequencies","category":"page"},{"location":"disturbance_gallery/#Alternative-names","page":"Disturbance gallery","title":"Alternative names","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Ornstein–Uhlenbeck process\nGaussian process: Exponential covariance function","category":"page"},{"location":"disturbance_gallery/#Higher-order-low-pass-filtered-white-noise","page":"Disturbance gallery","title":"Higher-order low-pass filtered white noise","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"If we add more poles to the low-pass filter, we can model Gaussian processes with the Matérn covariance function with half-integer smoothness. The Matérn covariance with ν=12 is equivalent to the first-order low-pass filter above, and with ν=32 we get the model","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"beginaligned\nA = beginbmatrix\n0  1 \n-lambda^2  -2 lambda\nendbmatrix \nB = beginbmatrix\n0  1\nendbmatrix \nC = beginbmatrix\n1  0\nendbmatrix \nlambda = sqrt3  l\nendaligned","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"where l is the length scale of the covariance function.","category":"page"},{"location":"disturbance_gallery/#Samples-from-Matérn-3/2-covariance-function","page":"Disturbance gallery","title":"Samples from Matérn 3/2 covariance function","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"using ControlSystemsBase, Plots\nTs = 0.01 # Sampling time\nl = 1.0 # Length scale\nλ = sqrt(3) / l\nA = [0 1; -λ^2 -2λ]\nB = [0; 1]\nC = [1 0]\nsys = c2d(ss(A, B, C, 0), Ts)\nres = map(1:10) do i\n    w = randn(1, 1000) # White noise input\n    lsim(sys, w)\nend\nfigsim = plot(res)\n(; B,C) = sys\nplot!(res[1].t, [1 -1] .* covariance_dynamics(sys), lab=\"2σ(t)\", color=:black, linestyle=:dash)\nfigspec = bodeplot(sys, plotphase=false)\nfigimp = plot(impulse(sys, 10), title=\"Impulse response\")\nplot(figsim, figspec, figimp, plot_title=\"Low-pass (second order) filtered white noise\")","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Note how this produces smoother signals compared to the first-order low-pass filter. The Matérn covariance function with ν=52 can be modeled by adding a third state to the system above, and so on.","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"For more details on the relation between temporal Gaussian processes and linear systems, see section 3.3 in \"Stochastic Differential Equation Methods for Spatio-Temporal Gaussian Process Regression\", Arno Solin.","category":"page"},{"location":"disturbance_gallery/#Periodic-disturbance","page":"Disturbance gallery","title":"Periodic disturbance","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"If disturbances have a dominant frequency or period, such as 50Hz from the electrical grid, or 24hr from the sun, a periodic disturbance model can be used. A second-order linear system with a complex-conjugate pair of poles close to the imaginary axis has a resonance peak in the frequency response which is suitable for modeling periodic disturbances. If the disturbance is perfectly sinusoidal but the phase is unknown, we may indicate this by setting the covariance of the driving noise to zero and placing the poles exactly on the imaginary axis. ","category":"page"},{"location":"disturbance_gallery/#Model-4","page":"Disturbance gallery","title":"Model","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Continuous time","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"beginaligned\ndotx = beginbmatrix\n-zeta  -omega_0 \nomega_0  -zeta\nendbmatrix x + beginbmatrix\nomega_0 \n0\nendbmatrix w \ny = beginbmatrix\n0  omega_0\nendbmatrix x\nendaligned","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Frequency domain","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"G(s) = fracomega_0s^2 + 2zeta omega_0 s + omega_0^2","category":"page"},{"location":"disturbance_gallery/#Samples-5","page":"Disturbance gallery","title":"Samples","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"using ControlSystemsBase, Plots\nω0 = 2π/3 # Resonance frequency [rad/s]\nζ = 0.1 # Damping ratio, smaller value gives higher amplitude\nTs = 0.05\nt = 0:Ts:20 # Time vector\nsys = c2d(ss([-ζ -ω0; ω0 -ζ], [ω0; 0], [0 ω0], 0), Ts)\nres = map(1:10) do i\n    w = randn(1, length(t)) # White noise input\n    lsim(sys, w, t)\nend\nfigsim = plot(res)\n(; B,C) = sys\nplot!(res[1].t, [1 -1] .* covariance_dynamics(sys, length(res[1].t)), lab=\"2σ(t)\", color=:black, linestyle=:dash)\nfigspec = bodeplot(sys, plotphase=false)\nfigimp = plot(impulse(sys, 10), title=\"Impulse response\")\nplot(figsim, figspec, figimp, plot_title=\"Periodic disturbance\")","category":"page"},{"location":"disturbance_gallery/#One-sided-random-bumps","page":"Disturbance gallery","title":"One sided random bumps","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"This is a nonlinear disturbance model that is useful when the disturbance is expected to be non-negative (or non-positive). The model is a combination of low-pass filtered white noise and a nonlinear integrator that integrates the low-pass filtered white noise only when it is positive. ","category":"page"},{"location":"disturbance_gallery/#Model-5","page":"Disturbance gallery","title":"Model","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Continuous time","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"beginaligned\ndotx = beginbmatrix\n-a x_1 + w \n-b x_2 + max(0 x_1)^n\nendbmatrix \ny = x_2\nendaligned","category":"page"},{"location":"disturbance_gallery/#Samples-6","page":"Disturbance gallery","title":"Samples","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Since this is a nonlinear model, we cannot use the lsim function to simulate it. Instead, we use the package SeeToDee.jl to discretize the nonlinear dynamics model, learn more under Discretization.","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"using LowLevelParticleFilters, SeeToDee, Plots, Random\nTs = 0.1 # Sampling time\na = 1 # Low-pass filter (inverse) time constant, controls how often the bumps appear (higher value ⟹ more often)\nb = 2 # Bump decay (inverse) time constant\nn = 2 # Nonlinearity exponent\n\n# Define dynamics function\nfunction dynamics(x, w, p, t) # We assume that the noise is coming in through the second argument here. When using this model with an UnscentedKalmanFilter, we may instead add w as the 5:th argument and let the second argument be the control input.\n    x1, x2 = x\n    dx1 = -a * x1 + w[1]\n    dx2 = -b * x2 + max(0.0, x1)^n\n    return [dx1, dx2]\nend\ndiscrete_dynamics = SeeToDee.Rk4(dynamics, Ts)\n# Measurement function (only observes x2)\nfunction measurement(x, w, p, t)\n    return [x[2]]\nend\n# Simulate the model\nt = 0:Ts:20 # Time vector\nx0 = [0.0; 0.0] # Initial state\nRandom.seed!(0) # For reproducibility\nres = map(1:10) do i\n    if i == 1\n        w = [(j==0)/Ts for j in t] # Pulse input for first sample\n    else\n        w = [randn(1) for j in t] # White noise input\n    end\n    x = LowLevelParticleFilters.rollout(discrete_dynamics, x0, w)\n    reduce(hcat, measurement.(x[1:end-1], w, nothing, t))'\nend\nplot(t, res, title=\"One-sided random bumps\", lw=[5 ones(1,9)])","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Note how the samples are all nonnegative, achieved by the nonlinearity. The first sample is the impulse response of the system, and this is drawn with a greater linewidth.","category":"page"},{"location":"disturbance_gallery/#Observability","page":"Disturbance gallery","title":"Observability","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"This is a nonlinear model, but it is piecewise linear and we may use linear observability tests to check if the system is observable in each mode.","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Ap = [-a 0; 1 -b]\nAn = [-a 0; 0 -b]\nB = [1.0; 0]\nC = [0 1]\nsysp = ss(Ap, B, C, 0)\nsysn = ss(An, B, C, 0)\n\nobservability(sysp)","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"When x_2 is positive, the system is observable, but when x_2 is negative","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"observability(sysn)","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"we have lost observability of the first state variable. This may pose a problem for, e.g., an ExtendedKalmanFilter, which performs linearization around the current state estimate. To mitigate the observability issue, we may change the nonlinearity to, e.g., a softplus function:","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"softplus(x, hardness=10) = log(1 + exp(hardness*x))/hardness # A softer version of ReLU\n\nfunction dynamics(x, w, p, t) # We assume that the noise is coming in through the second argument here. When using this model with an UnscentedKalmanFilter, we may instead add w as the 5:th argument and let the second argument be the control input.\n    x1, x2 = x\n    dx1 = -a * x1 + w[1]\n    dx2 = -b * x2 + softplus(x1)^n\n    return [dx1, dx2]\nend\ndiscrete_dynamics = SeeToDee.Rk4(dynamics, Ts)\nRandom.seed!(0) # For reproducibility\n\nres = map(1:10) do i\n    w = [randn(1) for i in t] # White noise input\n    x = LowLevelParticleFilters.rollout(discrete_dynamics, x0, w)\n    reduce(hcat, measurement.(x[1:end-1], w, nothing, t))'\nend\nplot(t, res, title=\"One-sided random bumps (softplus)\")","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"This produces a very similar result to the previous model, but adds a tunable hardness parameter that can trade off observability and tendency to output values that are closer to zero.","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"note: Tip\nThe function ControlSystemsBase.observability(f::AbstractKalmanFilter, x, u, p, t=0.0) is overloaded for nonlinear state estimators from this package.","category":"page"},{"location":"disturbance_gallery/#One-sided-periodic-bumps","page":"Disturbance gallery","title":"One sided periodic bumps","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"This is similar to the previous model, but with a periodic disturbance driving the nonlinear integrator, causing the bumps to have a dominant period.","category":"page"},{"location":"disturbance_gallery/#Model-6","page":"Disturbance gallery","title":"Model","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Continuous time","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"beginaligned\ndotx = beginbmatrix\nx_2 \n-2 zeta omega_0 x_2 - omega_0^2 x_1 + w \n-b x_3 + max(0 x_1)^n\nendbmatrix \ny = x_3\nendaligned","category":"page"},{"location":"disturbance_gallery/#Samples-7","page":"Disturbance gallery","title":"Samples","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"using SeeToDee, LowLevelParticleFilters, Plots\nperiod = 24.0\nω = 2π / period # Resonance frequency [rad/s]\nζ = 0.1\nb = 0.9 # Bump decay (inverse) time constant\nn = 2 # Nonlinearity exponent\nTs = 0.1 # Sampling time\n\n# Define dynamics function\nfunction dynamics(x, w, p, t)\n    x1, x2, x3 = x\n    dx1 = x2\n    dx2 = -2 * ζ * ω * x2 - ω^2 * x1 + w[1]\n    relu_x1 = max(0.0, x1)\n    dx3 = -b * x3 + relu_x1^n\n    return [dx1, dx2, dx3]\nend\n\ndiscrete_dynamics = SeeToDee.Rk4(dynamics, Ts)\n\n# Measurement function (only observes x3)\nfunction measurement(x, u, p, t)\n    return [x[3]]\nend\n\n# Simulate the model\nt = 0:Ts:120 # Time vector\nx0 = [0.0; 0.0; 0.0] # Initial state\n\nres = map(1:10) do i\n    if i == 1\n        w = [2*(j==0)/Ts for j in t] # Pulse input for first sample\n    else\n        w = [randn(1) for j in t] # White noise input\n    end\n    x = LowLevelParticleFilters.rollout(discrete_dynamics, x0, w)\n    reduce(hcat, measurement.(x[1:end-1], w, nothing, t))'\nend\nplot(t, res, title=\"One sided periodic bumps\", lw=[5 ones(1,9)])","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"The first sample is one possible impulse response of the system (the system is nonlinear and does not have a single unique impulse response), and this is drawn with a greater linewidth.","category":"page"},{"location":"disturbance_gallery/#Useful-for","page":"Disturbance gallery","title":"Useful for","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"One-sided periodic disturbances\nExample: Sunlight hitting a thermometer once per day, but only if it is sunny","category":"page"},{"location":"disturbance_gallery/#Deterministic-disturbances","page":"Disturbance gallery","title":"Deterministic disturbances","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"If there is a deterministic aspect to the disturbance, we may use the fact that dynamics and measurement functions (as well as Kalman filter matrices) may depend on time. For exactly, the perfectly deterministic measurement disturbance disturbance sin(t) is easily modeled by including it in the measurement function. ","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"measurement(x, u, p, t) = ... + sin(t)","category":"page"},{"location":"disturbance_gallery/#Heteroschedastic-disturbances","page":"Disturbance gallery","title":"Heteroschedastic disturbances","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"If the disturbance is heteroschedastic, i.e., the variance of the disturbance depends on time or on the state, we may easily indicate this to the state estimator by either","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Let the covariance matrix depend on time or state, applicable to all estimators.\nEncode varying the gain from disturbance to state/measurement in the corresponding dynamics/measurement function, applicable to nonlinear state estimators only.","category":"page"},{"location":"disturbance_gallery/#Non-Gaussian-driving-noise","page":"Disturbance gallery","title":"Non-Gaussian driving noise","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"All Kalman-type estimators assume that the driving noise is Gaussian. Particle filters are not limited to this assumption and can generally be used with any distribution that can be sampled from, see Smoothing the track of a moving beetle for an example, where the mode is affected by Binomial noise.","category":"page"},{"location":"disturbance_gallery/#Dynamical-models-of-measurement-disturbance","page":"Disturbance gallery","title":"Dynamical models of measurement disturbance","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"When using any of the dynamical models above to model measurement disturbances, the noise driving the disturbance dynamics must be sourced from the dynamics noise, e.g., for a Kalman filter for the model","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"beginaligned\nx = Ax + Bu + w \ny  = Cx + Du + e\nendaligned","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"we must let the dynamics noise w drive the disturbance model, and design C such that the estimated disturbance has the desired effect on the measurement. This model leaves no room to let the measurement noise e to pass through a dynamical system, and this is thus only useful (and required) to model white Gaussian measurement noise. See How to tune a Kalman filter for more insights.","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Dynamical models of measurement disturbances are useful in a lot of situations, such as","category":"page"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"Periodic measurement noise, such as 50Hz noise from the electrical grid.\nSlow sensor drift, such as gyroscopic drift.\nCalibration errors.\nSensor misalignment in rotating systems.\nComplimentary filtering for accelerometers and gyroscopes.\nSensor degradation, such as deposition of dust or algae growth.","category":"page"},{"location":"disturbance_gallery/#Continuous-vs.-discrete-time-covariance","page":"Disturbance gallery","title":"Continuous vs. discrete time covariance","text":"","category":"section"},{"location":"disturbance_gallery/","page":"Disturbance gallery","title":"Disturbance gallery","text":"See Discretization: Covariance matrices.","category":"page"},{"location":"adaptive_kalmanfilter/#Noise-adaptive-Kalman-filter","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Noise-adaptive Kalman filter","text":"","category":"section"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"In this tutorial we will consider filtering of a 1D position track, similar in spirit to what one could have obtained from a GPS device, but limited to 1D for easier visualization. We will use a constant-velocity model, i.e., use a double integrator,","category":"page"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"beginaligned\nx_k+1 = beginbmatrix 1  T_s  0  1 endbmatrix x_k + beginbmatrix T_s^22  T_s endbmatrix w_k \ny_k = beginbmatrix 1  0 endbmatrix x_k + v_k\nendaligned","category":"page"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"where w_k sim mathcalN(0 σ_w) is the process noise, and v_k sim mathcalN(0 R_2) is the measurement noise, and illustrate how we can make use of an adaptive noise covariance to improve the filter performance.","category":"page"},{"location":"adaptive_kalmanfilter/#Data-generation","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Data generation","text":"","category":"section"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"We start by generating some position data that we want to perform filtering on. The \"object\" we want to track is initially stationary, and transitions to moving with a constant velocity after a while. ","category":"page"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"using LowLevelParticleFilters, Plots, Random\nRandom.seed!(1)\n\n# Create a time series for filtering\nx = [zeros(50); 0:100]\nT = length(x)\nY = x + randn(T)\nplot([Y x], lab=[\"Measurement\" \"True state to be tracked\"], c=[1 :purple])","category":"page"},{"location":"adaptive_kalmanfilter/#Simple-Kalman-filtering","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Simple Kalman filtering","text":"","category":"section"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"We will use a Kalman filter to perform the filtering. The model is a double integrator, i.e., a constant-acceleration model. The state vector is thus x = p v^T, where p is the position and v is the velocity. When designing a Kalman filter, we need to specify the noise covariances R_1 and R_2. While it's often easy to measure the covariance of the measurement noise, R_2, it can be quite difficult to know ahead of time what the dynamics noise covariance, R_1, should be. In this example, we will use an adaptive filter, where we will increase the dynamics noise covariance if the filter prediction error is too large. However, we first run the filter twice, once with a large R_1 and once with a small R_1 to illustrate the difference.","category":"page"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"y = [[y] for y in Y] # create a vector of vectors for the KF\nu = fill([], T) # No inputs in this example :(\n\n# Define the model\nTs = 1\nA = [1 Ts; 0 1]\nB = zeros(2, 0)\nC = [1 0]\nD = zeros(0, 0)\nR2 = [1;;]\n\nσws = [1e-2, 1e-5] # Dynamics noise standard deviations\n\nfig = plot(Y, lab=\"Measurement\")\nfor σw in σws\n    R1 = σw*[Ts^3/3 Ts^2/2; Ts^2/2 Ts] # The dynamics noise covariance matrix is σw*Bw*Bw' where Bw = [Ts^2/2; Ts]\n    kf = KalmanFilter(A, B, C, D, R1, R2)\n    measure = LowLevelParticleFilters.measurement(kf)\n    yh = [measure(state(kf), u[1], nothing, 1)] \n    for t = 1:T # Main filter loop\n        kf(u[t], y[t]) # Performs both prediction and correction\n        xh = state(kf)\n        yht = measure(xh, u[t], nothing, t)\n        push!(yh, yht)\n    end\n\n    Yh = reduce(hcat, yh)\n    plot!(Yh', lab=\"Estimate \\$σ_w\\$ = $σw\")\nend\nfig","category":"page"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"When R_1 is small (controlled by σ_w), we get a nice and smooth filter estimate, but this estimate clearly lags behind the true state. When R_1 is large, the filter estimate is much more responsive, but it also has a lot of noise.","category":"page"},{"location":"adaptive_kalmanfilter/#Adaptive-noise-covariance","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Adaptive noise covariance","text":"","category":"section"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"Below, we will implement an adaptive filter, where we keep the dynamics noise covariance low by default, but increase it if the filter prediction error is too large. We will use a Z-score to determine if the prediction error is too large. The Z-score is defined as the number of standard deviations the prediction error is away from the estimated mean. This time around we use separate correct! and predict! calls, so that we can access the prediction error as well as the prior covariance of the prediction error, S. S (or the Cholesky factor Sᵪ) will be used to compute the Z-score.","category":"page"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"When implementing behavior such as time varying covariance, we may either implement the filtering loop manually, like we do below, or make use of the callback functionality available in forward_trajectory, which we do in the next code snippet.","category":"page"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"σw = 1e-5 # Set the covariance to a low value by default\nR1 = σw*[Ts^3/3 Ts^2/2; Ts^2/2 Ts]\nkf = KalmanFilter(A, B, C, D, R1, R2)\nmeasure = LowLevelParticleFilters.measurement(kf)\n\n# Some arrays to store simulation data\nyh = []\nes = Float64[]\nσs = Float64[]\nfor t = 1:T # Main filter loop\n    ll, e, S, Sᵪ = correct!(kf, u[t], y[t], nothing, t) # Manually call the prediction step\n    xh = state(kf)\n    yht = measure(xh, u[t], nothing, t)\n\n    σ = √(e'*(Sᵪ\\e)) # Compute the Z-score\n    push!(es, e[]) # Save for plotting\n    push!(σs, σ)\n    if σ > 3 # If the Z-score is too high\n        # we temporarily increase the dynamics noise covariance by 1000x to adapt faster\n        predict!(kf, u[t], nothing, t; R1 = 1000kf.R1) \n    else\n        predict!(kf, u[t], nothing, t)\n    end\n\n    push!(yh, yht)\nend\n\nYh = reduce(hcat, yh)\nplot([Y Yh'], lab=[\"Measurement\" \"Adaptive estimate\"])","category":"page"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"Not too bad! This time the filter estimate is much more responsive during the transition, but exhibits favorable noise properties during the stationary phases. We can also plot the prediction error and the Z-score to see how the filter adapts to the dynamics noise covariance.","category":"page"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"plot([es σs], lab=[\"Prediction error\" \"Z-score\"])","category":"page"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"Notice how the prediction errors, that should ideally be centered around zero, remain predominantly negative for a long time interval after the transition. This can be attributed to an overshoot in the velocity state of the estimator, but the rapid decrease of the covariance after the transition makes the filter slow at correcting its overshoot. If we want, we could mitigate this and make the adaptation even more sophisticated by letting the covariance remain large for a while after a transition in operating mode has been detected. Below, we implement a simple version of this, where we use a multiplier σ_wt that defaults to 1, but is increase to a very large value of 1000 if a transition is detected. When no transition is detected, σ_wt is decreased exponentially back down to 1.","category":"page"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"As mentioned above, in this code snippet we make use of the callback functionality of forward_trajectory rather than implementing the filtering loop manually, we thus add the logic for modifying the covariance in the pre_predict_cb callback function. ","category":"page"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"σw  = 1e-5 # Set the covariance to a low value by default\nσwt = 1.0\nR1  = σw*[Ts^3/3 Ts^2/2; Ts^2/2 Ts]\nkf  = KalmanFilter(A, B, C, D, R1, R2)\nmeasure = LowLevelParticleFilters.measurement(kf)\n\nfunction pre_predict_cb(kf, u, y, p, t, ll, e, S, Sᵪ)\n    σ = √(e'*(Sᵪ\\e)) # Compute the Z-score\n    global σwt\n    if σ > 3 # If the Z-score is too high\n        σwt = 1000.0 # Set the R1 multiplier to a very large value\n    else\n        σwt = max(0.9σwt, 1.0) # Decrease exponentially back to 1\n    end\n    push!(σs, σ)\n    push!(σwts, σwt)\n    σwt*kf.R1 # The pre_predict_cb may return either nothing (operate through side effects) or a modified R1 matrix to use for this particular time step. Here, we make use of both approaches.\nend\n\n# Some arrays to store simulation data\nσs = Float64[]\nσwts = Float64[]\n\nsol = forward_trajectory(kf, u, y; pre_predict_cb)\nes = reduce(vcat, sol.e) # Extract prediciton errors\nYh = reduce(hcat, measure.(sol.xt, sol.u, nothing, nothing)) # Extract predicted outputs\nplot([Y Yh'], lab=[\"Measurement\" \"Adaptive estimate\"])","category":"page"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"plot([es σs σwts], lab=[\"Prediction error\" \"Z-score\" \"\\$σ_{wt}\\$ multiplier\"], layout=2, sp=[1 1 2])","category":"page"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"This time, the prediction errors look more like white noise centered around zero after the initial transient caused by the transition.","category":"page"},{"location":"adaptive_kalmanfilter/#Summary","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Summary","text":"","category":"section"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"This tutorial demonstrated simple Kalman filtering for a double integrator without control inputs. We saw how the filtering estimate could be improved by playing around with the covariance matrices of the estimator, helping it catch up to fast changes in the behavior of the system without sacrificing steady-state noise properties.","category":"page"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"In this case, we handled the modification of R_1 outside of the filter, implementing our own filtering loop. Some applications get away with instead providing time-varying matrices in the form of a 3-dimension array, where the third dimension corresponds to time, or instead of providing a matrix, providing a function R_1(x u p t) allows the matrix to be a function of state, input, parameters and time. These options apply to all matrices in the filter, including the dynamics matrices, ABCD.","category":"page"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"Lastly, we mention the ability of the KalmanFilter to act like a recursive least-squares estimator, by setting the \"forgetting factor α1 when creating the KalmanFilter. α1 will cause the filter will exhibit exponential forgetting similar to an RLS estimator, in addition to the covariance inflation due to R1. It is thus possible to get a RLS-like algorithm by setting R_1 = 0 R_2 = 1α and α  1.","category":"page"},{"location":"adaptive_kalmanfilter/#Disturbance-modeling-and-noise-tuning","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Disturbance modeling and noise tuning","text":"","category":"section"},{"location":"adaptive_kalmanfilter/","page":"Kalman-filter tutorial with LowLevelParticleFilters","title":"Kalman-filter tutorial with LowLevelParticleFilters","text":"See this notebook for a blog post about disturbance modeling and noise tuning using LowLevelParticleFilter.jl","category":"page"},{"location":"neural_network/#Adaptive-Neural-Network-training","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"","category":"section"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"In this example, we will demonstrate how we can take the estimation of time-varying parameters to the extreme, and use a nonlinear state estimator to estimate the weights in a neural-network model of a dynamical system. ","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"In the tutorial Joint state and parameter estimation, we demonstrated how we can add a parameter as a state variable and let the state estimator estimate this alongside the state. In this example, we will try to learn an entire black-box model of the system dynamics using a neural network, and treat the network weights as time-varying parameters by adding them to the state.","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"We start by generating some data from a simple dynamical system, we will continue to use the quadruple-tank system from Joint state and parameter estimation.","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"using LowLevelParticleFilters, Lux, Random, SeeToDee, StaticArrays, Plots, LinearAlgebra, ComponentArrays, DifferentiationInterface, SparseMatrixColorings\nusing SparseConnectivityTracer: TracerSparsityDetector\nusing DisplayAs # hide\n\nusing LowLevelParticleFilters: SimpleMvNormal\n\nfunction quadtank(h,u,p,t)\n    kc = 0.5\n    k1, k2, g = 1.6, 1.6, 9.81\n    A1 = A3 = A2 = A4 = 4.9\n    a1, a3, a2, a4 = 0.03, 0.03, 0.03, 0.03\n    γ1, γ2 = 0.2, 0.2\n\n    if t > 2000\n        a1 *= 1.5 # Change the parameter at t = 2000\n    end\n\n    ssqrt(x) = √(max(x, zero(x)) + 1e-3) # For numerical robustness at x = 0\n\n    SA[\n        -a1/A1 * ssqrt(2g*h[1]) + a3/A1*ssqrt(2g*h[3]) +     γ1*k1/A1 * u[1]\n        -a2/A2 * ssqrt(2g*h[2]) + a4/A2*ssqrt(2g*h[4]) +     γ2*k2/A2 * u[2]\n        -a3/A3*ssqrt(2g*h[3])                          + (1-γ2)*k2/A3 * u[2]\n        -a4/A4*ssqrt(2g*h[4])                          + (1-γ1)*k1/A4 * u[1]\n    ]\nend\n\nTs = 30 # sample time\ndiscrete_dynamics = SeeToDee.Rk4(quadtank, Ts) # Discretize dynamics\nnu = 2 # number of control inputs\nnx = 4 # number of state variables\nny = 4 # number of measured outputs\n\nfunction generate_data()   \n    measurement(x,u,p,t) = x#SA[x[1], x[2]]\n    Tperiod = 200\n    t = 0:Ts:4000\n    u = vcat.((0.25 .* sign.(sin.(2pi/Tperiod .* t)) .+ 0.25) .* sqrt.(rand.()))\n    u = SVector{nu, Float32}.(vcat.(u,u))\n    x0 = Float32[2,2,3,3]\n    x = LowLevelParticleFilters.rollout(discrete_dynamics, x0, u)[1:end-1]\n    y = measurement.(x, u, 0, 0)\n    y = [Float32.(y .+ 0.01.*randn.()) for y in y] # Add some noise to the measurement\n\n    (; x, u, y, nx, nu, ny, Ts)\nend\n\nrng = Random.default_rng()\nRandom.seed!(rng, 1)\ndata = generate_data()\nnothing # hide","category":"page"},{"location":"neural_network/#Neural-network-dynamics","page":"Adaptive Neural-Network training","title":"Neural network dynamics","text":"","category":"section"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"Our neural network will be a small feedforward network built using the package Lux.jl. ","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"ni = ny + nu\nnhidden = 8\nconst model_ = Chain(Dense(ni, nhidden, tanh), Dense(nhidden, nhidden, tanh), Dense(nhidden, ny))","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"Since the network is rather small, we will train on the CPU only, this will be fast enough for this use case. We may extract the parameters of the network using the function Lux.setup, and convert them to a ComponentArray to make it easier to refer to different parts of the combined state vector.","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"dev = cpu_device()\nps, st = Lux.setup(rng, model_) |> dev\nparr = ComponentArray(ps)\nnothing # hide","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"The dynamics of our black-box model will call the neural network to predict the next state given the current state and input. We bias the dynamics towards low frequencies by adding a multiple of the current state to the prediction of the next state, 0.95*x (motivated in Tangent-Space Regularization for Neural-Network Models of Dynamical Systems). We also add a small amount of weight decay to the parameters of the neural network for regularization, 0.995*p.","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"function dynamics(out0, xp0, u, _, t)\n    xp = ComponentArray(xp0, getaxes(s0))\n    out = ComponentArray(out0, getaxes(s0))\n    x = xp.x\n    p = xp.p\n    xp, _ = Lux.apply(model_, [x; u], p, st)\n    @. out.x = 0.95f0*x+xp\n    @. out.p = 0.995f0*p\n    nothing\nend\n\n@views measurement(out, x, _, _, _) = out .= x[1:nx] # Assume measurement of the full state vector\nnothing # hide","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"For simplicity, we have assumed here that we have access to measurements of the entire state vector of the original process. This is many times unrealistic, and if we do not have such access, we may instead augment the measured signals with delayed versions of themselves (sometimes called a delay embedding). This is a common technique in discrete-time system identification, used in e.g., ControlSystemIdentification.arx and subspaceid.","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"The initial state of the process x0 and the initial parameters of the neural network parr can now be concatenated to form the initial augmented state s0.","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"x0 = Float32[2; 2; 3; 3]\ns0 = ComponentVector(; x=x0, p=parr)\nnothing # hide","category":"page"},{"location":"neural_network/#Kalman-filter-setup","page":"Adaptive Neural-Network training","title":"Kalman filter setup","text":"","category":"section"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"We will estimate the parameters using two different nonlinear Kalman filters, the ExtendedKalmanFilter and the UnscentedKalmanFilter. The covariance matrices for the filters, R1, R2, may be tuned such that we get the desired learning speed of the weights, where larger covariance for the network weights will allow for faster learning, but also more noise in the estimates. ","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"R1 = Diagonal([0.1ones(nx); 0.01ones(length(parr))]) .|> Float32\nR2 = Diagonal((1e-2)^2 * ones(ny)) .|> Float32\nnothing # hide","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"The ExtendedKalmanFilter uses Jacobians of the dynamics and measurement model, and if we do not provide those functions they will be automatically computed using ForwardDiff.jl. Since our Jacobians will be relatively large but sparse in this example, we will make use of the sparsity-aware features of DifferentiationInterface.jl in order to get efficient Jacobian computations. ","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"function Ajacfun(x,u,p,t) # Function that returns a function for the Jacobian of the dynamics\n    # For large neural networks, it might be faster to use an OOP formulation with Zygote instead of ForwardDiff. Zygote does not handle the in-place version\n    backend = AutoSparse(\n        AutoForwardDiff(),\n        # AutoZygote(),\n        sparsity_detector=TracerSparsityDetector(),\n        coloring_algorithm=GreedyColoringAlgorithm(),\n    )\n    out = similar(getdata(x))\n    inner = (out,x)->dynamics(out,x,u,p,t)\n    prep = prepare_jacobian(inner, out, backend, getdata(x))\n    jac = one(eltype(x)) .* sparsity_pattern(prep)\n    function (x,u,p,t)\n        inner2 = (out,x)->dynamics(out,x,u,p,t)\n        DifferentiationInterface.jacobian!(inner2, out, jac, prep, backend, x)\n    end\nend\n\nAjac = Ajacfun(s0, data.u[1], nothing, 0)\n\nconst CJ_ = [I(nx) zeros(Float32, nx, length(parr))] # The jacobian of the measurement model is constant\nCjac(x,u,p,t) = CJ_\nnothing # hide","category":"page"},{"location":"neural_network/#Estimation","page":"Adaptive Neural-Network training","title":"Estimation","text":"","category":"section"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"We may now initialize our filters and perform the estimation. Here, we use the function forward_trajectory to perform filtering along the entire data trajectory at once, but we may use this in a streaming fashion as well, as more data becomes available in real time.","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"We plot the one-step ahead prediction of the outputs and compare to the \"measured\" data.","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"ekf = ExtendedKalmanFilter(dynamics, measurement, R1, R2, SimpleMvNormal(s0, 100R1); nu, check=false, Ajac, Cjac, Ts)\nukf = UnscentedKalmanFilter(dynamics, measurement, R1, R2, SimpleMvNormal(s0, 100R1); nu, ny, Ts)\n\n@time sole = forward_trajectory(ekf, data.u, data.x)\n@time solu = forward_trajectory(ukf, data.u, data.x)\n\nkwargs = (plotx=false, plotxt=false, plotyh=true, plotyht=false, plotu=false, plote=true)\nplot(sole;  name=\"EKF\", layout=(nx, 1), size=(1200, 1500), kwargs...)\nplot!(solu; name=\"UKF\", ploty=false, kwargs...)\nDisplayAs.PNG(Plots.current()) # hide","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"We see that prediction errors, e, are large in the beginning when the network weights are randomly initialized, but after about half the trajectory the errors are significantly reduced. Just like in the tutorial Joint state and parameter estimation, we modified the true dynamics after some time, at t=2000, and we see that the filters are able to adapt to this change after a transient increase in prediction error variance.","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"We may also plot the evolution of the neural-network weights over time, and see how the filters adapt to the changing dynamics of the system.","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"plot(\n    plot(0:Ts:4000, reduce(hcat, sole.xt)'[:, nx+1:end], title=\"EKF parameters\"),\n    plot(0:Ts:4000, reduce(hcat, solu.xt)'[:, nx+1:end], title=\"UKF parameters\"),\n    legend = false,\n)\nDisplayAs.PNG(Plots.current()) # hide","category":"page"},{"location":"neural_network/#Smoothing","page":"Adaptive Neural-Network training","title":"Smoothing","text":"","category":"section"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"@time xTe,RTe = smooth(sole, ekf)\n@time xTu,RTu = smooth(solu, ukf)\nplot(\n    plot(0:Ts:4000, reduce(hcat, xTe)'[:, nx+1:end], title=\"EKF parameters\", c=1, alpha=0.2),\n    plot(0:Ts:4000, reduce(hcat, xTu)'[:, nx+1:end], title=\"UKF parameters\", c=1, alpha=0.2),\n    legend = false,\n)","category":"page"},{"location":"neural_network/#Benchmarking","page":"Adaptive Neural-Network training","title":"Benchmarking","text":"","category":"section"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"The neural network used in this example has","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"length(parr)","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"parameters, and the length of the data is","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"length(data.u)","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"Performing the estimation using the Extended Kalman Filter took","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"using BenchmarkTools\n@btime forward_trajectory(ekf, data.u, data.x);\n  # 46.034 ms (77872 allocations: 123.45 MiB)","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"and with the Unscented Kalman Filter","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"@btime forward_trajectory(ukf, data.u, data.x);\n  # 142.608 ms (2134370 allocations: 224.82 MiB)","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"The EKF is a bit faster, which is to be expected. Both methods are very fast from a neural-network training perspective, but the performance will not scale favorably to very large network sizes.","category":"page"},{"location":"neural_network/#Closing-remarks","page":"Adaptive Neural-Network training","title":"Closing remarks","text":"","category":"section"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"We have seen how to estimate train a black-box neural network dynamics model by treating the parameter estimation as a state-estimation problem. This example is very simple and leaves a lot of room for improvement, such as","category":"page"},{"location":"neural_network/","page":"Adaptive Neural-Network training","title":"Adaptive Neural-Network training","text":"We assumed very little prior knowledge of the dynamics. In practice, we may want to model as much as possible from first principles and add a neural network to capture only the residuals that our first-principles model cannot capture.\nWe started the training of the network weights directly from a random initialization. In practice, we may want to pre-train the network on a large offline dataset before updating the weights adaptively in real-time.\nWe used forward-mode AD to compute the Jacobian. The Jacobian of the dynamics has dense rows, which means that it's theoretically favorable to use reverse-mode AD to compute it. This is possible using Zygote.jl, but Zygote does not handle array mutation, and one must thus avoid the in-place version of the dynamics. Since the number of parameters in this example is small, sparse forward mode AD ended up being slightly faster.","category":"page"},{"location":"dae/#State-estimation-for-high-index-DAEs","page":"State estimation for DAE systems","title":"State estimation for high-index DAEs","text":"","category":"section"},{"location":"dae/","page":"State estimation for DAE systems","title":"State estimation for DAE systems","text":"This tutorial is hosted as a notebook.","category":"page"},{"location":"ut/#Unscented-transform","page":"Unscented transform","title":"Unscented transform","text":"","category":"section"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"In this section, we demonstrate how the unscented transform, used in the UnscentedKalmanFilter, propagates a normal distribution through a nonlinear function. ","category":"page"},{"location":"ut/#Covariance-propagation-through-nonlinear-functions","page":"Unscented transform","title":"Covariance propagation through nonlinear functions","text":"","category":"section"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"The propagation of a Gaussian distribution through an affine (or linear) function f(x) = Ax+b is trivial, the distribution N(μ Σ) is transformed into N(Aμ + b AΣA^T), i.e., it remains Gaussian. This fact is what makes the standard KalmanFilter so computationally efficient. However, when the function is nonlinear, the transformation is not as straightforward and the posterior is generally not Gaussian. The unscented transform (UT) is a method to approximate the transformation of a Gaussian distribution through a nonlinear function. The UT is based on the idea of propagating a set of sigma points through the function and then computing the mean and covariance of the resulting distribution. Below, we demonstrate how a normal distribution is transformed through a number of nonlinear functions.","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"For comparison, we also show how the ExtendedKalmanFilter and ParticleFilter propagate the covariance. EKF uses linearization while particle filters propagate a large number of samples. We load the ForwardDiff package to compute the Jacobian of the function.","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"using DisplayAs # hide\nusing LowLevelParticleFilters, Plots\nusing LowLevelParticleFilters: sigmapoints\nusing ForwardDiff, Distributions\nPlots.default(fillalpha=0.3) # This makes the covariance ellipse more transparent\nkwargs = (; markersize=4, markeralpha=0.7, markerstrokewidth=0)\n\nfunction ekf_propagate_plot(f, μ, Σ; kwargs...)\n    x = μ\n    A = ForwardDiff.jacobian(f, x)\n    μ = f(x)\n    Σ = A * Σ * A'\n    covplot!(μ, Σ; kwargs...)\nend\n\nfunction sample_propagate_plot(f, μ, Σ; kwargs...)\n    xpart = rand(MvNormal(μ, Σ), 10000)\n    ypart = f.(eachcol(xpart))\n    scatter!(first.(ypart), last.(ypart); markerstrokealpha=0, markerstrokewidth=0, markeralpha=0.15, markersize=1, kwargs..., lab=\"\")\n    ym = mean(ypart)\n    yS = cov(ypart)\n    covplot!(ym, yS; kwargs...)\n    scatter!([ym[1]], [ym[2]]; markersize=4, markershape=:x, kwargs..., lab=\"\")\nend","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"μ = [1.0, 2.0]\nΣ = [1.0 0.5; 0.5 1.0]\nx = sigmapoints(μ, Σ, TrivialParams())\nn = length(x)\nf1(x) = [x[1]^2+1, sin(x[2])]\ny = f1.(x)\nunscentedplot(x; lab=\"Input\", c=:blue, fillalpha=0.1, kwargs...)\nunscentedplot!(y; lab=\"Output UKF\", c=:red, kwargs...)\nekf_propagate_plot(f1, μ, Σ; lab=\"Output EKF\", c=:orange)\nsample_propagate_plot(f1, μ, Σ; lab=\"Output particles\", c=:green)\n# Plot lines from each input point to each output point\nplot!([first.(x)'; first.(y)'; fill(Inf, 1, n)][:], [last.(x)'; last.(y)'; fill(Inf, 1, n)][:], c=:black, alpha=0.5, primary=false)\nDisplayAs.PNG(Plots.current()) # hide","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"For this first function, f_1(x) = x_1^2+1 sin(x_2), the UT and linearization-based propagation produce somewhat similar results, but the posterior distribution of the UT is much closer to the particle distribution than the EKF.","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"f2(x) = [x[1]*x[2], x[1]+x[2]]\ny = f2.(x)\nunscentedplot(x; lab=\"Input\", c=:blue, fillalpha=0.1, kwargs...)\nunscentedplot!(y; lab=\"Output UKF\", c=:red, kwargs...)\nekf_propagate_plot(f2, μ, Σ; lab=\"Output EKF\", c=:orange)\nsample_propagate_plot(f2, μ, Σ; lab=\"Output particles\", c=:green)\nplot!([first.(x)'; first.(y)'; fill(Inf, 1, n)][:], [last.(x)'; last.(y)'; fill(Inf, 1, n)][:], c=:black, alpha=0.5, primary=false, xlims=(-5, 12))\nDisplayAs.PNG(Plots.current()) # hide","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"For the second function, f_2(x) = x_1 x_2 x_1+x_2, the posterior distribution is highly non-Gaussian. Both the UT and EKF style propagation do reasonable jobs capturing the posterior mean, but the UT does a better, although far from perfect, job at capturing the posterior covariance.","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"f3((x,y)) = [sqrt((1 - x)^2 + (0.1 - y)^2), atan(0.9 - y, 1.0 - x)] # Robot localization measurement model\ny = f3.(x)\nunscentedplot(x; lab=\"Input\", c=:blue, fillalpha=0.1, kwargs...)\nunscentedplot!(y; lab=\"Output UKF\", c=:red, kwargs...)\nekf_propagate_plot(f3, μ, Σ; lab=\"Output EKF\", c=:orange)\nsample_propagate_plot(f3, μ, Σ; lab=\"Output particles\", c=:green)\nplot!([first.(x)'; first.(y)'; fill(Inf, 1, n)][:], [last.(x)'; last.(y)'; fill(Inf, 1, n)][:], c=:black, alpha=0.5, primary=false)\nDisplayAs.PNG(Plots.current()) # hide","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"For the function f_3(x) = sqrt(1 - x)^2 + (01 - y)^2 arctan(09 - y 10 - x), the posterior distribution is once again highly non-Gaussian. The EKF misses to place any significant output probability mass in the region around the input, which the UT does by placing one sigma point in this region. When the particle distribution is approximated by a Gaussian, neither the UT or EKF does very well approximating this Gaussian.","category":"page"},{"location":"ut/#Tuning-parameters","page":"Unscented transform","title":"Tuning parameters","text":"","category":"section"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"The unscented transform that underpins the UnscentedKalmanFilter may be tuned to adjust the spread of the points. By default, TrivialParams are used, but one may also opt for the WikiParams or MerweParams which are more commonly used in the literature.","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"The code snippets below demonstrate how to create different sets of parameters and visualizes the sigma points generated by each set of parameters for a trivial normal 2D distribution, as well has how the points propagate through a simple function.","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"We start by visualizing the sigma points generated by the different parameters sets using their default parameters.","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"μ = [0.0, 0.0]\nΣ = [1.0 0.0; 0.0 1.0]\nwpars = WikiParams(α = 1.0, β = 0.0, κ = 1)\nwxs = sigmapoints(μ, Σ, wpars)\n\nmpars = MerweParams(α = 1.0, β = 2.0, κ = 0.0)\nmxs = sigmapoints(μ, Σ, mpars)\n\ntpars = TrivialParams()\ntxs = sigmapoints(μ, Σ, tpars)\n\nunscentedplot(wxs, wpars; lab=\"Wiki\", c=:green, kwargs...)\nunscentedplot!(mxs, mpars; lab=\"Merwe\", c=:red, kwargs...)\nunscentedplot!(txs, tpars; lab=\"Trivial\", c=:blue, kwargs...)","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"In this plot, we don't see the Merwe points because they are all behind the Trivial points. The Wiki points are less spread out. They all represent exactly the same distribution though, all their covariance ellipses overlap. Different sets of points can represent the same probability distribution by means of different weights that are assigned to each point.","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"Below, we demonstrate how the points propagate through a simple function. We use the function f(x) = max(0 x1) x2 which is a simple function that forces the first state component to be positive.","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"f(x) = [max(zero(x[1]), x[1]), x[2]]\nwxs2 = f.(wxs) # Propagate the points through the function\nmxs2 = f.(mxs)\ntxs2 = f.(txs)\nunscentedplot(wxs2, wpars; lab=\"Wiki\", c=:green ,kwargs...)\nunscentedplot!(mxs2, mpars; lab=\"Merwe\", c=:red ,kwargs...)\nunscentedplot!(txs2, tpars; lab=\"Trivial\", c=:blue ,kwargs...)","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"We now see that the Merwe points resulted in a large covariance of the output. We see that the posterior mean is skewed positively due to the clamping of f, but the mean is skewed less for the trivial parameters than the Wiki parameters, with Merwe somewhere inbetween.","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"By tweaking the parameters, we can obtain different behavior, below we show the spread of the points for different values of α β κ","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"wpars = WikiParams(α = 1, β = -3.0, κ = 10)\nwxs = sigmapoints(μ, Σ, wpars)\n\nmpars = MerweParams(α = 1, β = -3.0, κ = 10)\nmxs = sigmapoints(μ, Σ, mpars)\n\nunscentedplot(wxs, wpars; lab=\"Wiki\", c=:green, kwargs...)\nunscentedplot!(mxs, mpars; lab=\"Merwe\", c=:red, kwargs...)\nunscentedplot!(txs, tpars; lab=\"Trivial\", c=:blue, kwargs...)","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"This time, the Wiki and Merwe parameters are much more spread out than the Trivial parameters. When we propagate these points through the function:","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"wxs2 = f.(wxs)\nmxs2 = f.(mxs)\ntxs2 = f.(txs)\nunscentedplot(wxs2, wpars; lab=\"Wiki\", c=:green, kwargs...)\nunscentedplot!(mxs2, mpars; lab=\"Merwe\", c=:red, kwargs...)\nunscentedplot!(txs2, tpars; lab=\"Trivial\", c=:blue, kwargs...)","category":"page"},{"location":"ut/","page":"Unscented transform","title":"Unscented transform","text":"we see that the Wiki and Merwe parameters produced a posterior mean close to zero, while the Trivial parameters are more positively skewed.","category":"page"},{"location":"measurement_models/#measurement_models","page":"Multiple measurement models","title":"Measurement models","text":"","category":"section"},{"location":"measurement_models/","page":"Multiple measurement models","title":"Multiple measurement models","text":"The Kalman-type filters","category":"page"},{"location":"measurement_models/","page":"Multiple measurement models","title":"Multiple measurement models","text":"KalmanFilter\nExtendedKalmanFilter\nUnscentedKalmanFilter","category":"page"},{"location":"measurement_models/","page":"Multiple measurement models","title":"Multiple measurement models","text":"each come with their own built-in measurement model, e.g., the standard KalmanFilter uses the linear measurement model y = Cx + Du + e, while the ExtendedKalmanFilter and UnscentedKalmanFilter use the nonlinear measurement model y = h(xupt) + e or y = h(xupte). For covariance propagation, the ExtendedKalmanFilter uses linearization to approximate the nonlinear measurement model, while the UnscentedKalmanFilter uses the unscented transform.","category":"page"},{"location":"measurement_models/","page":"Multiple measurement models","title":"Multiple measurement models","text":"It is sometimes useful to mix and match dynamics and measurement models. For example, using the unscented transform from the UKF for the dynamics update (predict!), but the linear measurement model from the standard KalmanFilter for the measurement update (correct!) if the measurement model is linear.","category":"page"},{"location":"measurement_models/","page":"Multiple measurement models","title":"Multiple measurement models","text":"This is possible by constructing a filter with an explicitly created measurement model. The available measurement models are","category":"page"},{"location":"measurement_models/","page":"Multiple measurement models","title":"Multiple measurement models","text":"LinearMeasurementModel performs linear propagation of covariance (as is done in KalmanFilter).\nEKFMeasurementModel uses linearization to propagate covariance (as is done in ExtendedKalmanFilter).\nIEKFMeasurementModel uses iterated linearization to propagate covariance (as is done in IteratedExtendedKalmanFilter).\nUKFMeasurementModel uses the unscented transform to propagate covariance (as is done in UnscentedKalmanFilter).\nCompositeMeasurementModel combines multiple measurement models.","category":"page"},{"location":"measurement_models/#Constructing-a-filter-with-a-custom-measurement-model","page":"Multiple measurement models","title":"Constructing a filter with a custom measurement model","text":"","category":"section"},{"location":"measurement_models/","page":"Multiple measurement models","title":"Multiple measurement models","text":"Constructing a Kalman-type filter automatically creates a measurement model of the corresponding type, given the functions/matrices passed to the filter constructor. To construct a filter with a non-standard measurement model, e.g., and UKF with a KF measurement model, manually create the desired measurement model and pass it as the second argument to the constructor. For example, to construct an UKF with a linear measurement model, we do","category":"page"},{"location":"measurement_models/","page":"Multiple measurement models","title":"Multiple measurement models","text":"using LowLevelParticleFilters, LinearAlgebra\nnx = 100    # Dimension of state\nnu = 2      # Dimension of input\nny = 90     # Dimension of measurements\n\n# Define linear state-space system\nconst __A = 0.1*randn(nx, nx)\nconst __B = randn(nx, nu)\nconst __C = randn(ny,nx)\nfunction dynamics_ip(dx,x,u,p,t)\n    # __A*x .+ __B*u\n    mul!(dx, __A, x)\n    mul!(dx, __B, u, 1.0, 1.0)\n    nothing\nend\nfunction measurement_ip(y,x,u,p,t)\n    # __C*x\n    mul!(y, __C, x)\n    nothing\nend\n\nR1 = I(nx)\nR2 = I(ny)\n\nmm_kf = LinearMeasurementModel(__C, 0, R2; nx, ny)\nukf = UnscentedKalmanFilter(dynamics_ip, mm_kf, R1; ny, nu)","category":"page"},{"location":"measurement_models/","page":"Multiple measurement models","title":"Multiple measurement models","text":"When we create the filter with the custom measurement model, we do not pass the arguments that are associated with the measurement model to the filter constructor, i.e., we do not pass any measurement function, and not the measurement covariance matrix R_2.","category":"page"},{"location":"measurement_models/#Sensor-fusion:-Using-several-different-measurement-models","page":"Multiple measurement models","title":"Sensor fusion: Using several different measurement models","text":"","category":"section"},{"location":"measurement_models/","page":"Multiple measurement models","title":"Multiple measurement models","text":"Above we constructed a filter with a custom measurement model, we can also pass a custom measurement model when we call correct!. This may be useful when, e.g., performing sensor fusion with sensors operating at different sample rates, or when parts of the measurement model are linear, and other parts are nonlinear.","category":"page"},{"location":"measurement_models/","page":"Multiple measurement models","title":"Multiple measurement models","text":"The following example instantiates three different filters and three different measurement models. Each filter is updated with each measurement model, demonstrating that any combination of filter and measurement model can be used together.","category":"page"},{"location":"measurement_models/","page":"Multiple measurement models","title":"Multiple measurement models","text":"using LowLevelParticleFilters, LinearAlgebra\nnx = 10    # Dimension of state\nnu = 2     # Dimension of input\nny = 9     # Dimension of measurements\n\n# Define linear state-space system\nconst __A = 0.1*randn(nx, nx)\nconst __B = randn(nx, nu)\nconst __C = randn(ny,nx)\nfunction dynamics_ip(dx,x,u,p,t)\n    # __A*x .+ __B*u\n    mul!(dx, __A, x)\n    mul!(dx, __B, u, 1.0, 1.0)\n    nothing\nend\nfunction measurement_ip(y,x,u,p,t)\n    # __C*x\n    mul!(y, __C, x)\n    nothing\nend\n\nR1 = I(nx) # Covariance matrices\nR2 = I(ny)\n\n# Construct three different filters\nkf  = KalmanFilter(__A, __B, __C, 0, R1, R2)\nukf = UnscentedKalmanFilter(dynamics_ip, measurement_ip, R1, R2; ny, nu)\nekf = ExtendedKalmanFilter(dynamics_ip, measurement_ip, R1, R2; nu)\n\n# Simulate some data\nT    = 200 # Number of time steps\nU = [randn(nu) for _ in 1:T]\nx,u,y = LowLevelParticleFilters.simulate(kf, U) # Simulate trajectory using the model in the filter\n\n# Construct three different measurement models\nmm_kf = LinearMeasurementModel(__C, 0, R2; nx, ny)\nmm_ekf = EKFMeasurementModel{Float64, true}(measurement_ip, R2; nx, ny)\nmm_ukf = UKFMeasurementModel{Float64, true, false}(measurement_ip, R2; nx, ny)\n\n\nmms = [mm_kf, mm_ekf, mm_ukf]\nfilters = [kf, ekf, ukf]\n\nfor mm in mms, filter in filters\n    @info \"Updating $(nameof(typeof(filter))) with measurement model $(nameof(typeof(mm)))\"\n    correct!(filter, mm, u[1], y[1]) # Pass the measurement model as the second argument to the correct! function if not using the measurement model built into the filter\nend\nnothing # hide","category":"page"},{"location":"measurement_models/","page":"Multiple measurement models","title":"Multiple measurement models","text":"Since the dynamics in this particular example is in fact linear, we should get identical results for all three filters.","category":"page"},{"location":"measurement_models/","page":"Multiple measurement models","title":"Multiple measurement models","text":"using Test\n@test kf.x ≈ ekf.x ≈ ukf.x\n@test kf.R ≈ ekf.R ≈ ukf.R","category":"page"},{"location":"measurement_models/#Video-tutorial","page":"Multiple measurement models","title":"Video tutorial","text":"","category":"section"},{"location":"measurement_models/","page":"Multiple measurement models","title":"Multiple measurement models","text":"A video demonstrating the use of multiple measurement models in a sensor-fusion context is available on YouTube:","category":"page"},{"location":"measurement_models/","page":"Multiple measurement models","title":"Multiple measurement models","text":"<iframe style=\"height: 315px; width: 560px\" src=\"https://www.youtube.com/embed/BLsJrW5XXcg?si=bkob76-uJj27-S80\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","category":"page"},{"location":"api/#Exported-functions-and-types","page":"API","title":"Exported functions and types","text":"","category":"section"},{"location":"api/#Index","page":"API","title":"Index","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"api/#LowLevelParticleFilters.AdvancedParticleFilter-Tuple{Integer, Function, Function, Any, Any, Any}","page":"API","title":"LowLevelParticleFilters.AdvancedParticleFilter","text":"AdvancedParticleFilter(N::Integer, dynamics::Function, measurement::Function, measurement_likelihood, dynamics_density, initial_density; p = NullParameters(), threads = false, kwargs...)\n\nThis type represents a standard particle filter but affords extra flexibility compared to the ParticleFilter type, e.g., non-additive noise in the dynamics and measurement functions.\n\nSee the docs for more information: https://baggepinnen.github.io/LowLevelParticleFilters.jl/stable/#AdvancedParticleFilter-1\n\nArguments:\n\nN: Number of particles\ndynamics: A discrete-time dynamics function (x, u, p, t, noise=false) -> x⁺. It's important that the noise argument defaults to false.\nmeasurement: A measurement function (x, u, p, t, noise=false) -> y. It's important that the noise argument defaults to false.\nmeasurement_likelihood: A function (x, u, y, p, t)->logl to evaluate the log-likelihood of a measurement.\ndynamics_density: This field is not used by the advanced filter and can be set to nothing.\ninitial_density: The distribution of the initial state.\nthreads: use threads to propagate particles in parallel. Only activate this if your dynamics is thread-safe. SeeToDee.SimpleColloc is not thread-safe by default due to the use of internal caches, but SeeToDee.Rk4 is.\n\nExtended help\n\nMultiple measurement models\n\nThe measurement_likelihood function is used to evaluate the likelihood of a measurement. If you have multiple sensors and want to perform individual correct! steps for each, call correct!(..., g = custom_likelihood_function).\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.AuxiliaryParticleFilter-Tuple","page":"API","title":"LowLevelParticleFilters.AuxiliaryParticleFilter","text":"AuxiliaryParticleFilter(args...; kwargs...)\n\nTakes exactly the same arguments as ParticleFilter, or an instance of ParticleFilter.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.CompositeMeasurementModel-Tuple{Any, Vararg{Any}}","page":"API","title":"LowLevelParticleFilters.CompositeMeasurementModel","text":"CompositeMeasurementModel(model1, model2, ...)\n\nA composite measurement model that combines multiple measurement models. This model acts as all component models concatenated. The tuple returned from correct! will be\n\nll: The sum of the log-likelihood of all component models\ne: The concatenated innovation vector\nS: A vector of the innovation covariance matrices of the component models\nSᵪ: A vector of the Cholesky factorizations of the innovation covariance matrices of the component models\nK: A vector of the Kalman gains of the component models\n\nIf all sensors operate on at the same rate, and all measurement models are of the same type, it's more efficient to use a single measurement model with a vector-valued measurement function.\n\nFields:\n\nmodels: A tuple of measurement models\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.EKFMeasurementModel-Union{Tuple{IPM}, NTuple{4, Any}, NTuple{5, Any}} where IPM","page":"API","title":"LowLevelParticleFilters.EKFMeasurementModel","text":"EKFMeasurementModel{IPM}(measurement, R2, ny, Cjac, cache = nothing)\n\nA measurement model for the Extended Kalman Filter.\n\nArguments:\n\nIPM: A boolean indicating if the measurement function is inplace\nmeasurement: The measurement function y = h(x, u, p, t)\nR2: The measurement noise covariance matrix\nny: The number of measurement variables\nCjac: The Jacobian of the measurement function Cjac(x, u, p, t). If none is provided, ForwardDiff will be used.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.EKFMeasurementModel-Union{Tuple{M}, Tuple{IPM}, Tuple{T}, Tuple{M, Any}} where {T, IPM, M}","page":"API","title":"LowLevelParticleFilters.EKFMeasurementModel","text":"EKFMeasurementModel{T,IPM}(measurement::M, R2; nx, ny, Cjac = nothing)\n\nT is the element type used for arrays\nIPM is a boolean indicating if the measurement function is inplace\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.ExtendedKalmanFilter","page":"API","title":"LowLevelParticleFilters.ExtendedKalmanFilter","text":"ExtendedKalmanFilter(kf, dynamics, measurement; Ajac, Cjac)\nExtendedKalmanFilter(dynamics, measurement, R1,R2,d0=MvNormal(Matrix(R1)); nu::Int, p = NullParameters(), α = 1.0, check = true)\n\nA nonlinear state estimator propagating uncertainty using linearization.\n\nThe constructor to the extended Kalman filter takes dynamics and measurement functions, and either covariance matrices, or a KalmanFilter. If the former constructor is used, the number of inputs to the system dynamics, nu, must be explicitly provided with a keyword argument.\n\nBy default, the filter will internally linearize the dynamics using ForwardDiff. User provided Jacobian functions can be provided as keyword arguments Ajac and Cjac. These functions should have the signature (x,u,p,t)::AbstractMatrix where x is the state, u is the input, p is the parameters, and t is the time.\n\nThe dynamics and measurement function are on the following form\n\nx(t+1) = dynamics(x, u, p, t) + w\ny      = measurement(x, u, p, t) + e\n\nwhere w ~ N(0, R1), e ~ N(0, R2) and x(0) ~ d0\n\nThe matrices R1, R2 can be time varying such that, e.g., R1[:, :, t] contains the R_1 matrix at time index t. They can also be given as functions on the form\n\nRfun(x, u, p, t) -> R\n\nThis allows for, e.g., handling functions where the dynamics disturbance w is an input argument to the function, by linearizing the dynamics w.r.t. the disturbance input in a function for R_1, like this (assuming the dynamics have the function signalture f(x, u, p, t, w)):\n\nfunction R1fun(x,u,p,t)\n    Bw = ForwardDiff.jacobian(w->f(x, u, p, t, w), zeros(length(w)))\n    Bw * R1 * Bw'\nend\n\nWhen providing functions, the dimensions of the state, input and output, nx, nu, ny must be provided as keyword arguments to the ExtendedKalmanFilter constructor since these cannot be inferred from the function signature. For maximum performance, provide statically sized matrices from StaticArrays.jl\n\nSee also UnscentedKalmanFilter which is typically more accurate than ExtendedKalmanFilter. See KalmanFilter for detailed instructions on how to set up a Kalman filter kf.\n\n\n\n\n\n","category":"type"},{"location":"api/#LowLevelParticleFilters.IEKFMeasurementModel-Union{Tuple{IPM}, NTuple{4, Any}, NTuple{5, Any}, NTuple{6, Any}, NTuple{7, Any}, NTuple{8, Any}} where IPM","page":"API","title":"LowLevelParticleFilters.IEKFMeasurementModel","text":"IEKFMeasurementModel{IPM}(measurement, R2, ny, Cjac, cache = nothing)\n\nA measurement model for the Iterated Extended Kalman Filter.\n\nArguments:\n\nIPM: A boolean indicating if the measurement function is inplace\nmeasurement: The measurement function y = h(x, u, p, t)\nR2: The measurement noise covariance matrix\nny: The number of measurement variables\nCjac: The Jacobian of the measurement function Cjac(x, u, p, t). If none is provided, ForwardDiff will be used.\nstep: The step size in the Gauss-Newton method. Should be Float64 between 0 and 1.\nmaxiters: The maximum number of iterations of the Gauss-Newton method inside the IEKF\nepsilon: The convergence criterion for the Gauss-Newton method inside the IEKF\ncache: A cache for the Jacobian\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.IEKFMeasurementModel-Union{Tuple{M}, Tuple{IPM}, Tuple{T}, Tuple{M, Any}} where {T, IPM, M}","page":"API","title":"LowLevelParticleFilters.IEKFMeasurementModel","text":"IEKFMeasurementModel{T,IPM}(measurement::M, R2; nx, ny, Cjac = nothing, step = 1.0, maxiters = 10, epsilon = 1e-8)\n\nT is the element type used for arrays\nIPM is a boolean indicating if the measurement function is inplace\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.IMM-Tuple{Any, AbstractMatrix, AbstractVector}","page":"API","title":"LowLevelParticleFilters.IMM","text":"IMM(models, P, μ; check = true, p = NullParameters(), interact = true)\n\nInteracting Multiple Model (IMM) filter. This filter is a combination of multiple Kalman-type filters, each with its own state and covariance. The IMM filter is a probabilistically weighted average of the states and covariances of the individual filters. The weights are determined by the probability matrix P and the mixing probabilities μ.\n\nThis implmentation allows for any combination of Kalman-type estimators to be used in the internal ensemble of models, and is not limited to linear estimators. This class of models encompasses others, such as \n\nJump Markov Linear Systems (JMLS)\nMultiple-model filters (interactivity can be turned off by setting interact=false)\nMultiple Hypothesis Tracking (MHT)\n\nwarning: Experimental\nThis filter is currently considered experimental and the user interface may change in the future without respecting semantic versioning.\n\nIn addition to the predict! and correct! steps, the IMM filter has an interact! method that updates the states and covariances of the individual filters based on the mixing probabilities. The combine! method combines the states and covariances of the individual filters into a single state and covariance. These four functions are typically called in either of the orders\n\ncorrect!, combine!, interact!, predict! (as is done in update!)\ninteract!, predict!, correct!, combine! (as is done in the reference cited below)\n\nThese two orders are cyclic permutations of each other, and the order used in update! is chosen to align with the order used in the other filters, where the initial condition is corrected using the first measurement, i.e., we assume the first measurement updates x(0-1) to x(00).\n\nThe initial (combined) state and covariance of the IMM filter is made up of the weighted average of the states and covariances of the individual filters. The weights are the initial mixing probabilities μ.\n\nRef: \"Interacting multiple model methods in target tracking: a survey\", E. Mazor; A. Averbuch; Y. Bar-Shalom; J. Dayan\n\nArguments:\n\nmodels: An array of Kalman-type filters, such as KalmanFilter, ExtendedKalmanFilter, UnscentedKalmanFilter, etc. The state of each model must have the same meaning, such that forming a weighted average makes sense.\nP: The mode-transition probability matrix. P[i,j] is the probability of transitioning from mode i to mode j (each row must sum to one).\nμ: The initial mixing probabilities. μ[i] is the probability of being in mode i at the initial contidion (must sum to one).\ncheck: If true, check that the inputs are valid. If false, skip the checks.\np: Parameters for the filter. NOTE: this p is shared among all internal filters. The internal p of each filter will be overridden by this one.\ninteract: If true, the filter will run the interaction as part of update! and forward_trajectory. If false, the filter will not run the interaction step. This choice can be overridden by passing the keyword argument interact to the respective functions.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.KalmanFilter","page":"API","title":"LowLevelParticleFilters.KalmanFilter","text":"KalmanFilter(A,B,C,D,R1,R2,d0=MvNormal(R1); p = NullParameters(), α=1, check=true)\n\nThe matrices A,B,C,D define the dynamics\n\nx' = Ax + Bu + w\ny  = Cx + Du + e\n\nwhere w ~ N(0, R1), e ~ N(0, R2) and x(0) ~ d0\n\nThe matrices can be time varying such that, e.g., A[:, :, t] contains the A matrix at time index t. They can also be given as functions on the form\n\nAfun(x, u, p, t) -> A\n\nWhen providing functions, the dimensions of the state, input and output, nx, nu, ny must be provided as keyword arguments to the KalmanFilter constructor since these cannot be inferred from the function signature. For maximum performance, provide statically sized matrices from StaticArrays.jl\n\nα is an optional \"forgetting factor\", if this is set to a value > 1, such as 1.01-1.2, the filter will, in addition to the covariance inflation due to R_1, exhibit \"exponential forgetting\" similar to a Recursive Least-Squares (RLS) estimator. It is thus possible to get a RLS-like algorithm by setting R_1=0 R_2 = 1α and α  1 (α is the inverse of the traditional RLS parameter α = 1λ). The exact form of the covariance update is\n\nR(t+1t) = α AR(t)A^T + R_1\n\nIf check = true (default) the function will check that the eigenvalues of A are less than 2 in absolute value. Large eigenvalues may be an indication that the system matrices are representing a continuous-time system and the user has forgotten to discretize it. Turn off this check by setting check = false.\n\nTutorials on Kalman filtering\n\nThe tutorial \"How to tune a Kalman filter\" details how to figure out appropriate covariance matrices for the Kalman filter, as well as how to add disturbance models to the system model. See also the tutorial in the documentation\n\n\n\n\n\n","category":"type"},{"location":"api/#LowLevelParticleFilters.KalmanFilteringSolution","page":"API","title":"LowLevelParticleFilters.KalmanFilteringSolution","text":"KalmanFilteringSolution{Tx,Txt,TR,TRt,Tll} <: AbstractFilteringSolution\n\nFields\n\nx: predictions x(t+1t) (plotted if plotx=true)\nxt: filtered estimates x(tt) (plotted if plotxt=true)\nR: predicted covariance matrices R(t+1t) (plotted if plotR=true)\nRt: filter covariances R(tt) (plotted if plotRt=true)\nll: loglikelihood\ne: prediction errors e(tt-1) = y - y(tt-1) (plotted if plote=true)\n\nPlot\n\nThe solution object can be plotted\n\nplot(sol, plotx=true, plotxt=true, plotR=true, plotRt=true, plote=true, plotu=true, ploty=true, plotyh=true, plotyht=true, name=\"\")\n\nwhere\n\nplotx: Plot the predictions x(t|t-1)\nplotxt: Plot the filtered estimates x(t|t)\nplotR: Plot the predicted covariances R(t|t-1) as ribbons at ±2σ (1.96 σ to be precise)\nplotRt: Plot the filter covariances R(t|t) as ribbons at ±2σ (1.96 σ to be precise)\nplote: Plot the prediction errors e(t|t-1) = y - ŷ(t|t-1)\nplotu: Plot the input\nploty: Plot the measurements\nplotyh: Plot the predicted measurements ŷ(t|t-1)\nplotyht: Plot the filtered measurements ŷ(t|t)\nname: a string that is prepended to the labels of the plots, which is useful when plotting multiple solutions in the same plot.\n\nTo modify the signal names used in legend entries, construct an instance of SignalNames and pass this to the filter (or directly to the plot command) using the names keyword argument.\n\n\n\n\n\n","category":"type"},{"location":"api/#LowLevelParticleFilters.KalmanSmoothingSolution","page":"API","title":"LowLevelParticleFilters.KalmanSmoothingSolution","text":"struct KalmanSmoothingSolution\n\nA structure representing the solution to a Kalman smoothing problem.\n\nFields\n\nsol: A solution object containing the results of the filtering process.\nxT: The smoothed state estimate.\nRT: The smoothed state covariance.\n\nThe solution object can be plotted\n\nplot(sol; plotxT=true, plotRT=true, kwargs...)\n\nwhere\n\nplotxT: Plot the smoothed estimates x(t|T)\nplotRT: Plot the smoothed covariances R(t|T) as ribbons at ±2σ (1.96 σ to be precise)\nThe rest of the keyword arguments are the same as for KalmanFilteringSolution\n\nWhen plotting a smoothing solution, the filtering solution is also plotted. The same keyword arguments as for KalmanFilteringSolution may be used to control which signals are plotted\n\n\n\n\n\n","category":"type"},{"location":"api/#LowLevelParticleFilters.LinearMeasurementModel","page":"API","title":"LowLevelParticleFilters.LinearMeasurementModel","text":"LinearMeasurementModel{CT, DT, RT, CAT}\n\nA linear measurement model y = C*x + D*u + e.\n\nFields:\n\nC \nD\nR2: The measurement noise covariance matrix\nny: The number of measurement variables\n\n\n\n\n\n","category":"type"},{"location":"api/#LowLevelParticleFilters.MerweParams","page":"API","title":"LowLevelParticleFilters.MerweParams","text":"MerweParams(; α = 1.0, β = 2.0, κ = 0.0)\nMerweParams(; ακ = 1.0, β = 2.0) # Simplified interface with only one parameter for ακ\n\nUnscented transform parameters suggested by van der Merwe et al.\n\nα: Scaling parameter (0,1] for the spread of the sigma points. Reduce α to reduce the spread.\nβ: Incorporates prior knowledge of the distribution of the state.\nκ: Secondary scaling parameter that is usually set to 0. Increase κ to increase the spread of the sigma points.\n\nIf α^2 (L + κ)  L where L is the dimension of the sigma points, the center mean weight is negative. This is allowed, but may in some cases lead to an indefinite covariance matrix.\n\nThe spread of the points are α^2 (L + κ) where L is the dimension of each point. Visualize the spread by\n\nusing Plots\nμ = [0.0, 0.0]\nΣ = [1.0 0.0; 0.0 1.0]\npars = LowLevelParticleFilters.MerweParams(α = 1e-3, β = 2.0, κ = 0.0)\nxs = LowLevelParticleFilters.sigmapoints(μ, Σ, pars)\nunscentedplot(xs, pars)\n\nA simplified tuning rule \n\nIf a decrease in the spread of the sigma points is desired, use κ = 0 and α  1.\nIf an increase in the spread of the sigma points is desired, use κ  0 and α = 1.\n\nThis rule may be used when using the interface with only a single function argument ακ. See Nielsen, K. et al., 2021, \"UKF Parameter Tuning for Local Variation Smoothing\" for more details.\n\nSee also WikiParams and TrivialParams\n\n\n\n\n\n","category":"type"},{"location":"api/#LowLevelParticleFilters.ParticleFilter-Tuple{Integer, Function, Function, Any, Any, Any}","page":"API","title":"LowLevelParticleFilters.ParticleFilter","text":"ParticleFilter(N::Integer, dynamics, measurement, dynamics_density, measurement_density, initial_density; threads = false, p = NullParameters(), kwargs...)\n\nSee the docs for more information: https://baggepinnen.github.io/LowLevelParticleFilters.jl/stable/#Particle-filter-1\n\nArguments:\n\nN: Number of particles\ndynamics: A discrete-time dynamics function (x, u, p, t) -> x⁺\nmeasurement: A measurement function (x, u, p, t) -> y\ndynamics_density: A probability-density function for additive noise in the dynamics. Use AdvancedParticleFilter for non-additive noise.\nmeasurement_density: A probability-density function for additive measurement noise. Use AdvancedParticleFilter for non-additive noise.\ninitial_density: Distribution of the initial state.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.ParticleFilteringSolution","page":"API","title":"LowLevelParticleFilters.ParticleFilteringSolution","text":"ParticleFilteringSolution{F, Tu, Ty, Tx, Tw, Twe, Tll} <: AbstractFilteringSolution\n\nFields:\n\nf: The filter used to produce the solution.\nu: Input\ny: Output / measurements\nx: Particles, the size of this array is (N,T), where N is the number of particles and T is the number of time steps. Each element represents a weighted state hypothesis with weight given by we.\nw: Weights (log space). These are used for internal computations.\nwe: Weights (exponentiated / original space). These are the ones to use to compute weighted means etc., they sum to one for each time step.\nll: Log likelihood\n\nPlot\n\nThe solution object can be plotted\n\nplot(sol; nbinsy=30, xreal=nothing, dim=nothing, ploty=true, q=nothing)\n\nBy default, a weighted 2D histogram is plotted, one for each state variable. If a vector of quantiles are provided in q, the quantiles are plotted instead of the histogram. If xreal is provided, the true state is plotted as a scatter plot on top of the histogram. If dim is provided, only the specified dimension is plotted. If ploty is true, the measurements are plotted as well.\n\n\n\n\n\n","category":"type"},{"location":"api/#LowLevelParticleFilters.RBMeasurementModel","page":"API","title":"LowLevelParticleFilters.RBMeasurementModel","text":"RBMeasurementModel{IPM}(measurement, R2, ny)\n\nA measurement model for the Rao-Blackwellized particle filter.\n\nFields:\n\nmeasurement: The contribution from the nonlinar state to the output, g in y = g(x^n u p t) + C x^l + e\nR2: The probability distribution of the measurement noise. If C == 0, this may be any distribution, otherwise it must be an instance of MvNormal or SimpleMvNormal.\nny: The number of outputs\n\n\n\n\n\n","category":"type"},{"location":"api/#LowLevelParticleFilters.RBPF-Union{Tuple{AUGD}, Tuple{IPM}, Tuple{IPD}, Tuple{Int64, Any, Any, LowLevelParticleFilters.AbstractMeasurementModel, Any, Any}} where {IPD, IPM, AUGD}","page":"API","title":"LowLevelParticleFilters.RBPF","text":"RBPF{IPD,IPM,AUGD}(N::Int, kf, dynamics, nl_measurement_model::AbstractMeasurementModel, R1n, d0n; An, nu::Int, Ts=1.0, p=NullParameters(), names, rng = Xoshiro(), resample_threshold=0.1)\n\nRao-Blackwellized particle filter, also called \"Marginalized particle filter\". The filter is effectively a particle filter where each particle is a Kalman filter that is responsible for the estimation of a linear sub structure.\n\nwarning: Experimental\nThis filter is currently considered experimental and the user interface may change in the future without respecting semantic versioning.\n\nThe filter assumes that the dynamics follow \"model 2\" in the reference below, i.e., the dynamics is described by\n\n beginalign\n     x_t+1^n = f_n(x_t^n u p t) + A_n(x_t^n u p t) x_t^l + w_t^n quad w_t^n sim mathcalN(0 R_1^n) \n     x_t+1^l = A() x_t^l + Bu + w_t^l quad w_t^l sim mathcalN(0 R_1^l) \n     y_t = g(x_t^n u p t) + C() x_t^l + e_t quad e_t sim mathcalN(0 R_2)\n endalign\n\nwhere x^n is a subset of the state that has nonlinear dynamics, and x^l is the linear part of the state. The entire state vector is represented by a special type RBParticle that behaves like the vector [xn; xl], but stores xn, xl and the covariance R or xl separately.\n\nN: Number of particles\nkf: The internal Kalman filter that will be used for the linear part. This encodes the dynamics of the linear subspace. The matrices A B C D R_1^l of the Kalman filter may be functions of x, u, p, t that return a matrix. The state x received by such functions is of type RBParticle with the fields xn and xl.\ndynamics: The nonlinear part f_n of the dynamics of the nonlinear substate f(xn, u, p, t)\nnl_measurement_model: An instance of RBMeasurementModel that stores g and the measurement noise distribution R_2.\nR1n: The noise distribution of the nonlinear state dynamics, this may be a covariance matrix or a distribution. If An = nothing, this may be any distribution, otherwise it must be an instance of MvNormal or SimpleMvNormal.\nd0n: The initial distribution of the nonlinear state x_0^n.\nAn: The matrix that describes the linear effect on the nonlinear state, i.e., A_n x^l. This may be a matrix or a function of x u p t that returns a matrix. Pass An = nothing if there is no linear effect on the nonlinear state. The x received by such a function is of type RBParticle with the fields xn and xl.\nnu: The number of control inputs\nTs: The sampling time\np: Parameters\nnames: Signal names, an instance of SignalNames\nrng: Random number generator\nresample_threshold: The threshold for resampling\n\nBased on the article \"Marginalized Particle Filters for Mixed Linear/Nonlinear State-space Models\" by Thomas Schön, Fredrik Gustafsson, and Per-Johan Nordlund\n\nExtended help\n\nThe paper contains an even more general model, where the linear part is linearly affected by the nonlinear state. It further details a number of special cases in which possible simplifications arise. \n\nIf C == 0 and D == 0, the measurement is not used by the Kalman filter and we may thus have an arbitrary probability distribution for the measurement noise.\nIf An == 0, the nonlinear state is not affected by the linear state and we may have an arbitrary probability distribution for the nonlinear state noise R1n. Otherwise R1n must be Gaussian.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.RBParticle-Tuple{Any, Any, Any}","page":"API","title":"LowLevelParticleFilters.RBParticle","text":"RBParticle(xn, xl, R) <: AbstractVector\n\nA struct that represents the state of a Rao-Blackwellized particle filter. The struct is an abstract vector, and when indexed like a vector it behaves as [xn; xl]. To access nonlinear or linear substate individually, access the fields xn and xl.\n\nArguments:\n\nxn: The nonlinear state vector\nxl: The linear state vector\nR: The covariance matrix for the linear state\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.SignalNames","page":"API","title":"LowLevelParticleFilters.SignalNames","text":"SignalNames(; x, u, y, name)\n\nA structure representing the names of the signals in a system.\n\nx::Vector{String}: Names of the state variables\nu::Vector{String}: Names of the input variables\ny::Vector{String}: Names of the output variables\nname::String: Name of the system\n\n\n\n\n\n","category":"type"},{"location":"api/#LowLevelParticleFilters.SignalNames-Tuple{SignalNames, Any}","page":"API","title":"LowLevelParticleFilters.SignalNames","text":"SignalNames(sn::SignalNames, name)\n\nCopy the SignalNames structure and change the name of the system.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.SqKalmanFilter","page":"API","title":"LowLevelParticleFilters.SqKalmanFilter","text":"SqKalmanFilter(A,B,C,D,R1,R2,d0=MvNormal(R1); p = NullParameters(), α=1)\n\nA standard Kalman filter on square-root form. This filter may have better numerical performance when the covariance matrices are ill-conditioned.\n\nThe matrices A,B,C,D define the dynamics\n\nx' = Ax + Bu + w\ny  = Cx + Du + e\n\nwhere w ~ N(0, R1), e ~ N(0, R2) and x(0) ~ d0\n\nThe matrices can be time varying such that, e.g., A[:, :, t] contains the A matrix at time index t. They can also be given as functions on the form\n\nAfun(x, u, p, t) -> A\n\nThe internal fields storing covariance matrices are for this filter storing the upper-triangular Cholesky factor.\n\nα is an optional \"forgetting factor\", if this is set to a value > 1, such as 1.01-1.2, the filter will, in addition to the covariance inflation due to R_1, exhibit \"exponential forgetting\" similar to a Recursive Least-Squares (RLS) estimator. It is thus possible to get a RLS-like algorithm by setting R_1=0 R_2 = 1α and α  1 (α is the inverse of the traditional RLS parameter α = 1λ). The form of the covariance update is\n\nR(t+1t) = α AR(t)A^T + R_1\n\nRef: \"A Square-Root Kalman Filter Using Only QR Decompositions\", Kevin Tracy https://arxiv.org/abs/2208.06452\n\n\n\n\n\n","category":"type"},{"location":"api/#LowLevelParticleFilters.TrivialParams","page":"API","title":"LowLevelParticleFilters.TrivialParams","text":"TrivialParams()\n\nUnscented transform parameters representing a trivial choice of weights, where all weights are equal.\n\nSee also WikiParams and MerweParams\n\n\n\n\n\n","category":"type"},{"location":"api/#LowLevelParticleFilters.UKFMeasurementModel-Union{Tuple{AUGM}, Tuple{IPM}, NTuple{8, Any}, NTuple{9, Any}, NTuple{10, Any}} where {IPM, AUGM}","page":"API","title":"LowLevelParticleFilters.UKFMeasurementModel","text":"UKFMeasurementModel{inplace_measurement,augmented_measurement}(measurement, R2, ny, ne, innovation, mean, cov, cross_cov, weight_params, cache = nothing)\n\nA measurement model for the Unscented Kalman Filter.\n\nArguments:\n\nmeasurement: The measurement function y = h(x, u, p, t)\nR2: The measurement noise covariance matrix\nny: The number of measurement variables\nne: If augmented_measurement is true, the number of measurement noise variables\ninnovation(y::AbstractVector, yh::AbstractVector) where the arguments represent (measured output, predicted output)\nmean(ys::AbstractVector{<:AbstractVector}): computes the mean of the vector of vectors of output sigma points.\ncov(ys::AbstractVector{<:AbstractVector}, y::AbstractVector): computes the covariance matrix of the output sigma points.\ncross_cov(xs::AbstractVector{<:AbstractVector}, x::AbstractVector, ys::AbstractVector{<:AbstractVector}, y::AbstractVector, W::UKFWeights) where the arguments represents (state sigma points, mean state, output sigma points, mean output, weights). The function should return the weighted cross-covariance matrix between the state and output sigma points.\nweight_params: A type that holds the parameters for the unscented-transform weights. See UnscentedKalmanFilter and Docs: Unscented transform for more information.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.UKFMeasurementModel-Union{Tuple{AUGM}, Tuple{IPM}, Tuple{T}, Tuple{Any, Any}} where {T, IPM, AUGM}","page":"API","title":"LowLevelParticleFilters.UKFMeasurementModel","text":"UKFMeasurementModel{T,IPM,AUGM}(measurement, R2; nx, ny, ne = nothing, innovation = -, mean = weighted_mean, cov = weighted_cov, cross_cov = cross_cov, static = nothing)\n\nT is the element type used for arrays\nIPM is a boolean indicating if the measurement function is inplace\nAUGM is a boolean indicating if the measurement model is augmented\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.UKFWeights","page":"API","title":"LowLevelParticleFilters.UKFWeights","text":"UKFWeights\n\nWeights for the Unscented Transform.\n\nSigmapoints are by convention ordered such that the center (mean) point is first.\n\nFields\n\nwm: center weight when computing mean\nwc: center weight when computing covariance\nwmi: off-center weight when computing mean\nwci: off-center weight when computing covariance\nW: Cholesky weight\n\n\n\n\n\n","category":"type"},{"location":"api/#LowLevelParticleFilters.UnscentedKalmanFilter-Union{Tuple{AUGM}, Tuple{AUGD}, Tuple{IPM}, Tuple{IPD}, Tuple{Any, LowLevelParticleFilters.AbstractMeasurementModel, Any}, Tuple{Any, LowLevelParticleFilters.AbstractMeasurementModel, Any, Any}} where {IPD, IPM, AUGD, AUGM}","page":"API","title":"LowLevelParticleFilters.UnscentedKalmanFilter","text":"UnscentedKalmanFilter(dynamics, measurement, R1, R2, d0=MvNormal(Matrix(R1)); p = NullParameters(), ny, nu, weight_params)\nUnscentedKalmanFilter{IPD,IPM,AUGD,AUGM}(dynamics, measurement_model::AbstractMeasurementModel, R1, d0=SimpleMvNormal(R1); p=NullParameters(), nu, weight_params)\n\nA nonlinear state estimator propagating uncertainty using the unscented transform.\n\nThe dynamics and measurement function are on either of the following forms\n\nx' = dynamics(x, u, p, t) + w\ny  = measurement(x, u, p, t) + e\n\nx' = dynamics(x, u, p, t, w)\ny  = measurement(x, u, p, t, e)\n\nwhere w ~ N(0, R1), e ~ N(0, R2) and x(0) ~ d0. The former (default) assums that the noise is additive and added after the dynamics and measurement updates, while the latter assumes that the dynamics functions take an additional argument corresponding to the noise term. The latter form (sometimes refered to as the \"augmented\" form) is useful when the noise is multiplicative or when the noise is added before the dynamics and measurement updates. See \"Augmented UKF\" below for more details on how to use this form. In both cases should the noise be modeled as discrete-time white noise, see Discretization: Covariance matrices.\n\nThe matrices R1, R2 can be time varying such that, e.g., R1[:, :, t] contains the R_1 matrix at time index t. They can also be given as functions on the form\n\nRfun(x, u, p, t) -> R\n\nFor maximum performance, provide statically sized matrices from StaticArrays.jl\n\nny, nu indicate the number of outputs and inputs.\n\nCustom type of u\n\nThe input u may be of any type, e.g., a named tuple or a custom struct. The u provided in the input data is passed directly to the dynamics and measurement functions, so as long as the type is compatible with the dynamics it will work out. The one exception where this will not work is when calling simulate, which assumes that u is an array.\n\nAugmented UKF\n\nIf the noise is not additive, one may use the augmented form of the UKF. In this form, the dynamics functions take additional input arguments that correspond to the noise terms. To enable this form, the typed constructor\n\nUnscentedKalmanFilter{inplace_dynamics,inplace_measurement,augmented_dynamics,augmented_measurement}(...)\n\nis used, where the Boolean type parameters have the following meaning\n\ninplace_dynamics: If true, the dynamics function operates in-place, i.e., it modifies the first argument in dynamics(dx, x, u, p, t). Default is false.\ninplace_measurement: If true, the measurement function operates in-place, i.e., it modifies the first argument in measurement(y, x, u, p, t). Default is false.\naugmented_dynamics: If true the dynamics function is augmented with an additional noise input w, i.e., dynamics(x, u, p, t, w). Default is false.\naugmented_measurement: If true the measurement function is agumented with an additional noise input e, i.e., measurement(x, u, p, t, e). Default is false. (If the measurement noise has fewer degrees of freedom than the number of measurements, you may failure in Cholesky factorizations, see \"Custom Cholesky factorization\" below).\n\nUse of augmented dynamics incurs extra computational cost. The number of sigma points used is 2L+1 where L is the length of the augmented state vector. Without augmentation, L = nx, with augmentation L = nx + nw and L = nx + ne for dynamics and measurement, respectively.\n\nWeight tuning\n\nThe spread of the sigma points is controlled by weight_params::UTParams. See Docs: Unscented transform for a tutorial. The default is TrivialParams for unweighted sigma points, other options are WikiParams and MerweParams.\n\nSigma-point rejection\n\nFor problems with challenging dynamics, a mechanism for rejection of sigma points after the dynamics update is provided. A function reject(x) -> Bool can be provided through the keyword argument reject that returns true if a sigma point for x(t+1) should be rejected, e.g., if an instability or non-finite number is detected. A rejected point is replaced by the propagated mean point (the mean point cannot be rejected). This function may be provided either to the constructor of the UKF or passed to the predict! function.\n\nCustom measurement models\n\nBy default, standard arithmetic mean and e(y, yh) = y - yh are used as mean and innovation functions.\n\nBy passing and explicitly created UKFMeasurementModel, one may provide custom functions that compute the mean, the covariance and the innovation. This is useful in situations where the state or a measurement lives on a manifold. One may further override the mean and covariance functions for the state sigma points by passing the keyword arguments state_mean and state_cov to the constructor.\n\nstate_mean(xs::AbstractVector{<:AbstractVector}, w::UKFWeights) computes the weighted mean of the vector of vectors of state sigma points.\nstate_cov(xs::AbstractVector{<:AbstractVector}, m, w::UKFWeights) where the first argument represent state sigma points and the second argument, represents the weighted mean of those points. The function should return the covariance matrix of the state sigma points weighted by w.\n\nSee UKFMeasurementModel for more details on how to set up a custom measurement model. Pass the custom measurement model as the second argument to the UKF constructor.\n\nCustom Cholesky factorization\n\nThe UnscentedKalmanFilter supports providing a custom function to compute the Cholesky factorization of the covariance matrices for use in sigma-point generation.\n\nIf either of the following conditions are met, you may experience failure in internal Cholesky factorizations:\n\nThe dynamics noise or measurement noise covariance matrices (R_1 R_2) are singular\nThe measurement is augmented and the measurement noise has fewer degrees of freedom than the number of measurements\n(Under specific technical conditions) The dynamics is augmented and the dynamics noise has fewer degrees of freedom than the number of state variables. The technical conditions are easiest to understand in the linear-systems case, where it corresponds to the Riccati equation associated with the Kalman gain not having a solution. This may happen when the pair (A R1) has uncontrollable modes on the unit circle, for example, when there are integrating modes that are not affected through the noise.\n\nThe error message may look like\n\nERROR: PosDefException: matrix is not positive definite; Factorization failed.\n\nIn such situations, it is advicable to reconsider the noise model and covariance matrices, alternatively, you may provide a custom Cholesky factorization function to the UKF constructor through the keyword argument cholesky!. The function should have the signature cholesky!(A::AbstractMatrix)::Cholesky. A useful alternative factorizaiton when covariance matrices are expected to be singular is cholesky! = R->cholesky!(Positive, Matrix(R)) where the \"positive\" Cholesky factorization is provided by the package PositiveFactorizations.jl, which must be manually installed and loaded by the user.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.WikiParams","page":"API","title":"LowLevelParticleFilters.WikiParams","text":"WikiParams(; α = 1.0, β = 0.0, κ = 1.0)\nWikiParams(; ακ = 1.0, β = 0.0) # Simplified interface with only one parameter for ακ\n\nUnscented transform parameters suggested at Wiki: Kalmanfilter#Sigmapoints.\n\nα: Scaling parameter (0,1] for the spread of the sigma points. Reduce α to reduce the spread.\nβ: Incorporates prior knowledge of the distribution of the state.\nκ: Secondary scaling parameter that is usually set to 3nx/2 or 1. Increase κ to increase the spread of the sigma points.\n\nIf α^2 κ  L where L is the dimension ofthe sigma points, the center mean weight is negative. This is allowed, but may in some cases lead to an indefinite covariance matrix.\n\nThe spread of the points are α^2 κ, that is, independent on the point dimension. Visualize the spread by\n\nusing Plots\nμ = [0.0, 0.0]\nΣ = [1.0 0.0; 0.0 1.0]\npars = LowLevelParticleFilters.WikiParams(α = 1.0, β = 0.0, κ = 1.0)\nxs = LowLevelParticleFilters.sigmapoints(μ, Σ, pars)\nunscentedplot(xs, pars)\n\nA simplified tuning rule \n\nIf a decrease in the spread of the sigma points is desired, use κ = 1 and α  1.\nIf an increase in the spread of the sigma points is desired, use κ  1 and α = 1.\n\nThis rule may be used when using the interface with only a single function argument ακ. See Nielsen, K. et al., 2021, \"UKF Parameter Tuning for Local Variation Smoothing\" for more details.\n\nSee also MerweParams and TrivialParams\n\n\n\n\n\n","category":"type"},{"location":"api/#LowLevelParticleFilters.IteratedExtendedKalmanFilter","page":"API","title":"LowLevelParticleFilters.IteratedExtendedKalmanFilter","text":"IteratedExtendedKalmanFilter(kf, dynamics, measurement; Ajac, Cjac, step, maxiters, epsilon)\nIteratedExtendedKalmanFilter(dynamics, measurement, R1,R2,d0=SimpleMvNormal(Matrix(R1)); nu::Int, ny=size(R2,1), Cjac = nothing, step = 1.0, maxiters=10, epsilon=1e-8)\n\nA nonlinear state estimator propagating uncertainty using linearization. Returns an ExtendedKalmanFilter object but with Gauss-Newton based iterating measurement correction step.\n\nThe constructor to the iterated version of extended Kalman filter takes dynamics and measurement functions, and either covariance matrices, or a KalmanFilter. If the former constructor is used, the number of inputs to the system dynamics, nu, must be explicitly provided with a keyword argument.\n\nBy default, the filter will internally linearize the dynamics using ForwardDiff. User provided Jacobian functions can be provided as keyword arguments Ajac and Cjac. These functions should have the signature (x,u,p,t)::AbstractMatrix where x is the state, u is the input, p is the parameters, and t is the time.\n\nThe dynamics and measurement function are of the following form\n\nx(t+1) = dynamics(x, u, p, t) + w\ny      = measurement(x, u, p, t) + e\n\nwhere w ~ N(0, R1), e ~ N(0, R2) and x(0) ~ d0\n\nstep is the step size for the Gauss-Newton iterations. Float between 0 and 1. Default is 1.0 which should be good enough for most applications. For more challenging applications, a smaller step size might be necessary.\nmaxiters is the maximum number of iterations. Default is 10. Usually a small number of iterations is needed. If higher number is needed, consider using UKF.\nepsilon is the convergence criterion. Default is 1e-8\n\nSee also UnscentedKalmanFilter which is more robust than IteratedExtendedKalmanFilter. See KalmanFilter for detailed instructions on how to set up a Kalman filter kf.\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.combine!-Tuple{IMM}","page":"API","title":"LowLevelParticleFilters.combine!","text":"combine!(imm::IMM)\n\nCombine the models of the IMM filter into a single state imm.x and covariance imm.R. This is done by taking a weighted average of the states and covariances of the individual models, where the weights are the mixing probabilities μ.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.commandplot","page":"API","title":"LowLevelParticleFilters.commandplot","text":"commandplot(pf, u, y, p=parameters(pf); kwargs...)\n\nProduce a helpful plot. For customization options (kwargs...), see ?pplot. After each time step, a command from the user is requested.\n\nq: quit\ns n: step n steps\n\nnote: Note\nThis function requires using Plots to be called before it is used.\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.correct!","page":"API","title":"LowLevelParticleFilters.correct!","text":"ll, e = correct!(pf, u, y, p = parameters(f), t = index(f))\n\nUpdate state/weights based on measurement y,  returns log-likelihood and prediction error (the error is always 0 for particle filters).\n\nExtended help\n\nTo perform separate measurement updates for different sensors, see the \"Measurement models\" in the documentation. For AdvancedParticleFilter, this can be realized by passing a custom measurement_likelihood function as the keyword argument g to correct!, or by calling the lower-level function measurement_equation! with a custom measurement_likelihood.\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.correct!-2","page":"API","title":"LowLevelParticleFilters.correct!","text":"correct!(kf::SqKalmanFilter, u, y, p = parameters(kf), t::Real = index(kf); R2 = get_mat(kf.R2, kf.x, u, p, t))\n\nFor the square-root Kalman filter, a custom provided R2 must be the upper triangular Cholesky factor of the covariance matrix of the measurement noise.\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.correct!-3","page":"API","title":"LowLevelParticleFilters.correct!","text":"(; ll, e, S, Sᵪ, K) = correct!(kf::AbstractKalmanFilter, u, y, p = parameters(kf), t::Integer = index(kf), R2)\n\nThe correct step for a Kalman filter returns not only the log likelihood ll and the prediction error e, but also the covariance of the output S, its Cholesky factor Sᵪ and the Kalman gain K.\n\nIf R2 stored in kf is a function R2(x, u, p, t), this function is evaluated at the state before the correction is performed. The measurement noise covariance matrix R2 stored in the filter object can optionally be overridden by passing the argument R2, in this case R2 must be a matrix.\n\nExtended help\n\nTo perform separate measurement updates for different sensors, see the \"Measurement models\" in the documentation.\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.correct!-Tuple{IMM, Any, Any, Vararg{Any}}","page":"API","title":"LowLevelParticleFilters.correct!","text":"ll, lls, rest = correct!(imm::IMM, u, y, args; kwargs)\n\nThe correct step of the IMM filter corrects each model with the measurements y and control input u. The mixing probabilities imm.μ are updated based on the likelihood of each model given the measurements and the transition probability matrix P.\n\nThe returned tuple consists of the sum of the log-likelihood of all models, the vector of individual log-likelihoods and an array of the rest of the return values from the correct step of each model.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.correct!-Tuple{UnscentedKalmanFilter, Any, Any, Any, Real}","page":"API","title":"LowLevelParticleFilters.correct!","text":"correct!(ukf::UnscentedKalmanFilter{IPD, IPM, AUGD, AUGM}, u, y, p = parameters(ukf), t::Real = index(ukf) * ukf.Ts; R2 = get_mat(ukf.R2, ukf.x, u, p, t), mean, cross_cov, innovation)\n\nThe correction step for an UnscentedKalmanFilter allows the user to override, R2, mean, cross_cov, innovation.\n\nArguments:\n\nu: The input\ny: The measurement\np: The parameters\nt: The current time\nR2: The measurement noise covariance matrix, or a function that returns the covariance matrix (x,u,p,t)->R2.\nmean: The function that computes the weighted mean of the output sigma points.\ncross_cov: The function that computes the weighted cross-covariance of the state and output sigma points.\ninnovation: The function that computes the innovation between the measured output and the predicted output.\n\nExtended help\n\nTo perform separate measurement updates for different sensors, see the \"Measurement models\" in the documentation\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.covplot","page":"API","title":"LowLevelParticleFilters.covplot","text":"covplot(μ, Σ; n_std = 2, dims=1:2)\ncovplot(kf; n_std = 2, dims=1:2)\n\nPlot the covariance ellipse of the state μ and covariance Σ. dims indicate the two dimensions to plot, and defaults to the first two dimensions.\n\nIf a Kalman-type filter is passed, the state and covariance are extracted from the filter.\n\nSee also unscentedplot.\n\nnote: Note\nThis function requires using Plots to be called before it is used.\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.debugplot","page":"API","title":"LowLevelParticleFilters.debugplot","text":"debugplot(pf, u, y, p=parameters(pf); runall=false, kwargs...)\n\nProduce a helpful plot. For customization options (kwargs...), see ?pplot.\n\nrunall=false: if true, runs all time steps befor displaying (faster), if false, displays the plot after each time step.\n\nThe generated plot becomes quite heavy. Initially, try limiting your input to 100 time steps to verify that it doesn't crash.\n\nnote: Note\nThis function requires using Plots to be called before it is used.\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.densityplot","page":"API","title":"LowLevelParticleFilters.densityplot","text":"densityplot(x,[w])\n\nPlot (weighted) particles densities\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.double_integrator_covariance","page":"API","title":"LowLevelParticleFilters.double_integrator_covariance","text":"R = double_integrator_covariance(Ts, σ2=1)\n\nReturns the covariance matrix of a discrete-time integrator with piecewise constant stochastic force as input. Assumes the state [x; ẋ]. Ts is the sample time. σ2 scales the covariance matrix with the variance of the noise.\n\nThis matrix is rank deficient and some applications might require a small increase in the diagonal to make it positive definite (or use double_integrator_covariance_smooth).\n\nSee also double_integrator_covariance_smooth for the version that does not assume piecewise constant noise, leading to a full-rank covariance matrix that results in sample-tiem invariant covariance dynamics (often favorable).\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.double_integrator_covariance_smooth","page":"API","title":"LowLevelParticleFilters.double_integrator_covariance_smooth","text":"R = double_integrator_covariance_smooth(Ts, σ2=1)\n\nReturns the covariance matrix of a discrete-time integrator with continuous noise as input. Assumes the state [x; ẋ]. Ts is the sample time. σ2 scales the covariance matrix with the variance of the noise.\n\nThis matrix is full rank, but can be well approximated by a rank-1 matrix as double_integrator_covariance(h, σ2) ./ Ts. I.e., to make use of a single random number per step for augmented UKFs, but be have a resulting covariance dynamics that is approximately invariant to the sample interval, you can use double_integrator_covariance(h, σ2) ./ Ts instead of this function.\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.forward_trajectory","page":"API","title":"LowLevelParticleFilters.forward_trajectory","text":"forward_trajectory(imm::IMM, u, y, p = parameters(imm); interact = true)\n\nWhen performing batch filtering using an IMM filter, one may\n\nOverride the interact parameter of the filter\nAccess the mode probabilities along the trajectory as the sol.extra field. This is a matrix of size (n_modes, T) where T is the length of the trajectory (length of u and y).\n\nThe returned solution object is of type KalmanFilteringSolution and has the following fields:\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.forward_trajectory-2","page":"API","title":"LowLevelParticleFilters.forward_trajectory","text":"sol = forward_trajectory(pf, u::AbstractVector, y::AbstractVector, p=parameters(pf))\n\nRun the particle filter for a sequence of inputs and measurements (offline / batch filtering). Return a solution with x,w,we,ll = particles, weights, expweights and loglikelihood\n\nIf MonteCarloMeasurements.jl is loaded, you may transform the output particles to Matrix{MonteCarloMeasurements.Particles} using Particles(x,we). Internally, the particles are then resampled such that they all have unit weight. This is conventient for making use of the plotting facilities of MonteCarloMeasurements.jl.\n\nsol can be plotted\n\nplot(sol::ParticleFilteringSolution; nbinsy=30, xreal=nothing, dim=nothing)\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.forward_trajectory-3","page":"API","title":"LowLevelParticleFilters.forward_trajectory","text":"sol = forward_trajectory(kf::AbstractKalmanFilter, u::Vector, y::Vector, p=parameters(kf); debug=false)\n\nRun a Kalman filter forward to perform (offline / batch) filtering along an entire trajectory u, y.\n\nReturns a KalmanFilteringSolution: with the following\n\nx: predictions x(tt-1)\nxt: filtered estimates x(tt)\nR: predicted covariance matrices R(tt-1)\nRt: filter covariances R(tt)\nll: loglik\n\nsol can be plotted\n\nplot(sol::KalmanFilteringSolution; plotx = true, plotxt=true, plotu=true, ploty=true)\n\nSee KalmanFilteringSolution for more details.\n\nExtended help\n\nVery large systems\n\nIf your system is very large, i.e., the dimension of the state is very large, and the arrays u,y are long, this function may use a lot of memory to store all covariance matrices R, Rt. If you do not need all the information retained by this function, you may opt to call one of the functions\n\nloglik\nLowLevelParticleFilters.sse\nLowLevelParticleFilters.prediction_errors!\n\nThat store significantly less information. The amount of computation performed by all of these functions is identical, the only difference lies in what is stored and returned.\n\nCallbacks\n\nFor advanced usage, such as implementing conditional resetting and adaptive covariance, one may make use of the callback functions\n\npre_correct_cb(kf, u, y, p, t): called before the correction step, returns either nothing or a covariance matrix R2 to use in the correction step.\npre_predict_cb(kf, u, y, p, t, ll, e, S, Sᵪ): called before the prediction step, returns either nothing or a covariance matrix R1 to use in the prediction step. The arguments to this callback are filter, input, measurement, parameters, time, loglikelihood, prediction error, innovation covariance and Cholesky factor of the innovation covariance, essentially all the information available after the correct step.\n\nThe filter loop consists of the following steps, in this order:\n\npre_correct_cb\ncorrect!\npre_predict_cb\npredict!\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.interact!-Tuple{IMM}","page":"API","title":"LowLevelParticleFilters.interact!","text":"interact!(imm::IMM)\n\nThe interaction step of the IMM filter updates the state and covariance of each internal model based on the mixing probabilities imm.μ and the transition probability matrix imm.P.\n\nModels with small mixing probabilities will have their states and covariances updated more towards the states and covariances of models with higher mixing probabilities, and vice versa.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.log_likelihood_fun-Tuple{Any, AbstractVector, Vararg{Any}}","page":"API","title":"LowLevelParticleFilters.log_likelihood_fun","text":"ll(θ) = log_likelihood_fun(filter_from_parameters(θ::Vector)::Function, priors::Vector{Distribution}, u, y, p)\nll(θ) = log_likelihood_fun(filter_from_parameters(θ::Vector)::Function, priors::Vector{Distribution}, u, y, x, p)\n\nreturns function θ -> p(y|θ)p(θ)\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.loglik","page":"API","title":"LowLevelParticleFilters.loglik","text":"ll = loglik(filter, u, y, p=parameters(filter))\nll = loglik(filter, u, y, x, p=parameters(filter))\n\nCalculate log-likelihood for entire sequences u,y.\n\nFor Kalman-type filters when an accurate state sequence x is available, such as when data is obtained from a simulation or in a lab setting, the log-likelihood can be calculated using the state prediction errors rather than the output prediction errors. In this case, logpdf(f.R, x-x̂) is used rather than logpdf(S, y-ŷ).\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.logsumexp!","page":"API","title":"LowLevelParticleFilters.logsumexp!","text":"ll = logsumexp!(w, we [, maxw])\n\nNormalizes the weight vector w and returns the weighted log-likelihood\n\nhttps://arxiv.org/pdf/1412.8695.pdf eq 3.8 for p(y) https://discourse.julialang.org/t/fast-logsumexp/22827/7?u=baggepinnen for stable logsumexp\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.mean_trajectory-Tuple{Any, Vector, Vector}","page":"API","title":"LowLevelParticleFilters.mean_trajectory","text":"x,ll = mean_trajectory(pf, u::Vector{Vector}, y::Vector{Vector}, p=parameters(pf))\n\nThis method resets the particle filter to the initial state distribution upon start\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.mean_trajectory-Tuple{ParticleFilteringSolution}","page":"API","title":"LowLevelParticleFilters.mean_trajectory","text":"mean_trajectory(sol::ParticleFilteringSolution)\nmean_trajectory(x::AbstractMatrix, we::AbstractMatrix)\n\nCompute the weighted mean along the trajectory of a particle-filter solution. Returns a matrix of size T × nx. If x and we are supplied, the weights are expected to be in the original space (not log space).\n\nSee also mode_trajectory\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.metropolis","page":"API","title":"LowLevelParticleFilters.metropolis","text":"metropolis(ll::Function(θ), R::Int, θ₀::Vector, draw::Function(θ) = naive_sampler(θ₀))\n\nPerforms MCMC sampling using the marginal Metropolis (-Hastings) algorithm draw = θ -> θ' samples a new parameter vector given an old parameter vector. The distribution must be symmetric, e.g., a Gaussian. R is the number of iterations. See log_likelihood_fun\n\nExample:\n\nfilter_from_parameters(θ) = ParticleFilter(N, dynamics, measurement, MvNormal(n,exp(θ[1])), MvNormal(p,exp(θ[2])), d0)\npriors = [Normal(0,0.1),Normal(0,0.1)]\nll     = log_likelihood_fun(filter_from_parameters,priors,u,y,1)\nθ₀ = log.([1.,1.]) # Initial point\ndraw = θ -> θ .+ rand(MvNormal(0.1ones(2))) # Function that proposes new parameters (has to be symmetric)\nburnin = 200 # If using threaded call, provide number of burnin iterations\n# @time theta, lls = metropolis(ll, 2000, θ₀, draw) # Run single threaded\n# thetam = reduce(hcat, theta)'\n@time thetalls = LowLevelParticleFilters.metropolis_threaded(burnin, ll, 5000, θ₀, draw) # run on all threads, will provide (2000-burnin)*nthreads() samples\nhistogram(exp.(thetalls[:,1:2]), layout=3)\nplot!(thetalls[:,3], subplot=3) # if threaded call, log likelihoods are in the last column\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.mode_trajectory-Tuple{ParticleFilteringSolution}","page":"API","title":"LowLevelParticleFilters.mode_trajectory","text":"mode_trajectory(sol::ParticleFilteringSolution)\nmode_trajectory(x::AbstractMatrix, we::AbstractMatrix)\n\nCompute the mode (particle with largest weight) along the trajectory of a particle-filter solution. Returns a matrix of size T × nx.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.reset!-Tuple{LowLevelParticleFilters.AbstractKalmanFilter}","page":"API","title":"LowLevelParticleFilters.reset!","text":"reset!(kf::AbstractKalmanFilter; x0)\n\nReset the initial distribution of the state. Optionally, a new mean vector x0 can be provided.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.reset!-Tuple{LowLevelParticleFilters.AbstractParticleFilter}","page":"API","title":"LowLevelParticleFilters.reset!","text":"Reset the filter to initial state and covariance/distribution\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.reset!-Tuple{SqKalmanFilter}","page":"API","title":"LowLevelParticleFilters.reset!","text":"reset!(kf::SqKalmanFilter; x0)\n\nReset the initial distribution of the state. Optionally, a new mean vector x0 can be provided.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.simulate","page":"API","title":"LowLevelParticleFilters.simulate","text":"x,u,y = simulate(f::AbstractFilter, T::Int, du::Distribution, p=parameters(f), [N]; dynamics_noise=true, measurement_noise=true)\nx,u,y = simulate(f::AbstractFilter, u, p=parameters(f); dynamics_noise=true, measurement_noise=true)\n\nSimulate dynamical system forward in time T steps, or for the duration of u. Returns state sequence, inputs and measurements.\n\nu is an input-signal trajectory, alternatively, du is a distribution of random inputs.\n\nA simulation can be considered a draw from the prior distribution over the evolution of the system implied by the selected noise models. Such a simulation is useful in order to evaluate whether or not the noise models are reasonable.\n\nIf MonteCarloMeasurements.jl is loaded, the argument N::Int can be supplied, in which case N simulations are done and the result is returned in the form of Vector{MonteCarloMeasurements.Particles}.\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.smooth","page":"API","title":"LowLevelParticleFilters.smooth","text":"xb,ll = smooth(pf, M, u, y, p=parameters(pf))\nxb,ll = smooth(pf, xf, wf, wef, ll, M, u, y, p=parameters(pf))\n\nPerform particle smoothing using forward-filtering, backward simulation. Return smoothed particles and loglikelihood. See also smoothed_trajs, smoothed_mean, smoothed_cov\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.smooth-2","page":"API","title":"LowLevelParticleFilters.smooth","text":"sol = smooth(filtersol)\nsol = smooth(kf::AbstractKalmanFilter, u::Vector, y::Vector, p=parameters(kf))\n\nReturns a KalmanSmoothingSolution with smoothed estimates of state xT and covariance RT given all input output data u,y or an existing filtering solution filtersol obtained from forward_trajectory.\n\nThe return smoothing can be plotted using plot(sol), see KalmanSmoothingSolution and KalmanFilteringSolution for details.\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.smoothed_cov-Tuple{Any}","page":"API","title":"LowLevelParticleFilters.smoothed_cov","text":"smoothed_cov(xb)\n\nHelper function to calculate the covariance of smoothed particle trajectories\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.smoothed_mean-Tuple{Any}","page":"API","title":"LowLevelParticleFilters.smoothed_mean","text":"smoothed_mean(xb)\n\nHelper function to calculate the mean of smoothed particle trajectories\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.smoothed_trajs-Tuple{Any}","page":"API","title":"LowLevelParticleFilters.smoothed_trajs","text":"smoothed_trajs(xb)\n\nHelper function to get particle trajectories as a 3-dimensions array (N,M,T) instead of matrix of vectors.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.unscentedplot","page":"API","title":"LowLevelParticleFilters.unscentedplot","text":"unscentedplot(ukf;          n_std = 2, N = 100, dims=1:2)\nunscentedplot(sigmapoints;  n_std = 2, N = 100, dims=1:2)\n\nPlot the sigma points and their corresponding covariance ellipse. dims indicate the two dimensions to plot, and defaults to the first two dimensions.\n\nIf an UKF is passed, the sigma points after the last dynamics update are extracted from the filter. To plot the sigma points of the output, pass those in manually, they are available as ukf.measurement_model.cache.x0 and ukf.measurement_model.cache.x1, denoting the input and output points of the measurement model.\n\nNote: The covariance of the sigma points does not in general equal the predicted covariance of the state, since the state covariance is updated as cov(sigmapoints) + R1. Only when AUGD = true (augmented dynamics), the covariance of the state is given by the first nx sigmapoints.\n\nSee also covplot.\n\nnote: Note\nThis function requires using Plots to be called before it is used.\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.update!","page":"API","title":"LowLevelParticleFilters.update!","text":"ll, e = update!(f::AbstractFilter, u, y, p = parameters(f), t = index(f))\n\nPerform one step of predict! and correct!, returns log-likelihood and prediction error\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.update!-Tuple{IMM, Any, Any, Vararg{Any}}","page":"API","title":"LowLevelParticleFilters.update!","text":"update!(imm::IMM, u, y, p, t; correct_kwargs = (;), predict_kwargs = (;), interact = true)\n\nThe combined udpate for an IMM filter performs the following steps:\n\nCorrect each model with the measurements y and control input u.\nCombine the models into a single state and covariance.\nInteract the models to update their respective state and covariance.\nPredict each model to the next time step.\n\nThis differs slightly from the udpate step of other filters, where at the end of an update the state of the filter is the one-step ahead predicted value, whereas here each individual filter has a predicted state, but the combine! step of the IMM filter hasn't been performed on the predictions yet. The state of the IMM filter is thus x(tt) and not x(t+1t) like it is for other filters, and each filter internal to the IMM.\n\nArguments:\n\ncorrect_kwargs: An optional named tuple of keyword arguments that are sent to correct!.\npredict_kwargs: An optional named tuple of keyword arguments that are sent to predict!.\ninteract: Whether or not to run the interaction step.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.weighted_cov-Tuple{Any, Any}","page":"API","title":"LowLevelParticleFilters.weighted_cov","text":"weighted_cov(x,we)\n\nSimilar to weighted_mean, but returns covariances\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.weighted_mean-Tuple{Any, AbstractVector}","page":"API","title":"LowLevelParticleFilters.weighted_mean","text":"x̂ = weighted_mean(x,we)\n\nCalculated weighted mean of particle trajectories. we are expweights.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.weighted_mean-Tuple{Any}","page":"API","title":"LowLevelParticleFilters.weighted_mean","text":"x̂ = weighted_mean(pf)\nx̂ = weighted_mean(s::PFstate)\n\nSee also mean_trajectory\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.weighted_quantile-Tuple{Any, Any, Any}","page":"API","title":"LowLevelParticleFilters.weighted_quantile","text":"weighted_quantile(x,we,q)\nweighted_quantile(sol,q)\n\nCalculated weighted quantile q of particle trajectories. we are expweights. Returns a vector of length size(x, 2) where each entry has length nx. For a particle-filtering solution, this means the vector will be as long as the number of time steps in the solution.\n\n\n\n\n\n","category":"method"},{"location":"api/#StatsAPI.predict!","page":"API","title":"StatsAPI.predict!","text":"predict!(f, u, p = parameters(f), t = index(f))\n\nMove filter state forward in time using dynamics equation and input vector u.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsAPI.predict!-2","page":"API","title":"StatsAPI.predict!","text":"predict!(kf::AbstractKalmanFilter, u, p = parameters(kf), t::Integer = index(kf); R1, α = kf.α)\n\nPerform the prediction step (updating the state estimate to x(t+1t)). If R1 stored in kf is a function R1(x, u, p, t), this function is evaluated at the state before the prediction is performed. The dynamics noise covariance matrix R1 stored in kf can optionally be overridden by passing the argument R1, in this case R1 must be a matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsAPI.predict!-3","page":"API","title":"StatsAPI.predict!","text":"predict!(kf::SqKalmanFilter, u, p = parameters(kf), t::Real = index(kf); R1 = get_mat(kf.R1, kf.x, u, p, t), α = kf.α)\n\nFor the square-root Kalman filter, a custom provided R1 must be the upper triangular Cholesky factor of the covariance matrix of the process noise.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsAPI.predict!-Union{Tuple{CF}, Tuple{MF}, Tuple{AUGM}, Tuple{AUGD}, Tuple{IPM}, Tuple{IPD}, Tuple{UnscentedKalmanFilter{IPD, IPM, AUGD, AUGM}, Any}, Tuple{UnscentedKalmanFilter{IPD, IPM, AUGD, AUGM}, Any, Any}, Tuple{UnscentedKalmanFilter{IPD, IPM, AUGD, AUGM}, Any, Any, Real}} where {IPD, IPM, AUGD, AUGM, MF, CF}","page":"API","title":"StatsAPI.predict!","text":"predict!(ukf::UnscentedKalmanFilter, u, p = parameters(ukf), t::Real = index(ukf) * ukf.Ts; R1 = get_mat(ukf.R1, ukf.x, u, p, t), reject, mean, cov, dynamics)\n\nThe prediction step for an UnscentedKalmanFilter allows the user to override, R1 and any of the functions, reject, mean, cov, dynamics`.\n\nArguments:\n\nu: The input\np: The parameters\nt: The current time\nR1: The dynamics noise covariance matrix, or a function that returns the covariance matrix.\nreject: A function that takes a sigma point and returns true if it should be rejected.\nmean: The function that computes the mean of the state sigma points.\ncov: The function that computes the covariance of the state sigma points.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.prediction_errors!","page":"API","title":"LowLevelParticleFilters.prediction_errors!","text":"prediction_errors!(res, f::AbstractFilter, u, y, p = parameters(f), λ = 1)\n\nCalculate the prediction errors and store the result in res. Similar to sse, this function is useful for sum-of-squares optimization. In contrast to sse, this function returns the residuals themselves rather than their sum of squares. This is useful for Gauss-Newton style optimizers, such as LeastSquaresOptim.LevenbergMarquardt.\n\nArguments:\n\nres: A vector of length ny*length(y). Note, for each datapoint in u and u, there are ny outputs, and thus ny residuals.\nf: Any filter\nλ: A weighting factor to minimize dot(e, λ, e). A commonly used metric is λ = Diagonal(1 ./ (mag.^2)), where mag is a vector of the \"typical magnitude\" of each output. Internally, the square root of W = sqrt(λ) is calculated so that the residuals stored in res are W*e.\n\nSee example in Solving using Gauss-Newton optimization.\n\n\n\n\n\n","category":"function"},{"location":"adaptive_control/#Adaptive-Estimation-and-Control","page":"Adaptive estimation and control","title":"Adaptive Estimation and Control","text":"","category":"section"},{"location":"adaptive_control/","page":"Adaptive estimation and control","title":"Adaptive estimation and control","text":"This tutorial is hosted as a notebook.","category":"page"},{"location":"adaptive_control/","page":"Adaptive estimation and control","title":"Adaptive estimation and control","text":"It also has an associated video:","category":"page"},{"location":"adaptive_control/","page":"Adaptive estimation and control","title":"Adaptive estimation and control","text":"<iframe style=\"height: 315px; width: 560px\" src=\"https://www.youtube.com/embed/Ip_prmA7QTU?si=Fat_srMTQw5JtW2d\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","category":"page"},{"location":"beetle_example/#Smoothing-the-track-of-a-moving-beetle","page":"Particle-filter tutorial","title":"Smoothing the track of a moving beetle","text":"","category":"section"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"This is an example of smoothing the 2-dimensional trajectory of a moving dung beetle. The example spurred off of this Discourse topic. For more information about the research behind this example, see Artificial light disrupts dung beetles’ sense of direction and A dung beetle that path integrates without the use of landmarks. Special thanks to Yakir Gagnon for providing this example.","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"In this example we will describe the position coordinates, x and y, of the beetle as functions of its velocity, v_t, and direction, θ_t:","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"beginaligned\nx_t+1 = x_t + cos(θ_t)v_t \ny_t+1 = y_t + sin(θ_t)v_t \nv_t+1 = v_t + e_t \nθ_t+1 = θ_t + w_t\nendaligned","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"where e_t  N(0σ_e) w_t  N(0σ_w) The beetle further has two \"modes\", one where it's moving towards a goal, and one where it's searching in a more erratic manner. Figuring out when this mode switch occurs is the goal of the filtering. The mode will be encoded as a state variable, and used to determine the amount of dynamic noise affecting the angle of the beetle, i.e., in the searching mode, the beetle has more angle noise. The mode switching is modeled as a stochastic process with a binomial distribution (coin flip) describing the likelihood of a switch from mode 0 (moving to goal) and mode 1 (searching). Once the beetle has started searching, it stays in that mode, i.e., the searching mode is \"sticky\" or \"terminal\".","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"The image below shows an example video from which the data is obtained (Image: Bettle)","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"We load a single experiment from file for the purpose of this example (in practice, there may be hundreds of experiments)","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"using LowLevelParticleFilters, LinearAlgebra, StaticArrays, Distributions, Plots, Random\nusing DisplayAs # hide\nusing DelimitedFiles\npath = \"../track.csv\"\nxyt = readdlm(path)\ntosvec(y) = reinterpret(SVector{length(y[1]),Float64}, reduce(hcat,y))[:] |> copy # helper function\ny = tosvec(collect(eachrow(xyt[:,1:2])))\nnothing # hide","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"We then define some properties of the dynamics and the filter. We will use an AdvancedParticleFilter since we want to have fine-grained control over the noise sampling for the mode switch.","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"N = 2000 # Number of particles in the particle filter\nn = 4 # Dimension of state: we have position (2D), speed and angle\np = 2 # Dimension of measurements, we can measure the x and the y\n@inline pos(s) = s[SVector(1,2)]\n@inline vel(s) = s[3]\n@inline ϕ(s) = s[4]\n@inline mode(s) = s[5]\nnothing # hide","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"We then define the probability distributions we need.","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"dgσ = 1 # the deviation of the measurement noise distribution\ndvσ = 0.3 # the deviation of the dynamics noise distribution\nϕσ  = 0.5\nconst switch_prob = 0.03 # Probability of mode switch\nconst dg = MvNormal(@SVector(zeros(p)), dgσ^2) # Measurement noise Distribution\nconst df = LowLevelParticleFilters.TupleProduct((Normal.(0,[1e-1, 1e-1, dvσ, ϕσ])...,Binomial(1,switch_prob)))\nd0 = MvNormal(SVector(y[1]..., 0.5, atan((y[2]-y[1])...), 0), [3.,3,2,2,0])\nconst noisevec = zeros(5) # cache vector\nnothing # hide","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"We now define the dynamics, since we use the advanced filter, we include the noise=false argument. The dynamics is directly defined in discrete time.","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"@inline function dynamics(s,u,p,t,noise=false)\n    # current state\n    m = mode(s)\n    v = vel(s)\n    a = ϕ(s)\n    p = pos(s)\n    # get noise\n    if noise\n        y_noise, x_noise, v_noise, ϕ_noise,_ = rand!(df, noisevec)\n    else\n        y_noise, x_noise, v_noise, ϕ_noise = 0.,0.,0.,0.\n    end\n    # next state\n    v⁺ = max(0.999v + v_noise, 0.0)\n    m⁺ = Float64(m == 0 ? rand() < switch_prob : true)\n    a⁺ = a + (ϕ_noise*(1 + m*10))/(1 + v⁺) # next state velocity is used here\n    p⁺ = p + SVector(y_noise, x_noise) + SVector(sincos(a))*v⁺ # current angle but next velocity\n    SVector{5,Float64}(p⁺[1], p⁺[2], v⁺, a⁺, m⁺) # all next state\nend\nfunction measurement_likelihood(s,u,y,p,t)\n    logpdf(dg, pos(s)-y) # A simple linear measurement model with normal additive noise\nend\n@inline measurement(s,u,p,t,noise=false) = s[SVector(1,2)] + noise*rand(dg) # We observe the position coordinates with the measurement\nnothing # hide","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"In this example, we have no control inputs, we thus define a vector of only zeros. We then solve the forward filtering problem and plot the results.","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"u = zeros(length(y))\npf = AuxiliaryParticleFilter(AdvancedParticleFilter(N, dynamics, measurement, measurement_likelihood, df, d0))\nT = length(y)\nsol = forward_trajectory(pf,u[1:T],y[1:T])\n(; x,w,we,ll) = sol\nplot(sol, markerstrokecolor=:auto, m=(2,0.5))\nDisplayAs.PNG(Plots.current()) # hide","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"We can clearly see when the beetle switched mode (state variable 5). This corresponds well to annotations provided by a biologist and is the fundamental question we want to answer with the filtering procedure.","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"We can plot the mean of the filtered trajectory as well","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"xh = mean_trajectory(x,we)\n\n\"plotting helper function\"\nfunction to1series(x::AbstractVector, y)\n    r,c = size(y)\n    y2 = vec([y; fill(Inf, 1, c)])\n    x2 = repeat([x; Inf], c)\n    x2,y2\nend\nto1series(y) = to1series(1:size(y,1),y)\n\nfig1 = plot(xh[:,1],xh[:,2], c=:blue, lab=\"estimate\", legend=:bottomleft)\nplot!(xyt[:,1],xyt[:,2], c=:red, lab=\"measurement\")","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"as well as the angle state variable (we subsample the particles to not get sluggish plots)","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"fig2 = scatter(to1series(ϕ.(x)'[:,1:2:end])..., m=(:black, 0.03, 2), lab=\"\", size=(500,300))\nplot!(identity.(xh[:,4]), lab=\"Filtered angle\", legend=:topleft, ylims=(-30, 70))\nDisplayAs.PNG(fig2) # hide","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"The particle plot above indicate that the posterior is multimodal. This phenomenon arises due to the simple model that uses an angle that is allowed to leave the interval 0-2π rad. In this example, we are not interested in the angle, but rather when the beetle switches mode. The filtering distribution above gives a hint at when this happens, but we will not plot the mode trajectory until we have explored smoothing as well.","category":"page"},{"location":"beetle_example/#Smoothing","page":"Particle-filter tutorial","title":"Smoothing","text":"","category":"section"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"The filtering results above does not use all the available information when trying to figure out the state trajectory. To do this, we may call a smoother. We use a particle smoother and compute 10 smoothing trajectories.","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"M = 10 # Number of smoothing trajectories, NOTE: if this is set higher, the result will be better at the expense of linear scaling of the computational cost.\nsb,ll = smooth(pf, M, u, y) # Sample smoothing particles (b for backward-trajectory)\nsbm = smoothed_mean(sb)     # Calculate the mean of smoothing trajectories\nsbt = smoothed_trajs(sb)    # Get smoothing trajectories\nplot!(fig1, sbm[1,:],sbm[2,:], lab=\"xs\")","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"plot!(fig2, identity.(sbm'[:,4]), lab=\"smoothed\")\nDisplayAs.PNG(fig2) # hide","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"We see that the smoothed trajectory may look very different from the filter trajectory. This is an indication that it's hard to tell what state the beetle is currently in, but easier to look back and tell what state the beetle must have been in at a historical point. ","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"We can also visualize the mode state","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"plot(xh[:,5], lab=\"Filtering\")\nplot!(to1series(sbt[5,:,:]')..., lab=\"Smoothing\", title=\"Mode trajectories\", l=(:black,0.2))","category":"page"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"also this state variable indicates that it's hard to tell what state the beetle is in during filtering, but obvious with hindsight (smoothing). The mode switch occurs when the filtering distribution of the angle becomes drastically wider, indicating that increased dynamics noise is required in order to describe the motion of the beetle.","category":"page"},{"location":"beetle_example/#Summary","page":"Particle-filter tutorial","title":"Summary","text":"","category":"section"},{"location":"beetle_example/","page":"Particle-filter tutorial","title":"Particle-filter tutorial","text":"This example has demonstrated filtering and smoothing in an advanced application that includes manual control over noise, mixed continuous and discrete state.","category":"page"},{"location":"beetle_example_imm/#Filtering-the-track-of-a-moving-beetle-using-IMM","page":"IMM-filter tutorial","title":"Filtering the track of a moving beetle using IMM","text":"","category":"section"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"This tutorial is very similar to Smoothing the track of a moving beetle, but uses an Interacting Multiple Models (IMM) filter to model the mode switching of the beetle. The IMM filter is a mixture model, in this case with internal Unscented Kalman filters, where each Kalman filter represents a different mode of the system. The IMM filter is able to switch between these modes based on the likelihood of the mode given the data.","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"This is an example of smoothing the 2-dimensional trajectory of a moving dung beetle. The example spurred off of this Discourse topic. For more information about the research behind this example, see Artificial light disrupts dung beetles’ sense of direction and A dung beetle that path integrates without the use of landmarks. Special thanks to Yakir Gagnon for providing this example.","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"In this example we will describe the position coordinates, x and y, of the beetle as functions of its velocity, v_t, and direction, θ_t:","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"beginaligned\nx_t+1 = x_t + cos(θ_t)v_t \ny_t+1 = y_t + sin(θ_t)v_t \nv_t+1 = v_t + e_t \nθ_t+1 = θ_t + w_t\nendaligned","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"where e_t  N(0σ_e) w_t  N(0σ_w) The beetle further has two \"modes\", one where it's moving towards a goal, and one where it's searching in a more erratic manner. Figuring out when this mode switch occurs is the goal of the filtering. The mode will be encoded as two different models, where the difference between the models lies in the amount of dynamic noise affecting the angle of the beetle, i.e., in the searching mode, the beetle has more angle noise. The mode switching is modeled as a stochastic process with a binomial distribution (coin flip) describing the likelihood of a switch from mode 0 (moving to goal) and mode 1 (searching). Once the beetle has started searching, it stays in that mode, i.e., the searching mode is \"sticky\" or \"terminal\".","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"The image below shows an example video from which the data is obtained (Image: Bettle)","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"We load a single experiment from file for the purpose of this example (in practice, there may be hundreds of experiments)","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"using LowLevelParticleFilters, LinearAlgebra, StaticArrays, Distributions, Plots, Random\nusing DisplayAs # hide\nusing DelimitedFiles\ncd(@__DIR__)\npath = \"../track.csv\"\nxyt = readdlm(path)\ntosvec(y) = reinterpret(SVector{length(y[1]),Float64}, reduce(hcat,y))[:] |> copy # helper function\ny = tosvec(collect(eachrow(xyt[:,1:2])))\nnothing # hide","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"We then define some properties of the dynamics and the filter. We will use an AdvancedParticleFilter since we want to have fine-grained control over the noise sampling for the mode switch.","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"nx = 4 # Dimension of state: we have position (2d), speed and angle\nny = 2 # Dimension of measurements, we can measure the x and the y\n@inline pos(s) = s[SVector(1,2)]\n@inline vel(s) = s[3]\n@inline ϕ(s) = s[4]\nnothing # hide","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"We then define the probability distributions we need. The IMM filter takes a transition-probability matrix, P, and an initial mixing probability, μ. P is a Markov (stochastic) matrix, where each row sums to one, and P[i, j] is the probability of switching from mode i to mode j. μ is a vector of probabilities, where μ[i] is the probability of starting in mode i. We also define the noise distributions for the dynamics and the measurements. The dynamics noise is modeled as a Gaussian distribution with a standard deviation of dvσ for the velocity and ϕσ for the angle. The measurement noise is modeled as a Gaussian distribution with a standard deviation of dgσ. The initial state is modeled as a Gaussian distribution with a mean at the first measurement and a standard deviation of d0.","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"dgσ = 1.0 # the deviation of the measurement noise distribution\ndvσ = 0.3 # the deviation of the dynamics noise distribution\nϕσ  = 0.5\nP = [0.995 0.005; 0.0 1] # Transition probability matrix, we model the search mode as \"terminal\"\nμ = [1.0, 0.0] # Initial mixing probabilities\nR1 = Diagonal([1e-1, 1e-1, dvσ, ϕσ].^2)\nR2 = dgσ^2*I(ny) # Measurement noise covariance matrix\nd0 = MvNormal(SVector(y[1]..., 0.5, atan((y[2]-y[1])...)), [3.,3,2,2])\nnothing # hide","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"We now define the dynamics, which is directly defined in discrete time. The third argument is a parameter we call modegain, which is used to scale the amount of noise in the angle of the beetle depending on the mode in which it is in. The last argument is a boolean that tells the dynamics function which mode it is in, we will close over this argument when defining the dynamics for the individual Kalman filters that are part of the IMM, one will use m = false and one will use m = true.","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"@inline function dynamics(s,_,modegain,t,w,m)\n    # current state\n    v = vel(s)\n    a = ϕ(s)\n    p = pos(s)\n\n    y_noise, x_noise, v_noise, ϕ_noise = w\n\n    # next state\n    v⁺ = max(0.999v + v_noise, 0.0)\n    a⁺ = a + (ϕ_noise*(1 + m*modegain))/(1 + v⁺) # next state velocity is used here\n    p⁺ = p + SVector(y_noise, x_noise) + SVector(sincos(a))*v⁺ # current angle but next velocity\n    SVector(p⁺[1], p⁺[2], v⁺, a⁺) # all next state\nend\n@inline measurement(s,u,p,t) = s[SVector(1,2)] # We observe the position coordinates with the measurement\nnothing # hide","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"In this example, we have no control inputs, we thus define a vector of only zeros. We then solve the forward filtering problem and plot the results.","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"u = zeros(length(y)) # no control inputs\nkffalse = UnscentedKalmanFilter{false,false,true,false}((x,u,p,t,w)->dynamics(x,u,p,t,w,false), measurement, R1, R2, d0; ny, nu=0, p=10)\nkftrue = UnscentedKalmanFilter{false,false,true,false}((x,u,p,t,w)->dynamics(x,u,p,t,w,true), measurement, R1, R2, d0; ny, nu=0, p=10)\n\nimm = IMM([kffalse, kftrue], P, μ; p = 10)\n\nT = length(y)\nsol = forward_trajectory(imm, u, y, interact=true)\nfigx = plot(sol, plotu=false, plotRt=true)\nfigmode = plot(sol.extra', title=\"Mode\")\nplot(figx, figmode)\nDisplayAs.PNG(Plots.current()) # hide","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"If you have followed the particle filter tutorial Smoothing the track of a moving beetle, you will notice that the result here is much worse. We used noise parameters similar to in the particle-gilter example, but those were tuned fo the particle filter. Below, we will attempt to optimize the performance of the IMM filter.","category":"page"},{"location":"beetle_example_imm/#Tuning-by-optimization","page":"IMM-filter tutorial","title":"Tuning by optimization","text":"","category":"section"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"We will attempt to optimize the dynamics and measurement noise covariance matrices and the modegain parameter. We code this up in two functions, one that takes the parameter vector and returns an IMM filter, and one that calculates the loss given the filter. We will optimize the log-likelihood of the data given the filter.","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"params = [log10.(diag(R1)); log10(1); log10(10)]\n\nfunction get_opt_kf(p)\n    T = eltype(p)\n    R1i = Diagonal(SVector{4}(exp10.(p[1:4])))\n    R2i = SMatrix{2,2}(exp10(p[5])*R2)\n    d0i = MvNormal(SVector{4, T}(T.(d0.μ)), SMatrix{4,4}(T.(d0.Σ)))\n    modegain = 2+exp10(p[6])\n    Pi = SMatrix{2,2, Float64,4}(P)\n    # sigmoid(x) = 1/(1+exp(-x))\n    # switch_prob = sigmoid(p[7])\n    # Pi = [1-switch_prob switch_prob; 0 1]\n    kffalse = UnscentedKalmanFilter{false,false,true,false}((x,u,p,t,w)->dynamics(x,u,p,t,w,false), measurement, R1i, R2i, d0i; ny, nu=0)\n    kftrue = UnscentedKalmanFilter{false,false,true,false}((x,u,p,t,w)->dynamics(x,u,p,t,w,true), measurement, R1i, R2i, d0i; ny, nu=0)\n\n    IMM([kffalse, kftrue], Pi, T.(μ), p=modegain)\nend\nfunction cost(pars)\n    try\n        imm = get_opt_kf(pars)\n        T = length(y)\n        ll = loglik(imm, u[1:T], y[1:T], interact=true) - 1/2*logdet(imm.models[1].R1)\n        return -ll\n    catch e\n        # rethrow() # If you only get Inf, you can uncomment this line to see the error message\n        return eltype(pars)(Inf)\n\tend\nend\n\nusing Optim\nRandom.seed!(0)\nres = Optim.optimize(\n    cost,\n    params,\n    ParticleSwarm(), # Use a gradient-free optimizer. ForwardDiff works, but the algorithm is numerically difficult to compute gradients through and may suffer from overflows in the gradient computation\n    Optim.Options(\n        show_trace        = true,\n        show_every        = 5,\n        iterations        = 100,\n        time_limit        = 30,\n    ),\n)\n\nimm = get_opt_kf(res.minimizer)\nimm = get_opt_kf([-0.1981314138910982, -0.18626406669394405, -2.7342979645906547, 0.17994244691004058, -11.706419070755908, -54.16703441089562]) #make sure it goes well # hide\n\nsol = forward_trajectory(imm, u, y)\nplot(sol.extra', title=\"Mode (optimized filter)\")","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"If it went well, the filter should be in mode 1 (the false mode) from the start until around 200 time steps, at which point it should switch to model 2 (true). This method of detecting the mode switch of the beetle appears to be somewhat less robust than the particle filter, but is significantly cheaper computationally. ","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"The IMM filter does not stick in mode 2 perpetually after having reached it since it never actually becomes fully confident that mode 2 has been reached, but detecting the first switch is sufficient to know that the switch has occurred. ","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"The log-likelihood of the solution","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"sol.ll","category":"page"},{"location":"beetle_example_imm/","page":"IMM-filter tutorial","title":"IMM-filter tutorial","text":"should be similar to that of the particle-filter in the tutorial Smoothing the track of a moving beetle, which was around -1660.","category":"page"},{"location":"noisetuning/#How-to-tune-a-Kalman-filter","page":"Noise tuning and disturbance modeling for Kalman filtering","title":"How to tune a Kalman filter","text":"","category":"section"},{"location":"noisetuning/","page":"Noise tuning and disturbance modeling for Kalman filtering","title":"Noise tuning and disturbance modeling for Kalman filtering","text":"This tutorial is hosted as a notebook.","category":"page"},{"location":"noisetuning/","page":"Noise tuning and disturbance modeling for Kalman filtering","title":"Noise tuning and disturbance modeling for Kalman filtering","text":"See also section 3.3 in \"Stochastic Differential Equation Methods for Spatio-Temporal Gaussian Process Regression\", Arno Solin for how to model temporal Gaussian processes as linear statspace models, suitable for inclusion as disturbance models for Kalman filtering.","category":"page"},{"location":"rbpf_example/#Rao-Blackwellized-particle-filter","page":"Rao-Blackwellized PF tutorial","title":"Rao-Blackwellized particle filter","text":"","category":"section"},{"location":"rbpf_example/","page":"Rao-Blackwellized PF tutorial","title":"Rao-Blackwellized PF tutorial","text":"This example will demonstrate use of the Rao-Blackwellized particle filter (RBPF), also called \"Marginalized particle filter\".","category":"page"},{"location":"rbpf_example/","page":"Rao-Blackwellized PF tutorial","title":"Rao-Blackwellized PF tutorial","text":"This filter is effectively a particle filter where each particle is a Kalman filter that is responsible for the estimation of a linear sub structure.","category":"page"},{"location":"rbpf_example/","page":"Rao-Blackwellized PF tutorial","title":"Rao-Blackwellized PF tutorial","text":"The filter assumes that the dynamics follow \"model 2\" in the article \"Marginalized Particle Filters for Mixed Linear/Nonlinear State-space Models\" by Thomas Schön, Fredrik Gustafsson, and Per-Johan Nordlund, i.e., the dynamics is described by","category":"page"},{"location":"rbpf_example/","page":"Rao-Blackwellized PF tutorial","title":"Rao-Blackwellized PF tutorial","text":"beginalign\n    x_t+1^n = f_n(x_t^n u p t) + A_n(x_t^n u p t) x_t^l + w_t^n quad w_t^n sim mathcalN(0 R_1^n) \n    x_t+1^l = A() x_t^l + Bu + w_t^l quad w_t^l sim mathcalN(0 R_1^l) \n    y_t = g(x_t^n u p t) + C() x_t^l + e_t quad e_t sim mathcalN(0 R_2)\nendalign","category":"page"},{"location":"rbpf_example/","page":"Rao-Blackwellized PF tutorial","title":"Rao-Blackwellized PF tutorial","text":"where x^n is a subset of the state that has nonlinear dynamics or measurement function, and x^l is a subset of the state where both dynamics and measurement function are linear and Gaussian. The entire state vector is represented by a special type RBParticle that behaves like the vector [xn; xl], but stores xn, xl and the covariance R of xl separately.","category":"page"},{"location":"rbpf_example/","page":"Rao-Blackwellized PF tutorial","title":"Rao-Blackwellized PF tutorial","text":"warning: Experimental\nThis filter is currently considered experimental and the user interface may change in the future without respecting semantic versioning.","category":"page"},{"location":"rbpf_example/","page":"Rao-Blackwellized PF tutorial","title":"Rao-Blackwellized PF tutorial","text":"Below, we define all functions and matrices that are needed to perform marginalized particle filtering for the dynamical system","category":"page"},{"location":"rbpf_example/","page":"Rao-Blackwellized PF tutorial","title":"Rao-Blackwellized PF tutorial","text":"beginalign\nx_t+1^n = arctan x_t^n + beginpmatrix 1  0  0 endpmatrix x_t^1 + w_t^n tag1a \nx_t+1^1 = beginpmatrix\n1  03  0 \n0  092  -03 \n0  03  092\nendpmatrix x_t^1 + w_t^1 tag1b \ny_t = beginpmatrix\n01(x_t^n)^2 operatornamesgn(x_t^n) \n0\nendpmatrix + beginpmatrix\n0  0  0 \n1  -1  1\nendpmatrix x_t^1 + e_t tag1c \ntextwhere\nw_t = beginpmatrix\nw_t^n \nw_t^1\nendpmatrix sim mathcalN(0 001I_4times 4) tag1d \ne_t sim mathcalN(0 01I_2times 2) tag1e \nx_0^n sim mathcalN(0 1) tag1f \nx_0^1 sim mathcalN(0_3times 1 0_3times 3) tag1g\nendalign","category":"page"},{"location":"rbpf_example/","page":"Rao-Blackwellized PF tutorial","title":"Rao-Blackwellized PF tutorial","text":"Since this is a tracking problem without control inputs, and there are no parameters and time dependence, we define functions with the signature fn(xn, args...) to handle the fact that the filter will pass empty arguments for inputs, parameters and time.","category":"page"},{"location":"rbpf_example/","page":"Rao-Blackwellized PF tutorial","title":"Rao-Blackwellized PF tutorial","text":"Below, we define functions that return the matrix A_n despite that it is constant, we do this to illustrate that this matrix may in general be a function of the nonlinear state, parameter and time. If the matrix is constant, it's okay to let An be a Matrix or SMatrix instead of a function.","category":"page"},{"location":"rbpf_example/","page":"Rao-Blackwellized PF tutorial","title":"Rao-Blackwellized PF tutorial","text":"using LowLevelParticleFilters, LinearAlgebra\nusing LowLevelParticleFilters: SimpleMvNormal\nusing DisplayAs # hide\nnxn = 1         # Dimension of nonlinear state\nnxl = 3         # Dimension of linear state\nnx  = nxn + nxl # Total dimension of state\nnu  = 0         # Dimension of control input\nny  = 2         # Dimension of measurement\nN   = 200       # Number of particles\nfn(xn, args...) = atan.(xn)         # Nonlinear part of nonlinear state dynamics\nAn  = [1.0 0.0 0.0]     # Linear part of nonlinear state dynamics\nAl  = [1.0  0.3   0.0;  # Linear part of linear state dynamics (the standard Kalman-filter A matrix). It's defined as a matrix here, but it can also be a function of (x, u, p, t)\n                   0.0  0.92 -0.3; \n                   0.0  0.3   0.92] # 3x3 matrix\nCl = [0.0  0.0 0.0; \n      1.0 -1.0 1.0]    # 2x3 measurement matrix\ng(xn, args...) = [0.1 * xn[]^2 * sign(xn[]), 0.0] # 2x1 vector\n\nBl = zeros(nxl, nu)\n\n# Noise parameters\nR1n = [0.01;;]          # Scalar variance for w^n\nR1l = 0.01 * I(3)       # 3x3 covariance for w^l\nR2  = 0.1 * I(2)         # 2x2 measurement noise (shared between linear and nonlinear parts)\n\n# Initial states (xn ~ N(0,1), xl ~ N(0, 0.01I))\nx0n = zeros(nxn)\nR0n = [1.0;;]\nx0l = zeros(nxl)\nR0l = 0.01 * I(nxl)\n\nd0l = SimpleMvNormal(x0l, R0l)\nd0n = SimpleMvNormal(x0n, R0n)\n\nkf    = KalmanFilter(Al, Bl, Cl, 0, R1l, R2, d0l; ny, nu) # Since we are providing a function instead of a matrix for C, we also provide the number of outputs ny\nmm    = RBMeasurementModel(g, R2, ny)\nnames = SignalNames(x=[\"\\$x^n_1\\$\", \"\\$x^l_2\\$\", \"\\$x^l_3\\$\", \"\\$x^l_4\\$\"], u=[], y=[\"\\$y_1\\$\", \"\\$y_2\\$\"], name=\"RBPF\") # For nicer labels in the plot\npf    = RBPF(N, kf, fn, mm, R1n, d0n; nu, An, Ts=1.0, names)\n\n# Simulate some data from the filter dynamics\nu     = [zeros(nu) for _ in 1:100]\nx,u,y = simulate(pf, u)\n\n# Perform the filtering\nsol = forward_trajectory(pf, u, y)\n\nusing Plots\nplot(sol, size=(800,600), xreal=x, markersize=1, nbinsy=50, colorbar=false)\nfor i = 1:nx\n    plot!(ylims = extrema(getindex.(x, i)) .+ (-1, 1), sp=i)\nend\ncurrent()\nDisplayAs.PNG(Plots.current()) # hide","category":"page"},{"location":"rbpf_example/","page":"Rao-Blackwellized PF tutorial","title":"Rao-Blackwellized PF tutorial","text":"The cyan markers represent the true state in the state plots, and the measurements in the measurement plots. The heatmap represents the particle distribution. Note, since each particle has an additional covariance estimate for the linear sub state, the heatmaps for the linear sub state are constructed by drawing a small number of samples from this marginal distribution. Formally, the marginal distribution over the linear sub state is a gaussian-mixture model where the weight of each gaussian is the weight of the particle. This fact is not taken into account when the heat map for the predicted measurement is constructed, so interpret this heatmap with caution.","category":"page"},{"location":"rbpf_example/","page":"Rao-Blackwellized PF tutorial","title":"Rao-Blackwellized PF tutorial","text":"In this example, we made use of standard julia arrays for the dynamics and covariances etc., for optimum performance (the difference may be dramatic), make use of static arrays from StaticArrays.jl. ","category":"page"},{"location":"rbpf_example/","page":"Rao-Blackwellized PF tutorial","title":"Rao-Blackwellized PF tutorial","text":"The paper referenced above mention a lot of special cases in which the filter can be simplified, it's worth a read if you are considering using this filter.","category":"page"},{"location":"rbpf_example/#Details-of-the-marginal-distribution-over-the-linear-sub-state","page":"Rao-Blackwellized PF tutorial","title":"Details of the marginal distribution over the linear sub state","text":"","category":"section"},{"location":"rbpf_example/","page":"Rao-Blackwellized PF tutorial","title":"Rao-Blackwellized PF tutorial","text":"We can create a distribution object that represents the Gaussian mixture model that represents the marginal distribution over the linear sub state. This may be useful to compute confidence intervals or quantiles etc.","category":"page"},{"location":"rbpf_example/","page":"Rao-Blackwellized PF tutorial","title":"Rao-Blackwellized PF tutorial","text":"using Distributions\ntime_step = 100 # The time step at which to access the solution object from above\nwe = sol.we[:, time_step] # Extract the weights of the particles at the desired time step\nlinear_state_inds = nxn+1:nx\nxl = getindex.(sol.x[:, time_step], Ref(linear_state_inds)) # Extract the linear sub state from the particles at the desired time step\nRv = [sol.x[i, time_step].R for i = 1:num_particles(pf)] # Extract the covariance of each mixture component\n\ncomponents = [MvNormal(xl[i], Rv[i]) for i = 1:num_particles(pf)] # The component distribution in the mixture model\n\nD = Distributions.MixtureModel(components, we)\n\ncov(D)","category":"page"},{"location":"rbpf_example/","page":"Rao-Blackwellized PF tutorial","title":"Rao-Blackwellized PF tutorial","text":"Above, we showed how to compute the covariance of the mixture distribution. If we consider the marginal distribution of a single dimension of the linear sub state, we can compute, e.g., quantiles as well by calling quantile(D, q).","category":"page"},{"location":"fault_detection/#Fault-detection","page":"Fault detection","title":"Fault detection","text":"","category":"section"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"This is also a video tutorial, available below:","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"<iframe style=\"height: 315px; width: 560px\" src=\"https://www.youtube.com/embed/NgDcMuewPbI?si=6_bgIDiz9PFIE_gQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","category":"page"},{"location":"fault_detection/#Fault-detection-using-state-estimation","page":"Fault detection","title":"Fault detection using state estimation","text":"","category":"section"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"This tutorial explores the use of a Kalman filter for fault detection in a thermal system","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"Modeling\nFiltering\nMaximum-likelihood estimation of covariance and model parameters\nMonitor prediction-error Z-score to detect faults\nA fault may be faulty sensor or unexpected temperature fluctuations","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"using DelimitedFiles, Plots, Dates\nusing LowLevelParticleFilters, LinearAlgebra, StaticArrays\nusing LowLevelParticleFilters: AbstractKalmanFilter, particletype, covtype,state,  covariance, parameters, KalmanFilteringSolution\nusing Optim\nusing DisplayAs # hide","category":"page"},{"location":"fault_detection/#Load-data","page":"Fault detection","title":"Load data","text":"","category":"section"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"From kaggle.com/datasets/arashnic/sensor-fault-detection-data","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"A time series of temperature measurements","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"using Downloads\nurl = \"https://drive.google.com/uc?export=download&id=1zuIBaOhhrCxnifbvY7qJQTOyKWBDeBRh\"\nfilename = \"sensor-fault-detection.csv\"\nDownloads.download(url, filename)\nraw_data = readdlm(filename, ';')\nheader = raw_data[1,:]\ndf = dateformat\"yyyy-mm-ddTHH:MM:SS\"\nnothing # hide","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"The data is not stored in order","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"time_unsorted = DateTime.(getindex.(raw_data[2:end, 1], Ref(1:19)), df)","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"so we compute a sorting permutation that brings it into chronological order","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"perm = sortperm(time_unsorted)\ntime = time_unsorted[perm]\ny = raw_data[2:end, 3][perm] .|> float\nnothing # hide","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"y is the recorded temperature data.","category":"page"},{"location":"fault_detection/#Look-at-the-data","page":"Fault detection","title":"Look at the data","text":"","category":"section"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"plot(time, y, ylabel=\"Temperature\", legend=false)\nDisplayAs.PNG(Plots.current()) # hide","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"timev = Dates.value.(time)  ./ 1000 # A numerical time vector, time was in milliseconds\nplot(diff(timev), yscale=:log10, title=\"Time interval between measurement points\", legend=false)\nDisplayAs.PNG(Plots.current()) # hide","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"Samples are not evenly spaced (lots of missing data), but the interval is always a multiple of Ts","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"intervals = sort(unique(diff(timev)))\nintervals ./ intervals[1]\nTs = intervals[1]\nnothing # hide","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"Tf = intervals[end] - intervals[1]\nnothing # hide","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"We expand the data arrays such that we can treat them as having a constant sample interval, time points where there is no data available are indicated as missing","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"time_full = range(timev[1], timev[end], step=Ts)\n\navailable_inds = [findfirst(==(t), time_full) for t in timev]\n\ny_full = fill(NaN, length(time_full))\ny_full[available_inds] .= y\ny_full = replace(y_full, NaN=>missing)\ny_full = SVector{1}.(y_full)\nnothing # hide","category":"page"},{"location":"fault_detection/#Design-Kalman-filter","page":"Fault detection","title":"Design Kalman filter","text":"","category":"section"},{"location":"fault_detection/#Modeling","page":"Fault detection","title":"Modeling","text":"","category":"section"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"A simple model of temperature change is","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"dot T(t) = alpha big(T(t) - T_env(t)big) + w(t)","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"Where T is the temperature of the system, T_env the temperature of the environment and w represents thermal energy added or removed by unmodeled sources.","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"Since we have no knowledge of T_env and w, but we observe that they vary slowly, we add yet another state variable to the model corresponding to an integrating disturbance model:","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"beginaligned\ndot T(t) = z(t) + b_T w_T(t) \ndot z(t) =  b_z w_z(t)\nendaligned","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"This model is linear, and can be written on the form","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"beginaligned\ndot x = Ax + Bw \ny = Cx + e\nendaligned","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"with A matrix ","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"A = beginbmatrix\n0  1 \n0  0\nendbmatrix","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"which, when discretized (assuming unit sample interval), becomes","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"A = beginbmatrix\n1  1 \n0  1\nendbmatrix","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"A,B,C,D = SA[1.0 1; 0 1], @SMatrix(zeros(2,0)), SA[1.0 0], 0;\nnothing # hide","category":"page"},{"location":"fault_detection/#Picking-covariance-matrices","page":"Fault detection","title":"Picking covariance matrices","text":"","category":"section"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"R1 = 1e-4LowLevelParticleFilters.double_integrator_covariance(1) |> SMatrix{2,2}\nR2 = SA[0.1^2;;]\nd0 = LowLevelParticleFilters.SimpleMvNormal(SA[y[1], 0], SA[100.0 0; 0 0.1])\nkf = KalmanFilter(A,B,C,D,R1,R2,d0; Ts)","category":"page"},{"location":"fault_detection/#Perform-filtering","page":"Fault detection","title":"Perform filtering","text":"","category":"section"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"When data is missing, we omit the call to correct!. We still perform the prediction step though.","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"function special_forward_trajectory(kf::AbstractKalmanFilter, u::AbstractVector, y::AbstractVector, p=parameters(kf))\n    reset!(kf)\n    T    = length(y)\n    x    = Array{particletype(kf)}(undef,T)\n    xt   = Array{particletype(kf)}(undef,T)\n    R    = Array{covtype(kf)}(undef,T)\n    Rt   = Array{covtype(kf)}(undef,T)\n    e    = zeros(eltype(particletype(kf)), length(y))\n\tσs   = zeros(eltype(particletype(kf)), length(y))\n    ll   = zero(eltype(particletype(kf)))\n    for t = 1:T\n        ti = (t-1)*kf.Ts\n        x[t]  = state(kf)      |> copy\n        R[t]  = covariance(kf) |> copy\n\t\tif !any(ismissing, y[t])\n        \tlli, ei, S, Sᵪ = correct!(kf, u[t], y[t], p, ti)\n\t\t\tσs[t] = √(ei'*(Sᵪ\\ei)) # Compute the Z-score\n\t\t\te[t] = ei[]\n\t\t\tll += lli\n\t\tend\n        xt[t] = state(kf)      |> copy\n        Rt[t] = covariance(kf) |> copy\n        predict!(kf, u[t], p, ti)\n    end\n    KalmanFilteringSolution(kf,u,y,x,xt,R,Rt,ll,e), σs\nend\n\nu_full = [@SVector(zeros(0)) for y in y_full];\n\nstart = 1 # Change this value to display different parts of the data set\nN = 1000  # Number of data points to include (to limit plot size in the docs, plot with Plots.plotly() and N = length(y_full) to see the full data set with the ability to zoom interactively in the plot)\n\nsol, σs = special_forward_trajectory(kf, u_full[(1:N) .+ (start-1)], y_full[(1:N) .+ (start-1)])\n\nsol.ll","category":"page"},{"location":"fault_detection/#Smoothing","page":"Fault detection","title":"Smoothing","text":"","category":"section"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"For good measure, we also perform smoothing, computing","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"x(k  T_f)","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"as opposed to filtering which is computing","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"x(k  k)","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"or prediction","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"x(k  k-1)","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"smoothsol = smooth(sol)\nnothing # hide","category":"page"},{"location":"fault_detection/#Visualize-the-filtered-and-smoothed-trajectories","page":"Fault detection","title":"Visualize the filtered and smoothed trajectories","text":"","category":"section"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"timevec = range(0, step=Ts, length=length(sol.y))\n\nplot(smoothsol,\n    plotx   = false, # prediction\n    plotxt  = true,  # filtered\n    plotxT  = true,  # smoothed\n    plotRt  = true,\n    plotRT  = true,\n    plotyh  = false,\n    plotyht = true,\n    size = (650,600), seriestype = [:line :line :scatter :line], link = :x,\n)\nplot!(timevec, reduce(hcat, smoothsol.xT)[1,:], sp=3, label=\"Smoothed\")\nDisplayAs.PNG(Plots.current()) # hide","category":"page"},{"location":"fault_detection/#Estimate-the-dynamics-covariance-using-maximum-likelihood-estimation-(MLE)","page":"Fault detection","title":"Estimate the dynamics covariance using maximum-likelihood estimation (MLE)","text":"","category":"section"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"Since we have a single parameter only, we may plot the loss landscape.","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"svec = exp10.(range(-5, -2, length=30)) # Covariance values to try\n\n# Compute the log-likelihood for all covariance values\nlls = map(svec) do s # \n\tR1 = s*LowLevelParticleFilters.double_integrator_covariance(1) |> SMatrix{2,2}\n\tkf = KalmanFilter(A,B,C,D,R1,R2,d0; Ts)\n\tsol, σs = special_forward_trajectory(kf, u_full, y_full)\n\tsol.ll\nend\n\nplot(svec, lls, xscale=:log10, title=\"Log-likelihood estimation\")","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"Get the covariance parameter associated with the maximum likelihood:","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"svec[argmax(lls)]","category":"page"},{"location":"fault_detection/#Optimize-\"friction\"-and-covariance-jointly","page":"Fault detection","title":"Optimize \"friction\" and covariance jointly","text":"","category":"section"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"We can add some damping to the velocity state in the double-integrator model. When doing so, we should also estimate the full covariance matrix of the dynamics noise. This gives us an estimation problem with 1 + 3 parameters, 3 for the triangular part of the covariance matrix Cholesky factor. Estimating the Cholesky factor instead of the full covariance matrix yields fewer optimizaiton variables and ensures that the result is a valid, positive definite and symmetric covariance matrix. To ensure that the \"friction parameter\" is positive, we optimize the log of the parameter.","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"A double integrator has the dynamics matrix","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"beginbmatrix\n1  1 \n0  1\nendbmatrix","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"By modifying this to","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"beginbmatrix\n1  1 \n0  alpha\nendbmatrix","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"where 0 leq alpha leq 1, we can add some damping to the velocity, i.e., if no force is acting on it it will eventually slow down to velocity zero. It's not quite correct to call the parameter alpha a \"damping term\", the formulation beta = 1 - alpha would be closer to an actual discrete-time damping factor.","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"function triangular(x)\n    m = length(x)\n    n = round(Int, sqrt(2m-1))\n    T = zeros(eltype(x), n, n)\n    k = 1\n    for i = 1:n, j = i:n\n        T[i,j] = x[k]\n        k += 1\n    end\n    T\nend\n\ninvtriangular(T) = [T[i,j] for i = 1:size(T,1) for j = i:size(T,1)]\n\nparams = log.([invtriangular(cholesky(R1).U); 1])\n\nfunction get_opt_kf(logp)\n\tT = eltype(logp)\n\tp = exp.(logp)\n\tR1c = triangular(p[1:3]) |> SMatrix{2,2}\n\tR1 = R1c'R1c + 1e-8I\n\tvel = p[4]\n\tvel > 1 && (return T(Inf))\n\tA = SA[1 1; 0 vel]\n\td0T = LowLevelParticleFilters.SimpleMvNormal(T.(d0.μ), T.(d0.Σ + 0.01I))\n\tkf = KalmanFilter(A,B,C,D,R1,R2,d0T; Ts, check=false)\nend\n\nfunction cost(logp)\n\ttry\n\t\tkf = get_opt_kf(logp)\n\t\tsoli, σs = special_forward_trajectory(kf, u_full, y_full)\n\t\treturn -soli.ll\n\tcatch e\n\t\treturn eltype(logp)(Inf)\n\tend\nend\n\ncost(params)","category":"page"},{"location":"fault_detection/#Optimize","page":"Fault detection","title":"Optimize","text":"","category":"section"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"res = Optim.optimize(\n    cost,\n    params,\n    LBFGS(),\n    Optim.Options(\n        show_trace        = true,\n        show_every        = 5,\n        iterations        = 1000,\n\t\tx_tol \t\t\t  = 1e-7,\n    ),\n\tautodiff = :forward,\n)\nget_opt_kf(res.minimizer).R1","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"The initial guess was ","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"R1","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"Compare optimized parameter vector with initial guess:","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"exp.([params res.minimizer])","category":"page"},{"location":"fault_detection/#Visualize-optimized-filtering-trajectory","page":"Fault detection","title":"Visualize optimized filtering trajectory","text":"","category":"section"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"kf2 = get_opt_kf(res.minimizer)\nsol2, σs2 = special_forward_trajectory(kf2, u_full[(1:N) .+ (start-1)], y_full[(1:N) .+ (start-1)])\n\nsmoothsol2 = smooth(sol2, kf2, sol2.u, sol2.y)\n\nplot(smoothsol2, plotx=false, plotxt=true, plotRt=true, plotyh=false, plotyht=true, size=(650,600), seriestype=[:line :line :scatter :line], link=:x)\nplot!(timevec, reduce(hcat, smoothsol2.xT)[1,:], sp=3, label=\"Smoothed\")\n\noutliers = findall(σs2 .> 5)\nvline!([timevec[outliers]], sp=3)\nDisplayAs.PNG(Plots.current()) # hide","category":"page"},{"location":"fault_detection/#Fault-detection-2","page":"Fault detection","title":"Fault detection","text":"","category":"section"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"We implement a simple fault detector using Z-scores. When the Z-score is higher than 4, we consider it a fault.","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"plot(timevec, σs2); hline!([1 2 3 4], label=false)\nDisplayAs.PNG(Plots.current()) # hide","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"(change the value of the variable start to see different parts of the data set, e.g., set start = 30_000)","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"Z-scores may not capture large outliers if they occur when the estimator is very uncertain Does Z-score correlate with \"velocity\", i.e., are faults correlated with large continuous slopes in the data?","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"sol_full, σs_full = special_forward_trajectory(kf2, u_full, y_full)\nscatter(abs.(getindex.(sol_full.xt, 2)), σs_full, ylabel=\"Z-score\", xlabel=\"velocity\")\nDisplayAs.PNG(Plots.current()) # hide","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"not really, it looks like large Z-scores can appear even when the estimated velocity is small.","category":"page"},{"location":"fault_detection/#Summary","page":"Fault detection","title":"Summary","text":"","category":"section"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"A state estimator can indicate faults when the error is larger than expected\nWhat is expected is determined by the model","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"The notebook used in the tutorial is available here:","category":"page"},{"location":"fault_detection/","page":"Fault detection","title":"Fault detection","text":"identification_12_fault_detection.jl on GitHub","category":"page"},{"location":"discretization/#Discretization","page":"Discretization","title":"Discretization","text":"","category":"section"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"This package operates exclusively on discrete-time dynamics, and dynamics describing, e.g., ODE systems must thus be discretized. This page describes the details around discretization for nonlinear and linear systems, as well as how to discretize continuous-time noise processes. ","category":"page"},{"location":"discretization/#Nonlinear-ODEs","page":"Discretization","title":"Nonlinear ODEs","text":"","category":"section"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"Continuous-time dynamics functions on the form (x,u,p,t) -> ẋ can be discretized (integrated) using the function SeeToDee.Rk4, e.g.,","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"using SeeToDee\ndiscrete_dynamics = SeeToDee.Rk4(continuous_dynamics, sampletime; supersample=1)","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"where the integer supersample determines the number of RK4 steps that is taken internally for each change of the control signal (1 is often sufficient and is the default). The returned function discrete_dynamics is on the form (x,u,p,t) -> x⁺.","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"note: Note\nWhen solving state-estimation problems, accurate integration is often less important than during simulation. The motivations for this are severalThe dynamics model is often inaccurate, and solving an inaccurate model to high accuracy can be a waste of effort.\nThe performance is often dictated by the disturbances acting on the system.\nState-estimation enjoys feedback from measurements that corrects for slight errors due to integration.","category":"page"},{"location":"discretization/#Linear-systems","page":"Discretization","title":"Linear systems","text":"","category":"section"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"A linear system on the form ","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"beginaligned\ndotx(t) = Ax(t) + Bu(t)\ny(t) = Cx(t) + Du(t)\nendaligned","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"can be discretized using ControlSystems.c2d, which defaults to a zero-order hold discretization. See the example below for more info.","category":"page"},{"location":"discretization/#Covariance-matrices","page":"Discretization","title":"Covariance matrices","text":"","category":"section"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"Covariance matrices for continuous-time noise processes can also be discretized using ControlSystems.c2d","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"using ControlSystemIdentification\nR1d      = c2d(sys::StateSpace{Continuous}, R1c, Ts)\nR1d, R2d = c2d(sys::StateSpace{Continuous}, R1c, R2c, Ts)\nR1d      = c2d(sys::StateSpace{Discrete},   R1c)\nR1d, R2d = c2d(sys::StateSpace{Discrete},   R1c, R2c)","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"This samples a continuous-time covariance matrix to fit the provided system sys.","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"The method used comes from theorem 5 in the reference below.","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"Ref: \"Discrete-time Solutions to the Continuous-time Differential Lyapunov Equation With Applications to Kalman Filtering\",  Patrik Axelsson and Fredrik Gustafsson","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"On singular covariance matrices: The traditional double integrator with covariance matrix Q = diagm([0,σ²]) warrants special consideration since it is rank-deficient, i.e., it indicates that there is a single source of randomness only, despite the presence of two state variables. If we assume that the noise is piecewise constant, we can use the input matrix (\"Cholesky factor\") of Q, e.g., the noise of variance σ² enters like N = [0, 1] which is sampled using ZoH and becomes Nd = [Ts^2 / 2; Ts] which results in the covariance matrix σ² * Nd * Nd' (see example below). If we assume that the noise is a continuous-time white noise process, the discretized covariance matrix is full rank and can be computed by c2d(sys::StateSpace{Continuous}, R1c, Ts) or directly by the function double_integrator_covariance_smooth. In some applications, a rank-1 approximation to this matrix is favored, notably, when using an augmented UnscentedKalmanFilter. In such case, a good rank-1 approximation to this matrix is obtained by double_integrator_covariance(Ts, σ2) ./ Ts. This has the benefit of being both low rank, and produce covariance dynamics that are approximately invariant to the choice of sample interval. If the ZoH assumption is made, the covariance matrix is rank 1 but the covariance dynamics are not invariant to the choice of sample interval.`","category":"page"},{"location":"discretization/#Example","page":"Discretization","title":"Example","text":"","category":"section"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"The following example will discretize a linear double integrator system. Double integrators arise when the position of an object is controlled by a force, i.e., when Newtons second law f = ma governs the dynamics. The system can be written on the form","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"beginaligned\ndot x(t) = Ax(t) + Bu(t) + Nw(t)\ny(t) = Cx(t) + e(t)\nendaligned","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"where N = B are both equal to [0, 1], indicating that the noise w(t) enters like a force (this could be for instance due to air resistance or friction).","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"We start by defining the system that takes u as an input and discretize that with a sample time of T_s = 01.","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"using ControlSystemsBase\nA = [0 1; 0 0]\nB = [0; 1;;]\nC = [1 0]\nD = 0\nTs = 0.1 # Sample time\n\nsys = ss(A,B,C,D)\nsysd = c2d(sys, Ts) # Discretize the dynamics","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"We then form another system, this time with w(t) as the input, and thus N as the input matrix instead of B. We assume that the noise has a standard deviation of sigma_1 = 05","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"σ1 = 0.5\nN  = σ1*[0; 1;;]\nsys_w  = ss(A,N,C,D)\nsys_wd = c2d(sys_w, Ts) # Discretize the noise system\nNd  = sys_wd.B # The discretized noise input matrix\nR1d = Nd*Nd' # The final discrete-time covariance matrix","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"We can verify that the matrix we computed corresponds to the theoretical covariance matrix for a discrete-time double integrator where the noise is piecewise constant:","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"R1d ≈ σ1^2*[Ts^2 / 2; Ts]*[Ts^2 / 2; Ts]'","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"If the noise is not piecewise constant the discretized covariance matrix will be full rank, but a good rank-1 approximation in this case is R1d ./ Ts.","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"For a nonlinear system, we could adopt a similar strategy by first linearizing the system around a suitable operating point. Alternatively, we could make use of the fact that some of the state estimators in this package allows the covariance matrices to be functions of the state, and thus compute a new discretized covariance matrix using a linearization around the current state.","category":"page"},{"location":"discretization/#Sample-interval-insensitive-tuning","page":"Discretization","title":"Sample-interval insensitive tuning","text":"","category":"section"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"When the dynamics covariance of a state estimator is tuned, it may be desirable to have the covariance dynamics be approximately invariant to the choice of sample interval T_s. To achieve this, construct the covariance matrix as R1 = [...] ./ Ts, i.e., tune a matrix that is scaled by the inverse of the sample interval. If you later change Ts, you'll get approximately the same performance of the estimator for prediction intervals during which there are no measurements available.","category":"page"},{"location":"discretization/#Non-uniform-sample-rates","page":"Discretization","title":"Non-uniform sample rates","text":"","category":"section"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"Special care is needed if the sample rate is not constant, i.e., the time interval between measurements varies. ","category":"page"},{"location":"discretization/#Dropped-samples","page":"Discretization","title":"Dropped samples","text":"","category":"section"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"A common case is that the sample rate is constant, but some measurements are lost. This case is very easy to handle; the filter loop iterates between two steps","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"Prediction using predict!(filter, x, u, p, t)\nCorrection using\ncorrect!(f, u, y, p, t) if using the standard measurement model of the filter\ncorrect!(f, mm, u, y, p, t, mm) to use a custom measurement model mm","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"If a measurement y is lacking, one simply skips the corresponding call to correct! where y is missing. Repeated calls to predict! corresponds to simulating the system without any feedback from measurements, like if an ODE was solved. Internally, the filter will keep track of the covariance of the estimate, which is likely to grow if no measurements are used to inform the filter about the state of the system.","category":"page"},{"location":"discretization/#Sensors-with-different-sample-rates","page":"Discretization","title":"Sensors with different sample rates","text":"","category":"section"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"For Kalman-type filters, it is possible to construct custom measurement models, and pass an instance of a measurement model as the second argument to correct!. This allows for sensor fusion with sensors operating at different rates, or when parts of the measurement model are linear, and other parts are nonlinear. See examples in Measurement models for how to construct explicit measurement models.","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"A video demonstrating the use of multiple measurement models running at different rates is available on YouTube:","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"<iframe style=\"height: 315px; width: 560px\" src=\"https://www.youtube.com/embed/BLsJrW5XXcg?si=bkob76-uJj27-S80\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","category":"page"},{"location":"discretization/#Stochastic-sample-rate","page":"Discretization","title":"Stochastic sample rate","text":"","category":"section"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"In some situations, such as in event-based systems, the sample rate is truly stochastic. There is no single correct way of handling this, and we instead outline some alternative approaches.","category":"page"},{"location":"discretization/","page":"Discretization","title":"Discretization","text":"If the filtering is performed offline on a batch of data, time-varying dynamics can be used, for instance by supplying matrices to a KalmanFilter on the form A[:, :, t], R1[:, :, t]. Each A and R1 is then computed as the discretization with the sample time given as the time between measurement t and measurement t+1.\nA conceptually simple approach is to choose a very small sample interval T_s which is smaller than the smallest occurring sample interval in the data, and approximate each sample interval by rounding it to the nearest integer multiple of T_s. This transforms the problem to an instance of the \"dropped samples\" problem described above.\nMake use of an adaptive integrator instead of the fixed-step rk4 supplied in this package, and manually keep track of the step length that needs to be taken as well as the adjustment to the dynamics covariance.","category":"page"},{"location":"benchmark/#Benchmarks","page":"Benchmark","title":"Benchmarks","text":"","category":"section"},{"location":"benchmark/#Particle-filtering","page":"Benchmark","title":"Particle filtering","text":"","category":"section"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"To see how the performance varies with the number of particles, we simulate several times. The following code simulates the system and performs filtering using the simulated measurements. We do this for varying number of time steps and varying number of particles.","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"note: Note\nTo run this code, see the bottom of src/example_lineargaussian.jl.","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"function run_test()\n    particle_count = [10, 20, 50, 100, 200, 500, 1000]\n    time_steps = [20, 100, 200]\n    RMSE = zeros(length(particle_count),length(time_steps)) # Store the RMS errors\n    propagated_particles = 0\n    t = @elapsed for (Ti,T) = enumerate(time_steps)\n        for (Ni,N) = enumerate(particle_count)\n            montecarlo_runs = 2*maximum(particle_count)*maximum(time_steps) ÷ T ÷ N\n            E = sum(1:montecarlo_runs) do mc_run\n                pf = ParticleFilter(N, dynamics, measurement, df, dg, d0) # Create filter\n                u = @SVector randn(2)\n                x = SVector{2,Float64}(rand(rng, d0))\n                y = SVector{2,Float64}(sample_measurement(pf,x,u,0,1))\n                error = 0.0\n                @inbounds for t = 1:T-1\n                    pf(u, y) # Update the particle filter\n                    x = dynamics(x,u,t) + SVector{2,Float64}(rand(rng, df)) # Simulate the true dynamics and add some noise\n                    y = SVector{2,Float64}(sample_measurement(pf,x,u,0,t)) # Simulate a measuerment\n                    u = @SVector randn(2) # draw a random control input\n                    error += sum(abs2,x-weighted_mean(pf))\n                end # t\n                √(error/T)\n            end # MC\n            RMSE[Ni,Ti] = E/montecarlo_runs\n            propagated_particles += montecarlo_runs*N*T\n            @show N\n        end # N\n        @show T\n    end # T\n    println(\"Propagated $propagated_particles particles in $t seconds for an average of $(propagated_particles/t/1000) particles per millisecond\")\n    return RMSE\nend\n\n@time RMSE = run_test()","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"Propagated 8400000 particles in 1.140468043 seconds for an average of 7365.397085484139 particles per millisecond","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"We then plot the results","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"time_steps     = [20, 100, 200]\nparticle_count = [10, 20, 50, 100, 200, 500, 1000]\nnT             = length(time_steps)\nleg            = reshape([\"$(time_steps[i]) time steps\" for i = 1:nT], 1,:)\nplot(particle_count,RMSE,xscale=:log10, ylabel=\"RMS errors\", xlabel=\" Number of particles\", lab=leg)","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"(Image: window)","category":"page"},{"location":"benchmark/#Comparison-against-filterpy","page":"Benchmark","title":"Comparison against filterpy","text":"","category":"section"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"filterpy is a popular Python library for state estimation. Below, we compare performance on their UKF example, but we use a longer trajectory of 50k time steps:","category":"page"},{"location":"benchmark/#Python-implementation","page":"Benchmark","title":"Python implementation","text":"","category":"section"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"from filterpy import *\nfrom filterpy.kalman import *\nimport numpy as np\nfrom numpy.random import randn\nfrom filterpy.common import Q_discrete_white_noise\nimport time\ndef fx(x, dt):\n    # state transition function - predict next state based\n    # on constant velocity model x = vt + x_0\n    F = np.array([[1, dt, 0, 0],\n                  [0, 1, 0, 0],\n                  [0, 0, 1, dt],\n                  [0, 0, 0, 1]], dtype=float)\n    return np.dot(F, x)\n\ndef hx(x):\n   # measurement function - convert state into a measurement\n   # where measurements are [x_pos, y_pos]\n   return np.array([x[0], x[2]])\n\ndt = 0.1\n# create sigma points to use in the filter. This is standard for Gaussian processes\npoints = MerweScaledSigmaPoints(4, alpha=.1, beta=2., kappa=-1)\n\nkf = UnscentedKalmanFilter(dim_x=4, dim_z=2, dt=dt, fx=fx, hx=hx, points=points)\nkf.x = np.array([-1., 1., -1., 1]) # initial state\nkf.P *= 0.2 # initial uncertainty\nz_std = 0.1\nkf.R = np.diag([z_std**2, z_std**2]) # 1 standard\nkf.Q = Q_discrete_white_noise(dim=2, dt=dt, var=0.01**2, block_size=2)\n\nzs = [[i+randn()*z_std, i+randn()*z_std] for i in range(50000)] # measurements\n\nstart_time = time.time()\n\nfor z in zs:\n    kf.predict()\n    kf.update(z)\n    # print(kf.x, 'log-likelihood', kf.log_likelihood)\n\nend_time = time.time()\nprint(f\"Execution time: {end_time - start_time} seconds\")","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"Execution time: 6.390492916107178 seconds","category":"page"},{"location":"benchmark/#Julia-implementation","page":"Benchmark","title":"Julia implementation","text":"","category":"section"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"using LowLevelParticleFilters, StaticArrays, LinearAlgebra, BenchmarkTools\nconst dt = 0.1\nfunction fx(x,u,p,t)\n    # state transition function - predict next state based\n    # on constant velocity model x = vt + x_0\n    F = SA[1.0 dt 0.0 0.0;\n         0.0 1.0 0.0 0.0;\n         0.0 0.0 1.0 dt;\n         0.0 0.0 0.0 1.0]\n    return F*x\nend\n\nfunction hx(x,u,p,t)\n    # measurement function - convert state into a measurement\n    # where measurements are [x_pos, y_pos]\n    return x[SA[1,3]]\nend\n\nx0 = SA[-1.0, 1.0, -1.0, 1.0] # initial state\nR0 = 0.2I(4) # initial uncertainty\nz_std = 0.1\nR1 = LowLevelParticleFilters.double_integrator_covariance(dt, 0.01^2)\nR1 = SMatrix{4,4}(cat(R1, R1, dims=(1,2)))  # Called Q in the Python code\nR2 = Diagonal(SA[z_std^2, z_std^2])         # Called R in the Python code\nd0 = LowLevelParticleFilters.SimpleMvNormal(x0, R0)\nukf = UnscentedKalmanFilter(fx, hx, R1, R2, d0; nu=0, ny=2, Ts=dt, p=nothing)\nzs = [[i+randn()*z_std, i+randn()*z_std] for i in 1:50000] # measurements\n\nfunction runsim(ukf, zs)\n    for z in zs\n        predict!(ukf, SA[])\n        ll, _ = correct!(ukf, SA[], z)\n        # @show ll\n    end\nend\n\nrunsim(ukf, zs)\n\ntime_julia = @belapsed runsim($ukf, $zs)","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"0.017676814","category":"page"},{"location":"benchmark/#Result","page":"Benchmark","title":"Result","text":"","category":"section"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"time_python = 6.390492916107178\ntime_python / time_julia","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"361.51836615507625","category":"page"},{"location":"benchmark/","page":"Benchmark","title":"Benchmark","text":"The Julia version is about 360x faster than the Python version.","category":"page"},{"location":"#LowLevelParticleFilters","page":"Home","title":"LowLevelParticleFilters","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: CI) (Image: codecov)","category":"page"},{"location":"","page":"Home","title":"Home","text":"This is a library for state estimation, that is, given measurements y(t) from a dynamical system, estimate the state vector x(t). Throughout, we assume dynamics on the form","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\nx(t+1) = f(x(t) u(t) p t w(t))\ny(t) = g(x(t) u(t) p t e(t))\nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"or the linear version","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\nx(t+1) = Ax(t) + Bu(t) + w(t)\ny(t) = Cx(t) + Du(t) + e(t)\nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"where x is the state vector, u an input, p some form of parameters, t is the time and we are disturbances (noise). Throughout the documentation, we often call the function f dynamics and the function g measurement.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The dynamics above describe a discrete-time system, i.e., the function f takes the current state and produces the next state. This is in contrast to a continuous-time system, where f takes the current state but produces the time derivative of the state. A continuous-time system can be discretized, described in detail in Discretization.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The parameters p can be anything, or left out. You may write the dynamics functions such that they depend on p and include parameters when you create a filter object. You may also override the parameters stored in the filter object when you call any function on the filter object. This behavior is modeled after the SciML ecosystem.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Depending on the nature of f and g, the best method of estimating the state may vary. If fg are linear and the disturbances are additive and Gaussian, the KalmanFilter is an optimal state estimator. If any of the above assumptions fail to hold, we may need to resort to more advanced estimators. This package provides several filter types, outlined below.","category":"page"},{"location":"#Estimator-types","page":"Home","title":"Estimator types","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We provide a number of filter types","category":"page"},{"location":"","page":"Home","title":"Home","text":"KalmanFilter. A standard Kalman filter. Is restricted to linear dynamics (possibly time varying) and Gaussian noise.\nSqKalmanFilter. A standard Kalman filter on square-root form (slightly slower but more numerically stable with ill-conditioned covariance).\nExtendedKalmanFilter: For nonlinear systems, the EKF runs a regular Kalman filter on linearized dynamics. Uses ForwardDiff.jl for linearization (or user provided). The noise model must still be Gaussian and additive.\nIteratedExtendedKalmanFilter same as EKF, but performs iteration in the measurement update for increased accuracy in the covariance update.\nUnscentedKalmanFilter: The Unscented Kalman filter often performs slightly better than the Extended Kalman filter but may be slightly more computationally expensive. The UKF handles nonlinear dynamics and measurement models, but still requires a Gaussian noise model (may be non additive) and still assumes that all posterior distributions are Gaussian, i.e., can not handle multi-modal posteriors.\nParticleFilter: The particle filter is a nonlinear estimator. This version of the particle filter is simple to use and assumes that both dynamics noise and measurement noise are additive. Particle filters handle multi-modal posteriors.\nAdvancedParticleFilter: This filter gives you more flexibility, at the expense of having to define a few more functions. This filter does not require the noise to be additive and is thus the most flexible filter type.\nAuxiliaryParticleFilter: This filter is identical to ParticleFilter, but uses a slightly different proposal mechanism for new particles.\nIMM: (Currently considered experimental) The Interacting Multiple Models filter switches between multiple internal filters based on a hidden Markov model. This filter is useful when the system dynamics change over time and the change can be modeled as a discrete Markov chain, i.e., the system may switch between a small number of discrete \"modes\".\nRBPF: A Rao-Blackwellized particle filter that uses a Kalman filter for the linear part of the state and a particle filter for the nonlinear part.","category":"page"},{"location":"#Functionality","page":"Home","title":"Functionality","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Filtering, estimating x(t) given measurements up to and including time t. We call the filtered estimate x(tt) (read as x at t given t).\nSmoothing, estimating x(t) given data up to T  t, i.e., x(tT).\nParameter estimation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"All filters work in two distinct steps.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The prediction step (predict!). During prediction, we use the dynamics model to form x(tt-1) = f(x(t-1) )\nThe correction step (correct!). In this step, we adjust the predicted state x(tt-1) using the measurement y(t) to form x(tt).","category":"page"},{"location":"","page":"Home","title":"Home","text":"The following two exceptions to the above exist","category":"page"},{"location":"","page":"Home","title":"Home","text":"The IMM filter has two additional steps, combine! and interact!\nThe AuxiliaryParticleFilter makes use of the next measurement in the dynamics update, and thus only has an update! method.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In general, all filters represent not only a point estimate of x(t), but a representation of the complete posterior probability distribution over x given all the data available up to time t. One major difference between different filter types is how they represent these probability distributions.","category":"page"},{"location":"#Particle-filter","page":"Home","title":"Particle filter","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A particle filter represents the probability distribution over the state as a collection of samples, each sample is propagated through the dynamics function f individually. When a measurement becomes available, the samples, called particles, are given a weight based on how likely the particle is given the measurement. Each particle can thus be seen as representing a hypothesis about the current state of the system. After a few time steps, most weights are inevitably going to be extremely small, a manifestation of the curse of dimensionality, and a resampling step is incorporated to refresh the particle distribution and focus the particles on areas of the state space with high posterior probability.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Defining a particle filter (ParticleFilter) is straightforward, one must define the distribution of the noise df in the dynamics function, dynamics(x,u,p,t) and the noise distribution dg in the measurement function measurement(x,u,p,t). Both of these noise sources are assumed to be additive, but can have any distribution (see AdvancedParticleFilter for non-additive noise). The distribution of the initial state estimate d0 must also be provided. In the example below, we use linear Gaussian dynamics so that we can easily compare both particle and Kalman filters. (If we have something close to linear Gaussian dynamics in practice, we should of course use a Kalman filter and not a particle filter.)","category":"page"},{"location":"","page":"Home","title":"Home","text":"using LowLevelParticleFilters, LinearAlgebra, StaticArrays, Distributions, Plots\nusing DisplayAs # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"Define problem","category":"page"},{"location":"","page":"Home","title":"Home","text":"nx = 2   # Dimension of state\nnu = 1   # Dimension of input\nny = 1   # Dimension of measurements\nN = 500  # Number of particles\n\nconst dg = MvNormal(ny,0.2)          # Measurement noise Distribution\nconst df = MvNormal(nx,0.1)          # Dynamics noise Distribution\nconst d0 = MvNormal(randn(nx),2.0)   # Initial state Distribution\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"Define linear state-space system (using StaticArrays for maximum performance)","category":"page"},{"location":"","page":"Home","title":"Home","text":"const A = SA[0.97043   -0.097368\n             0.09736    0.970437]\nconst B = SA[0.1; 0;;]\nconst C = SA[0 1.0]\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"Next, we define the dynamics and measurement equations, they both take the signature (x,u,p,t) = (state, input, parameters, time) ","category":"page"},{"location":"","page":"Home","title":"Home","text":"dynamics(x,u,p,t) = A*x .+ B*u\nmeasurement(x,u,p,t) = C*x\nvecvec_to_mat(x) = copy(reduce(hcat, x)') # Helper function\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"the parameter p can be anything, and is often optional. If p is not provided when performing operations on filters, any p stored in the filter objects (if supported) is used. The default if none is provided and none is stored in the filter is p = LowLevelParticleFilters.NullParameters().","category":"page"},{"location":"","page":"Home","title":"Home","text":"We are now ready to define and use a filter","category":"page"},{"location":"","page":"Home","title":"Home","text":"pf = ParticleFilter(N, dynamics, measurement, df, dg, d0)","category":"page"},{"location":"","page":"Home","title":"Home","text":"With the filter in hand, we can simulate from its dynamics and query some properties","category":"page"},{"location":"","page":"Home","title":"Home","text":"du = MvNormal(nu,1.0)         # Random input distribution for simulation\nxs,u,y = simulate(pf,200,du) # We can simulate the model that the pf represents\npf(u[1], y[1])               # Perform one filtering step using input u and measurement y\nparticles(pf)                # Query the filter for particles, try weights(pf) or expweights(pf) as well\nx̂ = weighted_mean(pf)        # using the current state","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you want to perform batch filtering using an existing trajectory consisting of vectors of inputs and measurements, try any of the functions forward_trajectory, mean_trajectory:","category":"page"},{"location":"","page":"Home","title":"Home","text":"sol = forward_trajectory(pf, u, y) # Filter whole trajectories at once\nx̂,ll = mean_trajectory(pf, u, y)\nplot(sol, xreal=xs, markersize=2)\nDisplayAs.PNG(Plots.current()) # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"u ad y are then assumed to be vectors of vectors. StaticArrays is recommended for maximum performance.","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can also plot weighted quantiles instead of 2D histograms by providing a vector of desired quantiles through the q keyword argument","category":"page"},{"location":"","page":"Home","title":"Home","text":"plot(sol, xreal=xs, markersize=2, q=[0.1, 0.5, 0.9], ploty=false, legend=true)\nDisplayAs.PNG(Plots.current()) # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"If MonteCarloMeasurements.jl is loaded, you may transform the output particles to Matrix{MonteCarloMeasurements.Particles} with the layout T × n_state using Particles(x,we). Internally, the particles are then resampled such that they all have unit weight. This is conventient for making use of the plotting facilities of MonteCarloMeasurements.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For a full usage example, see the benchmark section below or example_lineargaussian.jl","category":"page"},{"location":"#Resampling","page":"Home","title":"Resampling","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The particle filter will perform a resampling step whenever the distribution of the weights has become degenerate. The resampling is triggered when the effective number of samples is smaller than pf.resample_threshold in 0 1, this value can be set when constructing the filter. How the resampling is done is governed by pf.resampling_strategy, we currently provide ResampleSystematic <: ResamplingStrategy as the only implemented strategy. See https://en.wikipedia.org/wiki/Particle_filter for more info.","category":"page"},{"location":"#Particle-Smoothing","page":"Home","title":"Particle Smoothing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Smoothing is the process of finding the best state estimate given both past and future data. Smoothing is thus only possible in an offline setting. This package provides a particle smoother, based on forward filtering, backward simulation (FFBS), example usage follows:","category":"page"},{"location":"","page":"Home","title":"Home","text":"N     = 2000 # Number of particles\nT     = 80   # Number of time steps\nM     = 100  # Number of smoothed backwards trajectories\npf    = ParticleFilter(N, dynamics, measurement, df, dg, d0)\ndu    = MvNormal(nu,1)     # Control input distribution\nx,u,y = simulate(pf,T,du) # Simulate trajectory using the model in the filter\ntosvec(y) = reinterpret(SVector{length(y[1]),Float64}, reduce(hcat,y))[:] |> copy\nx,u,y = tosvec.((x,u,y)) # It's good for performance to use StaticArrays to the extent possible\n\nxb,ll = smooth(pf, M, u, y) # Sample smoothing particles\nxbm   = smoothed_mean(xb)   # Calculate the mean of smoothing trajectories\nxbc   = smoothed_cov(xb)    # And covariance\nxbt   = smoothed_trajs(xb)  # Get smoothing trajectories\nxbs   = [diag(xbc) for xbc in xbc] |> vecvec_to_mat .|> sqrt\nplot(xbm', ribbon=2xbs, lab=\"PF smooth\")\nplot!(vecvec_to_mat(x), l=:dash, lab=\"True\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can plot the particles themselves as well","category":"page"},{"location":"","page":"Home","title":"Home","text":"downsample = 5\nplot(vecvec_to_mat(x), l=(4,), layout=(2,1), show=false)\nscatter!(xbt[1, 1:downsample:end, :]', subplot=1, show=false, m=(1,:black, 0.5), lab=\"\")\nscatter!(xbt[2, 1:downsample:end, :]', subplot=2, m=(1,:black, 0.5), lab=\"\")\nDisplayAs.PNG(Plots.current()) # hide","category":"page"},{"location":"#Kalman-filter","page":"Home","title":"Kalman filter","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The KalmanFilter (wiki) assumes that f and g are linear functions, i.e., that they can be written on the form","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\nx(t+1) = Ax(t) + Bu(t) + w(t)\ny(t) = Cx(t) + Du(t) + e(t)\nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"for some matrices ABCD where w sim N(0 R_1) and e sim N(0 R_2) are zero mean and Gaussian. The Kalman filter represents the posterior distributions over x by the mean and a covariance matrix. The magic behind the Kalman filter is that linear transformations of Gaussian distributions remain Gaussian, and we thus have a very efficient way of representing them.","category":"page"},{"location":"","page":"Home","title":"Home","text":"A Kalman filter is easily created using the constructor KalmanFilter. Many of the functions defined for particle filters, are defined also for Kalman filters, e.g.:","category":"page"},{"location":"","page":"Home","title":"Home","text":"R1 = cov(df)\nR2 = cov(dg)\nkf = KalmanFilter(A, B, C, 0, R1, R2, d0)\nsol = forward_trajectory(kf, u, y) # sol contains filtered state, predictions, pred cov, filter cov, loglik\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"It can also be called in a loop like the pf above","category":"page"},{"location":"","page":"Home","title":"Home","text":"for t = 1:T\n    kf(u,y) # Performs both correct! and predict!\n    # alternatively\n    ll, e = correct!(kf, y, nothing, t) # Returns loglikelihood and prediction error (plus other things if you want)\n    x     = state(kf)       # Access the state estimate\n    R     = covariance(kf)  # Access the covariance of the estimate\n    predict!(kf, u, nothing, t)\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"The matrices in the Kalman filter may be time varying, such that A[:, :, t] is A(t). They may also be provided as functions on the form A(t) = A(x u p t). This works for both dynamics and covariance matrices. When providing functions, the dimensions of the state, input and output, nx, nu, ny, must be provided as keyword arguments to the filter constructor since these cannot be inferred from the function signature.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The numeric type used in the Kalman filter is determined from the mean of the initial state distribution, so make sure that this has the correct type if you intend to use, e.g., Float32 or ForwardDiff.Dual for automatic differentiation.","category":"page"},{"location":"#Smoothing-using-KF","page":"Home","title":"Smoothing using KF","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Kalman filters can also be used for smoothing ","category":"page"},{"location":"","page":"Home","title":"Home","text":"kf = KalmanFilter(A, B, C, 0, cov(df), cov(dg), d0)\nsmoothsol = smooth(kf, u, y) # Returns a smoothing solution including smoothed state and smoothed cov\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"Plot and compare PF and KF","category":"page"},{"location":"","page":"Home","title":"Home","text":"# plot(smoothsol) The smoothing solution object can also be plotted directly\nplot(vecvec_to_mat(smoothsol.xT), lab=\"Kalman smooth\", layout=2)\nplot!(xbm', lab=\"pf smooth\")\nplot!(vecvec_to_mat(x), lab=\"true\")","category":"page"},{"location":"#Kalman-filter-tuning-tutorial","page":"Home","title":"Kalman filter tuning tutorial","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The tutorial \"How to tune a Kalman filter\" details how to figure out appropriate covariance matrices for the Kalman filter, as well as how to add disturbance models to the system model.","category":"page"},{"location":"#Unscented-Kalman-Filter","page":"Home","title":"Unscented Kalman Filter","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The UnscentedKalmanFilter represents posterior distributions over x as Gaussian distributions just like the KalmanFilter, but propagates them through a nonlinear function f by a deterministic sampling of a small number of particles called sigma points (this is referred to as the unscented transform). This UKF thus handles nonlinear functions fg, but only Gaussian disturbances and unimodal posteriors. The UKF will by default treat the noise as additive, but by using the augmented UKF form, non-additive noise may be handled as well. See the docstring of UnscentedKalmanFilter for more details.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The UKF takes the same arguments as a regular KalmanFilter, but the matrices defining the dynamics are replaced by two functions, dynamics and measurement, working in the same way as for the ParticleFilter above (unless the augmented form is used).","category":"page"},{"location":"","page":"Home","title":"Home","text":"ukf = UnscentedKalmanFilter(dynamics, measurement, cov(df), cov(dg), MvNormal(SA[1.,1.]); nu, ny)","category":"page"},{"location":"","page":"Home","title":"Home","text":"info: Info\nIf your function dynamics describes a continuous-time ODE, do not forget to discretize it before passing it to the UKF. See Discretization for more information.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The UnscentedKalmanFilter has many customization options, see the docstring for more details. In particular, the UKF may be created with a linear measurement model as an optimization.","category":"page"},{"location":"#Extended-Kalman-Filter","page":"Home","title":"Extended Kalman Filter","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The ExtendedKalmanFilter (EKF) is similar to the UKF, but propagates Gaussian distributions by linearizing the dynamics and using the formulas for linear systems similar to the standard Kalman filter. This can be slightly faster than the UKF (not always), but also less accurate for strongly nonlinear systems. The linearization is performed automatically using ForwardDiff.jl unless the user provides Jacobian functions that compute A and C. In general, the UKF is recommended over the EKF unless the EKF is faster and computational performance is the top priority.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The EKF constructor has the following two signatures","category":"page"},{"location":"","page":"Home","title":"Home","text":"ExtendedKalmanFilter(dynamics, measurement, R1, R2, d0=MvNormal(R1); nu::Int, p = LowLevelParticleFilters.NullParameters(), α = 1.0, check = true, Ajac = nothing, Cjac = nothing)\nExtendedKalmanFilter(kf, dynamics, measurement; Ajac = nothing, Cjac = nothing)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The first constructor takes all the arguments required to initialize the extended Kalman filter, while the second one takes an already defined standard Kalman filter. using the first constructor, the user must provide the number of inputs to the system, nu.","category":"page"},{"location":"","page":"Home","title":"Home","text":"where kf is a standard KalmanFilter from which the covariance properties are taken.","category":"page"},{"location":"","page":"Home","title":"Home","text":"info: Info\nIf your function dynamics describes a continuous-time ODE, do not forget to discretize it before passing it to the UKF. See Discretization for more information.","category":"page"},{"location":"#AdvancedParticleFilter","page":"Home","title":"AdvancedParticleFilter","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The AdvancedParticleFilter works very much like the ParticleFilter, but admits more flexibility in its noise models.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The AdvancedParticleFilter type requires you to implement the same functions as the regular ParticleFilter, but in this case you also need to handle sampling from the noise distributions yourself. The function dynamics must have a method signature like below. It must provide one method that accepts state vector, control vector, parameter, time and noise::Bool that indicates whether or not to add noise to the state. If noise should be added, this should be done inside dynamics An example is given below","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Random\nconst rng = Random.Xoshiro()\nfunction dynamics(x, u, p, t, noise=false) # It's important that `noise` defaults to false\n    x = A*x .+ B*u # A simple linear dynamics model in discrete time\n    if noise\n        x += rand(rng, df) # it's faster to supply your own rng\n    end\n    x\nend\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"The measurement_likelihood function must have a method accepting state, input, measurement, parameter and time, and returning the log-likelihood of the measurement given the state, a simple example below:","category":"page"},{"location":"","page":"Home","title":"Home","text":"function measurement_likelihood(x, u, y, p, t)\n    logpdf(dg, C*x-y) # An example of a simple linear measurement model with normal additive noise\nend\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"This gives you very high flexibility. The noise model in either function can, for instance, be a function of the state, something that is not possible for the simple ParticleFilter. To be able to simulate the AdvancedParticleFilter like we did with the simple filter above, the measurement method with the signature measurement(x,u,p,t,noise=false) must be available and return a sample measurement given state (and possibly time). For our example measurement model above, this would look like this","category":"page"},{"location":"","page":"Home","title":"Home","text":"# This function is only required for simulation\nmeasurement(x, u, p, t, noise=false) = C*x + noise*rand(rng, dg)\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"We now create the AdvancedParticleFilter and use it in the same way as the other filters:","category":"page"},{"location":"","page":"Home","title":"Home","text":"apf = AdvancedParticleFilter(N, dynamics, measurement, measurement_likelihood, df, d0)\nsol = forward_trajectory(apf, u, y, ny) # Perform batch filtering","category":"page"},{"location":"","page":"Home","title":"Home","text":"plot(sol, xreal=x)\nDisplayAs.PNG(Plots.current()) # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can even use this type as an AuxiliaryParticleFilter","category":"page"},{"location":"","page":"Home","title":"Home","text":"apfa = AuxiliaryParticleFilter(apf)\nsol = forward_trajectory(apfa, u, y, ny)\nplot(sol, dim=1, xreal=x) # Same as above, but only plots a single dimension\nDisplayAs.PNG(Plots.current()) # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"See the tutorials section for more advanced examples, including state estimation for DAE (Differential-Algebraic Equation) systems.","category":"page"},{"location":"#Troubleshooting-and-tuning","page":"Home","title":"Troubleshooting and tuning","text":"","category":"section"},{"location":"#Particle-filters","page":"Home","title":"Particle filters","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Tuning a particle filter can be quite the challenge. To assist with this, we provide som visualization tools","category":"page"},{"location":"","page":"Home","title":"Home","text":"debugplot(pf,u[1:20],y[1:20], runall=true, xreal=x[1:20])","category":"page"},{"location":"","page":"Home","title":"Home","text":"The plot displays all state variables and all measurements. The heatmap in the background represents the weighted particle distributions per time step. For the measurement sequences, the heatmap represent the distributions of predicted measurements. The blue dots corresponds to measured values. In this case, we simulated the data and we had access to the state as well, if we do not have that, just omit xreal. You can also manually step through the time-series using","category":"page"},{"location":"","page":"Home","title":"Home","text":"commandplot(pf,u,y; kwargs...)","category":"page"},{"location":"","page":"Home","title":"Home","text":"For options to the debug plots, see ?pplot.","category":"page"},{"location":"#Troubleshooting-Kalman-filters","page":"Home","title":"Troubleshooting Kalman filters","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A commonly occurring error is \"Cholesky factorization failed\", which may occur due to several different reasons","category":"page"},{"location":"","page":"Home","title":"Home","text":"The dynamics is diverging and the covariance matrices end up with NaNs or Infs. If this is the case, verify that the dynamics is correctly implemented and that the integration is sufficiently accurate, especially if using a fixed-step integrator like any of those from SeeToDee.jl.\nThe covariance matrix is poorly conditioned and numerical issues make causes it to lose positive definiteness. This issue is rare, but can be mitigated by using the SqKalmanFilter, rescaling the dynamics or by using a different cholesky factorization method (available in UKF only).","category":"page"},{"location":"#Tuning-noise-parameters-through-optimization","page":"Home","title":"Tuning noise parameters through optimization","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"See examples in Parameter Estimation.","category":"page"},{"location":"#Tuning-through-simulation","page":"Home","title":"Tuning through simulation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"It is possible to sample from the Bayesian model implied by a filter and its parameters by calling the function simulate. A simple tuning strategy is to adjust the noise parameters such that a simulation looks \"similar\" to the data, i.e., the data must not be too unlikely under the model.","category":"page"},{"location":"#Videos","page":"Home","title":"Videos","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Several video tutorials using this package are available in the playlists","category":"page"},{"location":"","page":"Home","title":"Home","text":"System identification in Julia\nControl systems in Julia","category":"page"},{"location":"","page":"Home","title":"Home","text":"Some examples featuring this package in particular are","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Using an optimizer to optimize the likelihood of an UnscentedKalmanFilter:","category":"page"},{"location":"","page":"Home","title":"Home","text":"<iframe style=\"height: 315px; width: 560px\" src=\"https://www.youtube.com/embed/0RxQwepVsoM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Estimation of time-varying parameters:","category":"page"},{"location":"","page":"Home","title":"Home","text":"<iframe style=\"height: 315px; width: 560px\" src=\"https://www.youtube.com/embed/zJcOPPLqv4A?si=XCvpo3WD-4U3PJ2S\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Adaptive control by means of estimation of time-varying parameters:","category":"page"},{"location":"","page":"Home","title":"Home","text":"<iframe style=\"height: 315px; width: 560px\" src=\"https://www.youtube.com/embed/Ip_prmA7QTU?si=Fat_srMTQw5JtW2d\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","category":"page"}]
}
