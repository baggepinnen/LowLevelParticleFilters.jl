var documenterSearchIndex = {"docs":
[{"location":"api/#Exported-functions-and-types-1","page":"API","title":"Exported functions and types","text":"","category":"section"},{"location":"api/#Index-1","page":"API","title":"Index","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"","category":"page"},{"location":"api/#","page":"API","title":"API","text":"Modules = [LowLevelParticleFilters]\nPrivate = false","category":"page"},{"location":"api/#LowLevelParticleFilters.AdvancedParticleFilter-Tuple{Integer, Function, Function, Any, Any, Any}","page":"API","title":"LowLevelParticleFilters.AdvancedParticleFilter","text":"AdvancedParticleFilter(Nparticles, dynamics, measurement, measurement_likelihood, dynamics_density, initial_density; p = SciMLBase.NullParameters(), kwargs...)\n\nSee the docs for more information: https://baggepinnen.github.io/LowLevelParticleFilters.jl/stable/#AdvancedParticleFilter-1\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.AuxiliaryParticleFilter-Tuple","page":"API","title":"LowLevelParticleFilters.AuxiliaryParticleFilter","text":"AuxiliaryParticleFilter(args...; kwargs...)\n\nTakes exactly the same arguments as ParticleFilter, or an instance of ParticleFilter.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.DAEUnscentedKalmanFilter-Tuple{Any}","page":"API","title":"LowLevelParticleFilters.DAEUnscentedKalmanFilter","text":"DAEUnscentedKalmanFilter(ukf; g, get_x_z, build_xz, xz0, threads=false)\n\nAn Unscented Kalman filter for differential-algebraic systems (DAE).\n\nRef: \"Nonlinear State Estimation of Differential Algebraic Systems\",  Mandela, Rengaswamy, Narasimhan\n\nwarning: Warning\nThis filter is still considered experimental and subject to change without respecting semantic versioning. Use at your own risk.\n\nArguments\n\nukf is a regular UnscentedKalmanFilter that contains dynamics(xz, u, p, t) that propagates the combined state xz(k) to xz(k+1) and a measurement function with signature (xz, u, p, t)\ng(x, z, u, p, t) is a function that should fulfill g(x, z, u, p, t) = 0\n\n_ get_x_z(xz) -> x, z is a function that decomposes xz into x and z\n\nbuild_xz(x, z) is the inverse of get_x_z\nxz0 the initial full state.\nthreads: If true, evaluates dynamics on Sigma points in parallel. This typically requires the dynamics to be non-allocating (use StaticArrays) to improve performance. \n\nAssumptions\n\nThe DAE dynamics is index 1 and can be written on the form \n\nẋ = f(x, z, u, p, t) # Differential equations\n0 = g(x, z, u, p, t) # Algebraic equations\ny = h(x, z, u, p, t) # Measurements\n\nthe measurements may be functions of both differential states x and algebraic variables z. Note, the actual dynamcis and measurement functions stored in the internal ukf should have signatures (xz, u, p, t), i.e., they take the combined state containing both x and z in a single vector as dictated by the function build_xz. It is only the function g that is assumed to actually have the signature g(x,z,u,p,t).\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.ExtendedKalmanFilter","page":"API","title":"LowLevelParticleFilters.ExtendedKalmanFilter","text":"ExtendedKalmanFilter(kf, dynamics, measurement)\n\nA nonlinear state estimator propagating uncertainty using linearization.\n\nAn extended Kalman filter takes a standard Kalman filter as well as dynamics and measurement functions. The filter will linearize the dynamics using ForwardDiff. The dynamics and measurement function are on the following form\n\nx' = dynamics(x, u, p, t) + w\ny  = measurement(x, u, p, t) + e\n\nwhere w ~ N(0, R1), e ~ N(0, R2) and x(0) ~ d0\n\nSee also UnscentedKalmanFilter which is typically more accurate than ExtendedKalmanFilter. See KalmanFilter for detailed instructions on how to set up the Kalman filter kf.\n\n\n\n\n\n","category":"type"},{"location":"api/#LowLevelParticleFilters.KalmanFilter","page":"API","title":"LowLevelParticleFilters.KalmanFilter","text":"KalmanFilter(A,B,C,D,R1,R2,d0=MvNormal(R1); p = SciMLBase.NullParameters())\n\nThe matrices A,B,C,D define the dynamics\n\nx' = Ax + Bu + w\ny  = Cx + Du + e\n\nwhere w ~ N(0, R1), e ~ N(0, R2) and x(0) ~ d0\n\nThe matrices can be time varying such that, e.g., A[:, :, t] contains the A matrix at time index t. They can also be given as functions on the form\n\nAfun(x, u, p, t) -> A\n\nFor maximum performance, provide statically sized matrices from StaticArrays.jl\n\n\n\n\n\n","category":"type"},{"location":"api/#LowLevelParticleFilters.ParticleFilter-Tuple{Integer, Function, Function, Any, Any, Any}","page":"API","title":"LowLevelParticleFilters.ParticleFilter","text":"ParticleFilter(num_particles, dynamics, measurement, dynamics_density, measurement_density, initial_density; p = SciMLBase.NullParameters())\n\nSee the docs for more information: https://baggepinnen.github.io/LowLevelParticleFilters.jl/stable/#Particle-filter-1\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.TupleProduct","page":"API","title":"LowLevelParticleFilters.TupleProduct","text":"TupleProduct(v::NTuple{N,UnivariateDistribution})\n\nCreate a product distribution where the individual distributions are stored in a tuple. Supports mixed/hybrid Continuous and Discrete distributions\n\n\n\n\n\n","category":"type"},{"location":"api/#LowLevelParticleFilters.UnscentedKalmanFilter","page":"API","title":"LowLevelParticleFilters.UnscentedKalmanFilter","text":"UnscentedKalmanFilter(dynamics, measurement, R1, R2, d0=MvNormal(Matrix(R1)); p = SciMLBase.NullParameters(), ny, nu)\n\nA nonlinear state estimator propagating uncertainty using the unscented transform.\n\nThe dynamics and measurement function are on the following form\n\nx' = dynamics(x, u, p, t) + w\ny  = measurement(x, u, p, t) + e\n\nwhere w ~ N(0, R1), e ~ N(0, R2) and x(0) ~ d0\n\nThe matrices R1, R2 can be time varying such that, e.g., R1[:, :, t] contains the R_1 matrix at time index t. They can also be given as functions on the form\n\nRfun(x, u, p, t) -> R\n\nFor maximum performance, provide statically sized matrices from StaticArrays.jl\n\nny, nu indicate the number of outputs and inputs.\n\n\n\n\n\n","category":"type"},{"location":"api/#LowLevelParticleFilters.commandplot","page":"API","title":"LowLevelParticleFilters.commandplot","text":"commandplot(pf, u, y, p=parameters(pf); kwargs...)\n\nProduce a helpful plot. For customization options (kwargs...), see ?pplot. After each time step, a command from the user is requested.\n\nq: quit\ns n: step n steps\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.correct!","page":"API","title":"LowLevelParticleFilters.correct!","text":"ll, e = correct!(f, u, y, p = parameters(f), t = index(f))\n\nUpdate state/covariance/weights based on measurement y,  returns loglikelihood and prediction error (the error is always 0 for particle filters).\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.debugplot","page":"API","title":"LowLevelParticleFilters.debugplot","text":"debugplot(pf, u, y, p=parameters(pf); runall=false, kwargs...)\n\nProduce a helpful plot. For customization options (kwargs...), see ?pplot.\n\nrunall=false: if true, runs all time steps befor displaying (faster), if false, displays the plot after each time step.\n\nThe generated plot becomes quite heavy. Initially, try limiting your input to 100 time steps to verify that it doesn't crash.\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.densityplot","page":"API","title":"LowLevelParticleFilters.densityplot","text":"densityplot(x,[w])\n\nPlot (weighted) particles densities\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.forward_trajectory","page":"API","title":"LowLevelParticleFilters.forward_trajectory","text":"sol = forward_trajectory(kf::AbstractKalmanFilter, u::Vector, y::Vector, p=parameters(kf))\n\nRun a Kalman filter forward\n\nReturns a KalmanFilteringSolution: with the following\n\nx: predictions\nxt: filtered estimates\nR: predicted covariance matrices\nRt: filter covariances\nll: loglik\n\nsol can be plotted\n\nplot(sol::KalmanFilteringSolution; plotx = true, plotxt=true, plotu=true, ploty=true)\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.forward_trajectory","page":"API","title":"LowLevelParticleFilters.forward_trajectory","text":"sol = forward_trajectory(pf, u::AbstractVector, y::AbstractVector, p=parameters(pf))\n\nRun the particle filter for a sequence of inputs and measurements. Return a solution with x,w,we,ll = particles, weights, expweights and loglikelihood\n\nIf MonteCarloMeasurements.jl is loaded, you may transform the output particles to Matrix{MonteCarloMeasurements.Particles} using Particles(x,we). Internally, the particles are then resampled such that they all have unit weight. This is conventient for making use of the plotting facilities of MonteCarloMeasurements.jl.\n\nsol can be plotted\n\nplot(sol::ParticleFilteringSolution; nbinsy=30, xreal=nothing, dim=nothing)\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.log_likelihood_fun-Tuple{Any, Vector{<:Distributions.Distribution}, Any, Any, Any}","page":"API","title":"LowLevelParticleFilters.log_likelihood_fun","text":"ll(θ) = log_likelihood_fun(filter_from_parameters(θ::Vector)::Function, priors::Vector{Distribution}, u, y, p)\n\nreturns function θ -> p(y|θ)p(θ)\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.loglik","page":"API","title":"LowLevelParticleFilters.loglik","text":"ll = loglik(filter, u, y, p=parameters(filter))\n\nCalculate loglikelihood for entire sequences u,y\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.logsumexp!","page":"API","title":"LowLevelParticleFilters.logsumexp!","text":"ll = logsumexp!(w, we [, maxw])\n\nNormalizes the weight vector w and returns the weighted log-likelihood\n\nhttps://arxiv.org/pdf/1412.8695.pdf eq 3.8 for p(y) https://discourse.julialang.org/t/fast-logsumexp/22827/7?u=baggepinnen for stable logsumexp\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.mean_trajectory-Tuple{Any, Vector, Vector}","page":"API","title":"LowLevelParticleFilters.mean_trajectory","text":"x,ll = mean_trajectory(pf, u::Vector{Vector}, y::Vector{Vector}, p=parameters(pf))\n\nThis Function resets the particle filter to the initial state distribution upon start\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.metropolis","page":"API","title":"LowLevelParticleFilters.metropolis","text":"metropolis(ll::Function(θ), R::Int, θ₀::Vector, draw::Function(θ) = naive_sampler(θ₀))\n\nPerforms MCMC sampling using the marginal Metropolis (-Hastings) algorithm draw = θ -> θ' samples a new parameter vector given an old parameter vector. The distribution must be symmetric, e.g., a Gaussian. R is the number of iterations. See log_likelihood_fun\n\nExample:\n\nfilter_from_parameters(θ) = ParticleFilter(N, dynamics, measurement, MvNormal(n,exp(θ[1])), MvNormal(p,exp(θ[2])), d0)\npriors = [Normal(0,0.1),Normal(0,0.1)]\nll     = log_likelihood_fun(filter_from_parameters,priors,u,y,1)\nθ₀ = log.([1.,1.]) # Initial point\ndraw = θ -> θ .+ rand(MvNormal(0.1ones(2))) # Function that proposes new parameters (has to be symmetric)\nburnin = 200 # If using threaded call, provide number of burnin iterations\n# @time theta, lls = metropolis(ll, 2000, θ₀, draw) # Run single threaded\n# thetam = reduce(hcat, theta)'\n@time thetalls = LowLevelParticleFilters.metropolis_threaded(burnin, ll, 5000, θ₀, draw) # run on all threads, will provide (2000-burnin)*nthreads() samples\nhistogram(exp.(thetalls[:,1:2]), layout=3)\nplot!(thetalls[:,3], subplot=3) # if threaded call, log likelihoods are in the last column\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.predict!","page":"API","title":"LowLevelParticleFilters.predict!","text":"predict!(f, u, p = parameters(f), t = index(f))\n\nMove filter state forward in time using dynamics equation and input vector u.\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.reset!-Tuple{LowLevelParticleFilters.AbstractParticleFilter}","page":"API","title":"LowLevelParticleFilters.reset!","text":"Reset the filter to initial state and covariance/distribution\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.simulate","page":"API","title":"LowLevelParticleFilters.simulate","text":"x,u,y = simulate(f::AbstractFilter, T::Int, du::Distribution, p=parameters(f), [N]; dynamics_noise=true)\nx,u,y = simulate(f::AbstractFilter, u, p=parameters(f); dynamics_noise=true)\n\nSimulate dynamical system forward in time T steps, or for the duration of u, returns state sequence, inputs and measurements du is a distribution of random inputs.\n\nIf MonteCarloMeasurements.jl is loaded, the argument N::Int can be supplied, in which case N simulations are done and the result is returned in the form of Vector{MonteCarloMeasurements.Particles}.\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.smooth","page":"API","title":"LowLevelParticleFilters.smooth","text":"xT,RT,ll = smooth(kf::KalmanFilter, u::Vector, y::Vector, p=parameters(kf))\n\nReturns smoothed estimates of state x and covariance R given all input output data u,y\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.smooth","page":"API","title":"LowLevelParticleFilters.smooth","text":"xb,ll = smooth(pf, M, u, y, p=parameters(pf))\nxb,ll = smooth(pf, xf, wf, wef, ll, M, u, y, p=parameters(pf))\n\nPerform particle smoothing using forward-filtering, backward simulation. Return smoothed particles and loglikelihood. See also smoothed_trajs, smoothed_mean, smoothed_cov\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.smoothed_cov-Tuple{Any}","page":"API","title":"LowLevelParticleFilters.smoothed_cov","text":"smoothed_cov(xb)\n\nHelper function to calculate the covariance of smoothed particle trajectories\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.smoothed_mean-Tuple{Any}","page":"API","title":"LowLevelParticleFilters.smoothed_mean","text":"smoothed_mean(xb)\n\nHelper function to calculate the mean of smoothed particle trajectories\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.smoothed_trajs-Tuple{Any}","page":"API","title":"LowLevelParticleFilters.smoothed_trajs","text":"smoothed_trajs(xb)\n\nHelper function to get particle trajectories as a 3-dimensions array (N,M,T) instead of matrix of vectors.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.update!","page":"API","title":"LowLevelParticleFilters.update!","text":"ll, e = update!(f::AbstractFilter, u, y, p = parameters(f), t = index(f))\n\nPerform one step of predict! and correct!, returns loglikelihood and prediction error\n\n\n\n\n\n","category":"function"},{"location":"api/#LowLevelParticleFilters.weigthed_cov-Tuple{Any, Any}","page":"API","title":"LowLevelParticleFilters.weigthed_cov","text":"weigthed_cov(x,we)\n\nSimilar to weigthed_mean, but returns covariances\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.weigthed_mean-Tuple{Any, AbstractVector}","page":"API","title":"LowLevelParticleFilters.weigthed_mean","text":"x̂ = weigthed_mean(x,we)\n\nCalculated weighted mean of particle trajectories. we are expweights.\n\n\n\n\n\n","category":"method"},{"location":"api/#LowLevelParticleFilters.weigthed_mean-Tuple{Any}","page":"API","title":"LowLevelParticleFilters.weigthed_mean","text":"x̂ = weigthed_mean(pf)\nx̂ = weigthed_mean(s::PFstate)\n\n\n\n\n\n","category":"method"},{"location":"api/#","page":"API","title":"API","text":"LowLevelParticleFilters.rk4","category":"page"},{"location":"api/#LowLevelParticleFilters.rk4","page":"API","title":"LowLevelParticleFilters.rk4","text":"f_discrete = rk4(f, Ts; supersample = 1)\n\nDiscretize a continuous-time dynamics function f using RK4 with sample time Ts.  f is assumed to have the signature f : (x,u,p,t)->ẋ and the returned function f_discrete : (x,u,p,t)->x(t+1).\n\nsupersample determins the number of internal steps, 1 is often sufficient.\n\n\n\n\n\n","category":"function"},{"location":"parameter_estimation/#Parameter-Estimation-1","page":"Parameter estimation","title":"Parameter Estimation","text":"","category":"section"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"State estimation is an integral part of many parameter-estimation methods. Below, we will illustrate several different methods of performing parameter estimation. We can roughly divide the methods into two camps","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"Methods that optimize prediction error or likelihood by tweaking model parameters.\nMethods that add the parameters to be estimated as states in the model and estimate them using standard state estimation. ","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"From the first camp, we provide som basic functionality for maximum likelihood estimation and MAP estimation, described below. An example of 2), joint state and parameter estimation, is provided in Joint state and parameter estimation.","category":"page"},{"location":"parameter_estimation/#Maximum-likelihood-estimation-1","page":"Parameter estimation","title":"Maximum-likelihood estimation","text":"","category":"section"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"Filters calculate the likelihood and prediction errors while performing filtering, this can be used to perform maximum likelihood estimation or prediction-error minimization. We may for example plot likelihood as function of the variance of the dynamics noise","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"using LowLevelParticleFilters, LinearAlgebra, StaticArrays, Distributions, Plots\nnx = 2   # Dimension of state\nnu = 2   # Dimension of input\nny = 2   # Dimension of measurements\nN = 800 # Number of particles\n\nconst dg = MvNormal(ny,1.0)          # Measurement noise Distribution\nconst df = MvNormal(nx,1.0)          # Dynamics noise Distribution\nconst d0 = MvNormal(randn(nx),2.0)   # Initial state Distribution\n\nTr = randn(nx,nx)\nconst A = SA[1 0.1; 0 1]\nconst B = @SMatrix [0.0 0.1; 1 0.1]\nconst C = @SMatrix [1.0 0; 0 1]\n\ndynamics(x,u,p,t) = A*x .+ B*u \nmeasurement(x,u,p,t) = C*x\nvecvec_to_mat(x) = copy(reduce(hcat, x)') # Helper function\npf = ParticleFilter(N, dynamics, measurement, df, dg, d0)\nxs,u,y = simulate(pf,300,df)","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"p = nothing\nsvec = exp10.(LinRange(-1.0, 1.2, 60))\nllspf = map(svec) do s\n    df = MvNormal(nx,s)\n    pfs = ParticleFilter(2000, dynamics, measurement, df, dg, d0)\n    loglik(pfs, u, y, p)\nend\nplot( svec, llspf,\n    xscale = :log10,\n    title = \"Log-likelihood\",\n    xlabel = \"Dynamics noise standard deviation\",\n    lab = \"PF\",\n)\nvline!([svec[findmax(llspf)[2]]], l=(:dash,:blue), primary=false)","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"We can do the same with a Kalman filter","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"eye(n) = Matrix{Float64}(I,n,n)\nllskf = map(svec) do s\n    kfs = KalmanFilter(A, B, C, 0, s^2*eye(nx), eye(ny), d0)\n    loglik(kfs, u, y, p)\nend\nplot!(svec, llskf, yscale=:identity, xscale=:log10, lab=\"Kalman\", c=:red)\nvline!([svec[findmax(llskf)[2]]], l=(:dash,:red), primary=false)","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"as we can see, the result is quite noisy due to the stochastic nature of particle filtering.","category":"page"},{"location":"parameter_estimation/#MAP-estiamtion-1","page":"Parameter estimation","title":"MAP estiamtion","text":"","category":"section"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"In this example, we will estimate the variance of the noises in the dynamics and the measurement functions.","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"To solve a MAP estimation problem, we need to define a function that takes a parameter vector and returns a filter, the parameters are used to construct the covariance matrices:","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"filter_from_parameters(θ, pf = nothing) = KalmanFilter(A, B, C, 0, exp(θ[1])^2*eye(nx), exp(θ[2])^2*eye(ny), d0) # Works with particle filters as well\nnothing # hide","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"The call to exp on the parameters is so that we can define log-normal priors","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"priors = [Normal(0,2),Normal(0,2)]","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"Now we call the function log_likelihood_fun that returns a function to be minimized","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"ll = log_likelihood_fun(filter_from_parameters, priors, u, y, p)\nnothing # hide","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"Since this is a low-dimensional problem, we can plot the LL on a 2d-grid","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"function meshgrid(a,b)\n    grid_a = [i for i in a, j in b]\n    grid_b = [j for i in a, j in b]\n    grid_a, grid_b\nend\nNv       = 20\nv        = LinRange(-0.7,1,Nv)\nllxy     = (x,y) -> ll([x;y])\nVGx, VGy = meshgrid(v,v)\nVGz      = llxy.(VGx, VGy)\nheatmap(\n    VGz,\n    xticks = (1:Nv, round.(v, digits = 2)),\n    yticks = (1:Nv, round.(v, digits = 2)),\n    xlabel = \"sigma v\",\n    ylabel = \"sigma w\",\n) # Yes, labels are reversed","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"For higher-dimensional problems, we may estimate the parameters using an optimizer, e.g., Optim.jl.","category":"page"},{"location":"parameter_estimation/#Bayesian-inference-using-PMMH-1","page":"Parameter estimation","title":"Bayesian inference using PMMH","text":"","category":"section"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"We proceed like we did for MAP above, but when calling the function metropolis, we will get the entire posterior distribution of the parameter vector, for the small cost of a massive increase in the amount of computations.","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"N = 1000\nfilter_from_parameters(θ, pf = nothing) = KalmanFilter(A, B, C, 0, exp(θ[1])^2*I(nx), exp(θ[2])^2*I(ny), d0) # Works with particle filters as well\nnothing # hide","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"The call to exp on the parameters is so that we can define log-normal priors","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"priors = [Normal(0,2),Normal(0,2)]\nll     = log_likelihood_fun(filter_from_parameters, priors, u, y, p)\nθ₀     = log.([1.0, 1.0]) # Starting point\nnothing # hide","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"We also need to define a function that suggests a new point from the \"proposal distribution\". This can be pretty much anything, but it has to be symmetric since I was lazy and simplified an equation.","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"draw   = θ -> θ .+ 0.05 .* rand.()\nburnin = 200\n@info \"Starting Metropolis algorithm\"\n@time theta, lls = metropolis(ll, 2200, θ₀, draw) # Run PMMH for 2200  iterations\nthetam = reduce(hcat, theta)'[burnin+1:end,:] # Build a matrix of the output (was vecofvec)\nhistogram(exp.(thetam), layout=(3,1)); plot!(lls[burnin+1:end], subplot=3) # Visualize","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"If you are lucky, you can run the above threaded as well. I tried my best to make particle filters thread safe with their own rngs etc., but your milage may vary. For threading to help, the dynamics must be non-allocating, e.g., by using StaticArrays etc.","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"@time thetalls = LowLevelParticleFilters.metropolis_threaded(burnin, ll, 2200, θ₀, draw, nthreads=2)\nhistogram(exp.(thetalls[:,1:2]), layout=3)\nplot!(thetalls[:,3], subplot=3)","category":"page"},{"location":"parameter_estimation/#Joint-state-and-parameter-estimation-1","page":"Parameter estimation","title":"Joint state and parameter estimation","text":"","category":"section"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"In this example, we'll show how to perform parameter estimation by treating a parameter as a state. This method can not only estimate constant parameters, but also time-varying parameters. The system we will consider is a quadruple tank, where two upper tanks feed into two lower tanks. The outlet for tank 1 can vary in size, simulating, e.g., that something partially blocks the outlet. We start by defining the dynamics on a form that changes the outlet area a_1 at time t=500:","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"using LowLevelParticleFilters\nusing Distributions\nusing StaticArrays\nusing Plots, LinearAlgebra\n\nfunction quadtank(h,u,p,t)\n    kc = 0.5\n    k1, k2, g = 1.6, 1.6, 9.81\n    A1 = A3 = A2 = A4 = 4.9\n    a1, a3, a2, a4 = 0.03, 0.03, 0.03, 0.03\n    γ1, γ2 = 0.2, 0.2\n\n    if t > 500\n        a1 *= 2 # Change the parameter at t = 500\n    end\n\n    ssqrt(x) = √(max(x, zero(x)) + 1e-3) # For numerical robustness at x = 0\n    \n    xd = SA[\n        -a1/A1 * ssqrt(2g*h[1]) + a3/A1*ssqrt(2g*h[3]) +     γ1*k1/A1 * u[1]\n        -a2/A2 * ssqrt(2g*h[2]) + a4/A2*ssqrt(2g*h[4]) +     γ2*k2/A2 * u[2]\n        -a3/A3*ssqrt(2g*h[3])                          + (1-γ2)*k2/A3 * u[2]\n        -a4/A4*ssqrt(2g*h[4])                          + (1-γ1)*k1/A4 * u[1]\n    ]\nend\n\nnu = 2 # number of control inputs\nnx = 4 # number of states\nny = 2 # number of measured outputs\nTs = 1 # sample time\nnothing # hide","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"We then define a measurement function, we measure the levels of tanks 1 and 2, and discretize the continuous-time dynamics using a Runge-Kutta 4 integrator LowLevelParticleFilters.rk4:","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"measurement(x,u,p,t) = SA[x[1], x[2]]\ndiscrete_dynamics = LowLevelParticleFilters.rk4(quadtank, Ts, supersample=2)\nnothing # hide","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"We simulate the system using the rollout function and add some noise to the measurements. The inputs in this case are just square waves.","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"Tperiod = 200\nt = 0:Ts:1000\nu = vcat.(0.25 .* sign.(sin.(2pi/Tperiod .* t)) .+ 0.25)\nu = vcat.(u,u)\nx0 = Float64[2,2,3,3]\nx = LowLevelParticleFilters.rollout(discrete_dynamics, x0, u)[1:end-1]\ny = measurement.(x, u, 0, 0)\ny = [y .+ 0.01randn(ny) for y in y]\n\nplot(\n    plot(reduce(hcat, x)', title=\"States\"),\n    plot(reduce(hcat, u)', title=\"Inputs\")\n)","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"To perform the joint state and parameter estimation, we define a version of the dynamics that contains an extra state, corresponding to the unknown or time varying parameter, in this case a_1. We do not have any apriori information about how this parameter changes, so we say that its derivative is 0 and it's thus only driven by noise:","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"function quadtank_paramest(h, u, p, t)\n    kc = 0.5\n    k1, k2, g = 1.6, 1.6, 9.81\n    A1 = A3 = A2 = A4 = 4.9\n    a3, a2, a4 = 0.03, 0.03, 0.03\n    γ1, γ2 = 0.2, 0.2\n\n    a1 = h[5] # the a1 parameter is a state\n\n    ssqrt(x) = √(max(x, zero(x)) + 1e-3) # For numerical robustness at x = 0\n    \n    xd = SA[\n        -a1/A1 * ssqrt(2g*h[1]) + a3/A1*ssqrt(2g*h[3]) +     γ1*k1/A1 * u[1]\n        -a2/A2 * ssqrt(2g*h[2]) + a4/A2*ssqrt(2g*h[4]) +     γ2*k2/A2 * u[2]\n        -a3/A3*ssqrt(2g*h[3])                          + (1-γ2)*k2/A3 * u[2]\n        -a4/A4*ssqrt(2g*h[4])                          + (1-γ1)*k1/A4 * u[1]\n        0 # the state is only driven by noise\n    ]\nend\n\ndiscrete_dynamics_params = LowLevelParticleFilters.rk4(quadtank_paramest, Ts, supersample=2)\nnothing # hide","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"We then define a nonlinear state estimator, we will use the UnscentedKalmanFilter, and solve the filtering problem. We start by an initial state estimate x_0 that is slightly off for the parameter a_1","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"nx = 5\nR1 = Diagonal([0.1, 0.1, 0.1, 0.1, 0.0001])\nR2 = Diagonal((1e-2)^2 * ones(ny))\nx0 = [2, 2, 3, 3, 0.02]\n\nkf = UnscentedKalmanFilter(discrete_dynamics_params, measurement, R1, R2, MvNormal(x0, R1); ny, nu)\n\nsol = forward_trajectory(kf, u, y)\nplot(sol, plotx=false, plotxt=true, plotu=false, ploty=true, legend=:bottomright)\nplot!([0,500,500,1000], [0.03, 0.03, 0.06, 0.06], l=(:dash, :black), sp=5, lab=\"True param\")","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"as we can see, the correct value of the parameter is quickly found (x_5), and it also adapts at t=500 when the parameter value changes. The speed with which the parameter adapts to changes is determined by the covariance matrix R_1, a higher value results in faster adaptation, but also higher sensitivity to noise. ","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"If adaptive parameter estimation is coupled with a model-based controller, we get an adaptive controller! Note: the state that corresponds to the estimated parameter is typically not controllable, a fact that may require some special care for some control methods.","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"We may ask ourselves, what's the difference between a parameter and a state variable if we can add parameters as states? Typically, parameters do not vary with time, and if they do, they vary significantly slower than the states. State variables also have dynamics associate with them, whereas we often have no idea about how the parameters vary other than that they vary slowly.","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"Abrupt changes to the dynamics like in the example above can happen in practice, for instance, due to equipment failure or change of operating mode. This can be treated as a scenario with time-varying parameters that are continuously estimated. ","category":"page"},{"location":"parameter_estimation/#Using-an-optimizer-1","page":"Parameter estimation","title":"Using an optimizer","text":"","category":"section"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"Maximum-likelihood or prediction-error estimation is straight-forward by simply differentiating through the state estimator using automatic differentiation. In this example, we will continue the example from above, but now estimate all the parameters of the quad-tank process. This time, they will not vary with time.","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"We now define the dynamics function such that it takes its parameters from the p input argument. We also define a variable p_true that contains the true values that we will use to simulate some estimation data","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"function quadtank(h, u, p, t)\n    kc = p[1]\n    k1, k2, g = p[2], p[3], 9.81\n    A1 = A3 = A2 = A4 = p[4]\n    a1 = a3 = a2 = a4 = p[5]\n    γ1 = γ2 = p[6]\n\n    ssqrt(x) = √(max(x, zero(x)) + 1e-3) # For numerical robustness at x = 0\n    \n    xd = SA[\n        -a1/A1 * ssqrt(2g*h[1]) + a3/A1*ssqrt(2g*h[3]) +     γ1*k1/A1 * u[1]\n        -a2/A2 * ssqrt(2g*h[2]) + a4/A2*ssqrt(2g*h[4]) +     γ2*k2/A2 * u[2]\n        -a3/A3*ssqrt(2g*h[3])                          + (1-γ2)*k2/A3 * u[2]\n        -a4/A4*ssqrt(2g*h[4])                          + (1-γ1)*k1/A4 * u[1]\n    ]\nend\n\ndiscrete_dynamics = LowLevelParticleFilters.rk4(quadtank, Ts, supersample=2)\np_true = [0.5, 1.6, 1.6, 4.9, 0.03, 0.2]\nnothing # hide","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"Similar to previous example, we simulate the system, this time using a more exciting input in order to be able to identify several parameters","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"Tperiod = 200\nt = 0:Ts:1000\nu1 = vcat.(0.25 .* sign.(sin.(2pi/Tperiod .* (t ./ 40).^2)) .+ 0.25)\nu2 = vcat.(0.25 .* sign.(sin.(2pi/Tperiod .* (t ./ 40).^2 .+ pi/2)) .+ 0.25)\nu  = vcat.(u1,u2)\nx0 = Float64[2,2,3,3]\nx = LowLevelParticleFilters.rollout(discrete_dynamics, x0, u, p_true)[1:end-1]\ny = measurement.(x, u, 0, 0)\ny = [y .+ 0.01randn(ny) for y in y]\n\nplot(\n    plot(reduce(hcat, x)', title=\"States\"),\n    plot(reduce(hcat, u)', title=\"Inputs\")\n)","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"This time, we define a cost function for the optimizer to optimize, we'll use the sum of squared errors (sse). It's important to define the UKF with an initial state distribution with the same element type as the parameter vector so that automatic differentiation through the state estimator works, hence the explicit casting T.(x0).","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"nx = 4\nR1 = Diagonal([0.1, 0.1, 0.1, 0.1])\nR2 = Diagonal((1e-2)^2 * ones(ny))\nx0 = [2, 2, 3, 3]\n\nfunction cost(p::Vector{T}) where T\n    kf = UnscentedKalmanFilter(discrete_dynamics, measurement, R1, R2, MvNormal(T.(x0), R1); ny, nu)\n    LowLevelParticleFilters.sse(kf, u, y, p)\nend\nnothing # hide","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"We generate a random initial guess for the estimation problem","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"p_guess = p_true .+  0.1*p_true .* randn(length(p_true))","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"and solve it using Optim","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"using Optim\nres = Optim.optimize(\n    cost,\n    p_guess,\n    BFGS(),\n    Optim.Options(\n        show_trace = true,\n        show_every = 5,\n        iterations = 100,\n        time_limit = 30,\n    ),\n    autodiff = :forward,\n)","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"We started out with a normalized parameter error of","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"using LinearAlgebra\nnorm(p_true - p_guess) / norm(p_true)","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"and ended with","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"p_opt = res.minimizer\nnorm(p_true - p_opt) / norm(p_true)","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"There is no guarantee that we will recover the true parameters for this system, especially not if the input excitation is poor, but we will generally find parameters that results in a good predictor for the system (this is after all what we're optimizing for). A tool like StructuralIdentifiability.jl may be used to determine the identifiability of parameters and states, something that for this system could look like","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"using StructuralIdentifiability\n\node = @ODEmodel(\n    h1'(t) = -a1/A1 * h1(t) + a3/A1*h3(t) +     gam*k1/A1 * u1(t),\n    h2'(t) = -a2/A2 * h2(t) + a4/A2*h4(t) +     gam*k2/A2 * u2(t),\n    h3'(t) = -a3/A3*h3(t)                          + (1-gam)*k2/A3 * u2(t),\n    h4'(t) = -a4/A4*h4(t)                          + (1-gam)*k1/A4 * u1(t),\n\ty1(t) = h1(t),\n    y2(t) = h2(t),\n)\n\nlocal_id = assess_local_identifiability(ode, 0.99)","category":"page"},{"location":"parameter_estimation/#","page":"Parameter estimation","title":"Parameter estimation","text":"where we have made the substitution sqrt h rightarrow h due to a limitation of the tool. The output of the above analysis is  julia julia> local_id = assess_local_identifiability(ode, 0.99) Dict{Nemo.fmpq_mpoly, Bool} with 15 entries:   a3  => 0   gam => 1   k2  => 0   A4  => 0   h4  => 0   h2  => 1   A3  => 0   a1  => 0   A2  => 0   k1  => 0   a4  => 0   h3  => 0   h1  => 1   A1  => 0   a2  => 0   indicating that we can not hope to resolve all of the parameters. However, using appropriate regularization from prior information, we might still recover a lot of information about the system. Regularization could easily be added to the function cost above, e.g., using a penalty like (p-p_guess)'Γ*(p-p_guess) for some matrix Gamma, to indicate our confidence in the initial guess.","category":"page"},{"location":"distributions/#High-performance-Distributions-1","page":"High-performance distributions","title":"High performance Distributions","text":"","category":"section"},{"location":"distributions/#","page":"High-performance distributions","title":"High-performance distributions","text":"When using LowLevelParticleFilters, a number of methods related to distributions are defined for static arrays, making logpdf etc. faster. We also provide a new kind of distribution: TupleProduct <: MultivariateDistribution that behaves similarly to the Product distribution. The TupleProduct however stores the individual distributions in a tuple, has compile-time known length and supports Mixed <: ValueSupport, meaning that it can be a product of both Continuous and Discrete dimensions, somthing not supported by the standard Product. Example","category":"page"},{"location":"distributions/#","page":"High-performance distributions","title":"High-performance distributions","text":"using BenchmarkTools, LowLevelParticleFilters, Distributions\ndt = TupleProduct((Normal(0,2), Normal(0,2), Binomial())) # Mixed value support","category":"page"},{"location":"distributions/#","page":"High-performance distributions","title":"High-performance distributions","text":"A small benchmark","category":"page"},{"location":"distributions/#","page":"High-performance distributions","title":"High-performance distributions","text":"sv = @SVector randn(2)\nd = Product([Normal(0,2), Normal(0,2)])\ndt = TupleProduct((Normal(0,2), Normal(0,2)))\ndm = MvNormal(2, 2)\n@btime logpdf($d,$(Vector(sv))) # 32.449 ns (1 allocation: 32 bytes)\n@btime logpdf($dt,$(Vector(sv))) # 21.141 ns (0 allocations: 0 bytes)\n@btime logpdf($dm,$(Vector(sv))) # 48.745 ns (1 allocation: 96 bytes)","category":"page"},{"location":"distributions/#","page":"High-performance distributions","title":"High-performance distributions","text":"@btime logpdf($d,$sv) # 22.651 ns (0 allocations: 0 bytes)\n@btime logpdf($dt,$sv) # 0.021 ns (0 allocations: 0 bytes)\n@btime logpdf($dm,$sv) # 0.021 ns (0 allocations: 0 bytes)","category":"page"},{"location":"distributions/#","page":"High-performance distributions","title":"High-performance distributions","text":"Without loading LowLevelParticleFilters, the timing for the native distributions are the following","category":"page"},{"location":"distributions/#","page":"High-performance distributions","title":"High-performance distributions","text":"@btime logpdf($d,$sv) # 32.621 ns (1 allocation: 32 bytes)\n@btime logpdf($dm,$sv) # 46.415 ns (1 allocation: 96 bytes)","category":"page"},{"location":"benchmark/#Benchmark-test-1","page":"Benchmark","title":"Benchmark test","text":"","category":"section"},{"location":"benchmark/#","page":"Benchmark","title":"Benchmark","text":"To see how the performance varies with the number of particles, we simulate several times. The following code simulates the system and performs filtering using the simulated measurements. We do this for varying number of time steps and varying number of particles.","category":"page"},{"location":"benchmark/#","page":"Benchmark","title":"Benchmark","text":"note: Note\nTo run this code, see the bottom of src/example_lineargaussian.jl.","category":"page"},{"location":"benchmark/#","page":"Benchmark","title":"Benchmark","text":"function run_test()\n    particle_count = [10, 20, 50, 100, 200, 500, 1000]\n    time_steps = [20, 100, 200]\n    RMSE = zeros(length(particle_count),length(time_steps)) # Store the RMS errors\n    propagated_particles = 0\n    t = @elapsed for (Ti,T) = enumerate(time_steps)\n        for (Ni,N) = enumerate(particle_count)\n            montecarlo_runs = 2*maximum(particle_count)*maximum(time_steps) ÷ T ÷ N\n            E = sum(1:montecarlo_runs) do mc_run\n                pf = ParticleFilter(N, dynamics, measurement, df, dg, d0) # Create filter\n                u = @SVector randn(2)\n                x = SVector{2,Float64}(rand(rng, d0))\n                y = SVector{2,Float64}(sample_measurement(pf,x,u,0,1))\n                error = 0.0\n                @inbounds for t = 1:T-1\n                    pf(u, y) # Update the particle filter\n                    x = dynamics(x,u,t) + SVector{2,Float64}(rand(rng, df)) # Simulate the true dynamics and add some noise\n                    y = SVector{2,Float64}(sample_measurement(pf,x,u,0,t)) # Simulate a measuerment\n                    u = @SVector randn(2) # draw a random control input\n                    error += sum(abs2,x-weigthed_mean(pf))\n                end # t\n                √(error/T)\n            end # MC\n            RMSE[Ni,Ti] = E/montecarlo_runs\n            propagated_particles += montecarlo_runs*N*T\n            @show N\n        end # N\n        @show T\n    end # T\n    println(\"Propagated $propagated_particles particles in $t seconds for an average of $(propagated_particles/t/1000) particles per millisecond\")\n    return RMSE\nend\n\n@time RMSE = run_test()","category":"page"},{"location":"benchmark/#","page":"Benchmark","title":"Benchmark","text":"Propagated 8400000 particles in 1.140468043 seconds for an average of 7365.397085484139 particles per millisecond","category":"page"},{"location":"benchmark/#","page":"Benchmark","title":"Benchmark","text":"We then plot the results","category":"page"},{"location":"benchmark/#","page":"Benchmark","title":"Benchmark","text":"time_steps     = [20, 100, 200]\nparticle_count = [10, 20, 50, 100, 200, 500, 1000]\nnT             = length(time_steps)\nleg            = reshape([\"$(time_steps[i]) time steps\" for i = 1:nT], 1,:)\nplot(particle_count,RMSE,xscale=:log10, ylabel=\"RMS errors\", xlabel=\" Number of particles\", lab=leg)","category":"page"},{"location":"benchmark/#","page":"Benchmark","title":"Benchmark","text":"(Image: window)","category":"page"},{"location":"#LowLevelParticleFilters-1","page":"Home","title":"LowLevelParticleFilters","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"(Image: CI) (Image: codecov)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"This is a library for state estimation, that is, given measurements y(t) from a dynamical system, estimate the state vector x(t). Throughout, we assume dynamics on the form","category":"page"},{"location":"#","page":"Home","title":"Home","text":"x(t+1) = f(x(t) u(t) p t w(t))\ny(t) = g(x(t) u(t) p t e(t))","category":"page"},{"location":"#","page":"Home","title":"Home","text":"where x is the state vector, u an input, p some form of parameters, t is the time and we are disturbances (noise). Throughout the documentation, we often call the function f dynamics and the function g measurement.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The dynamics above describe a discrete-time system, i.e., the function f takes the current state and produces the next state. This is in contrast to a continuous-time system, where f takes the current state but produces the time derivative of the state. A continuous-time system can be discretized, described in detail in Discretization.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The parameters p can be anything, or left out. You may write the dynamics functions such that they depend on p and include parameters when you create a filter object. You may also override the parameters stored in the filter object when you call any function on the filter object. This behavior is modeled after the SciML ecosystem.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Depending on the nature of f and g, the best method of estimating the state may vary. If fg are linear and the disturbances are additive and Gaussian, the KalmanFilter is an optimal state estimator. If any of the above assumptions fail to hold, we may need to resort to more advanced estimators. This package provides several filter types, outlined below.","category":"page"},{"location":"#Estimator-types-1","page":"Home","title":"Estimator types","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"We provide a number of filter types","category":"page"},{"location":"#","page":"Home","title":"Home","text":"KalmanFilter. A standard Kalman filter. Is restricted to linear dynamics (possibly time varying) and Gaussian noise.\nExtendedKalmanFilter: For nonlinear systems, the EKF runs a regular Kalman filter on linearized dynamics. Uses ForwardDiff.jl for linearization. The noise model must be Gaussian.\nUnscentedKalmanFilter: The Unscented Kalman filter often performs slightly better than the Extended Kalman filter but may be slightly more computationally expensive. The UKF handles nonlinear dynamics and measurement model, but still requires an additive Gaussian noise model and assumes all posterior distributions are Gaussian, i.e., can not handle multi-modal posteriors.\nParticleFilter: The particle filter is a nonlinear estimator. This filter is simple to use and assumes that both dynamics noise and measurement noise are additive. Particle filters handle multi-modal posteriors.\nAuxiliaryParticleFilter: This filter is identical to ParticleFilter, but uses a slightly different proposal mechanism for new particles.\nAdvancedParticleFilter: This filter gives you more flexibility, at the expense of having to define a few more functions. This filter does not require the noise to be additive and is thus the most flexible filter type.\nDAEUnscentedKalmanFilter: An Unscented Kalman filter for differential-algebraic systems (DAE).","category":"page"},{"location":"#Functionality-1","page":"Home","title":"Functionality","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"This package provides ","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Filtering, estimating x(t) given measurements up to and including time t. We call the filtered estimate x(tt) (read as x at t given t).\nSmoothing, estimating x(t) given data up to T  t, i.e., x(tT).\nParameter estimation.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"All filters work in two distinct steps.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The prediction step (predict!). During prediction, we use the dynamics model to form x(tt-1) = f(x(t-1) )\nThe correction step (correct!). In this step, we adjust the predicted state x(tt-1) using the measurement y(t) to form x(tt).","category":"page"},{"location":"#","page":"Home","title":"Home","text":"In general, all filters represent not only a point estimate of x(t), but a representation of the complete posterior probability distribution over x given all the data available up to time t. One major difference between different filter types is how they represent these probability distributions.","category":"page"},{"location":"#Particle-filter-1","page":"Home","title":"Particle filter","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"A particle filter represents the probability distribution over the state as a collection of samples, each sample is propagated through the dynamics function f individually. When a measurement becomes available, the samples, called particles, are given a weight based on how likely the particle is given the measurement. Each particle can thus be seen as representing a hypothesis about the current state of the system. After a few time steps, most weights are inevitably going to be extremely small, a manifestation of the curse of dimensionality, and a resampling step is incorporated to refresh the particle distribution and focus the particles on areas of the state space with high posterior probability.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Defining a particle filter is straightforward, one must define the distribution of the noise df in the dynamics function, dynamics(x,u,p,t) and the noise distribution dg in the measurement function measurement(x,u,p,t). Both of these noise sources are assumed to be additive, but can have any distribution. The distribution of the initial state d0 must also be provided. An example for a linear Gaussian system is given below.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"using LowLevelParticleFilters, LinearAlgebra, StaticArrays, Distributions, Plots","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Define problem","category":"page"},{"location":"#","page":"Home","title":"Home","text":"n = 2   # Dimension of state\nm = 1   # Dimension of input\np = 1   # Dimension of measurements\nN = 500 # Number of particles\n\nconst dg = MvNormal(p,1.0)          # Measurement noise Distribution\nconst df = MvNormal(n,1.0)          # Dynamics noise Distribution\nconst d0 = MvNormal(randn(n),2.0)   # Initial state Distribution\nnothing # hide","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Define linear state-space system (using StaticArrays for maximum performance)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"const A = SA[0.97043   -0.097368\n             0.09736    0.970437]\nconst B = SA[0.1; 0;;]\nconst C = SA[0 1.0]\nnothing # hide","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Next, we define the dynamics and measurement equations, they both take the signature (x,u,p,t) = (state, input, parameters, time) ","category":"page"},{"location":"#","page":"Home","title":"Home","text":"dynamics(x,u,p,t) = A*x .+ B*u\nmeasurement(x,u,p,t) = C*x\nvecvec_to_mat(x) = copy(reduce(hcat, x)') # Helper function\nnothing # hide","category":"page"},{"location":"#","page":"Home","title":"Home","text":"the parameter p can be anything, and is often optional. If p is not provided when performing operations on filters, any p stored in the filter objects (if supported) is used. The default if none is provided and none is stored in the filter is p = SciMLBase.NullParameters().","category":"page"},{"location":"#","page":"Home","title":"Home","text":"We are now ready to define and use a filter","category":"page"},{"location":"#","page":"Home","title":"Home","text":"pf = ParticleFilter(N, dynamics, measurement, df, dg, d0)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"With the filter in hand, we can simulate from its dynamics and query some properties","category":"page"},{"location":"#","page":"Home","title":"Home","text":"du = MvNormal(m,1.0)         # Random input distribution for simulation\nxs,u,y = simulate(pf,200,du) # We can simulate the model that the pf represents\npf(u[1], y[1])               # Perform one filtering step using input u and measurement y\nparticles(pf)                # Query the filter for particles, try weights(pf) or expweights(pf) as well\nx̂ = weigthed_mean(pf)        # using the current state","category":"page"},{"location":"#","page":"Home","title":"Home","text":"If you want to perform filtering using vectors of inputs and measurements, try any of the functions","category":"page"},{"location":"#","page":"Home","title":"Home","text":"sol = forward_trajectory(pf, u, y) # Filter whole vectors of signals\nx̂,ll = mean_trajectory(pf, u, y)\nplot(sol, xreal=xs, markersize=2)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"u ad y are then assumed to be vectors of vectors. StaticArrays is recommended for maximum performance.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"If MonteCarloMeasurements.jl is loaded, you may transform the output particles to Matrix{MonteCarloMeasurements.Particles} with the layout T × n_states using Particles(x,we). Internally, the particles are then resampled such that they all have unit weight. This is conventient for making use of the plotting facilities of MonteCarloMeasurements.jl.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"For a full usage example, see the benchmark section below or example_lineargaussian.jl","category":"page"},{"location":"#Resampling-1","page":"Home","title":"Resampling","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"The particle filter will perform a resampling step whenever the distribution of the weights has become degenerate. The resampling is triggered when the effective number of samples is smaller than pf.resample_threshold in 0 1, this value can be set when constructing the filter. How the resampling is done is governed by pf.resampling_strategy, we currently provide ResampleSystematic <: ResamplingStrategy as the only implemented strategy. See https://en.wikipedia.org/wiki/Particle_filter for more info.","category":"page"},{"location":"#Particle-Smoothing-1","page":"Home","title":"Particle Smoothing","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Smoothing is the process of finding the best state estimate given both past and future data. Smoothing is thus only possible in an offline setting. This package provides a particle smoother, based on forward filtering, backward simulation (FFBS), example usage follows:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"N     = 2000 # Number of particles\nT     = 80   # Number of time steps\nM     = 100  # Number of smoothed backwards trajectories\npf    = ParticleFilter(N, dynamics, measurement, df, dg, d0)\ndu    = MvNormal(m,1)     # Control input distribution\nx,u,y = simulate(pf,T,du) # Simulate trajectory using the model in the filter\ntosvec(y) = reinterpret(SVector{length(y[1]),Float64}, reduce(hcat,y))[:] |> copy\nx,u,y = tosvec.((x,u,y))\n\nxb,ll = smooth(pf, M, u, y) # Sample smooting particles\nxbm   = smoothed_mean(xb)   # Calculate the mean of smoothing trajectories\nxbc   = smoothed_cov(xb)    # And covariance\nxbt   = smoothed_trajs(xb)  # Get smoothing trajectories\nxbs   = [diag(xbc) for xbc in xbc] |> vecvec_to_mat .|> sqrt\nplot(xbm', ribbon=2xbs, lab=\"PF smooth\")\nplot!(vecvec_to_mat(x), l=:dash, lab=\"True\")","category":"page"},{"location":"#","page":"Home","title":"Home","text":"We can plot the particles themselves as well","category":"page"},{"location":"#","page":"Home","title":"Home","text":"downsample = 5\nplot(vecvec_to_mat(x), l=(4,), layout=(2,1), show=false)\nscatter!(xbt[1, 1:downsample:end, :]', subplot=1, show=false, m=(1,:black, 0.5), lab=\"\")\nscatter!(xbt[2, 1:downsample:end, :]', subplot=2, m=(1,:black, 0.5), lab=\"\")","category":"page"},{"location":"#Kalman-filter-1","page":"Home","title":"Kalman filter","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"The KalmanFilter (wiki) assumes that f and g are linear functions, i.e., that they can be written on the form","category":"page"},{"location":"#","page":"Home","title":"Home","text":"x(t+1) = Ax(t) + Bu(t) + w(t)\ny(t) = Cx(t) + Du(t) + e(t)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"for some matrices ABCD where w sim N(0 R_1) and e sim N(0 R_2) are zero mean and Gaussian. The Kalman filter represents the posterior distributions over x by the mean and a covariance matrix. The magic behind the Kalman filter is that linear transformations of Gaussian distributions remain Gaussian, and we thus have a very efficient way of representing them.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"A Kalman filter is easily created using the constructor. Many of the functions defined for particle filters, are defined also for Kalman filters, e.g.:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"R1 = cov(df)\nR2 = cov(dg)\nkf = KalmanFilter(A, B, C, 0, R1, R2, d0)\nsol = forward_trajectory(kf, u, y) # filtered, prediction, pred cov, filter cov, loglik\nnothing # hide","category":"page"},{"location":"#","page":"Home","title":"Home","text":"It can also be called in a loop like the pf above","category":"page"},{"location":"#","page":"Home","title":"Home","text":"for t = 1:T\n    kf(u,y) # Performs both correct and predict!!\n    # alternatively\n    ll, e = correct!(kf, y, nothing, t) # Returns loglikelihood and prediction error\n    x     = state(kf)\n    R     = covariance(kf)\n    predict!(kf, u, nothing, t)\nend","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The matrices in the Kalman filter may be time varying, such that A[:, :, t] is A(t). They may also be provided as functions on the form A(t) = A(x u p t). This works for both dynamics and covariance matrices.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The numeric type used in the Kalman filter is determined from the mean of the initial state distribution, so make sure that this has the correct type if you intend to use, e.g., Float32 or ForwardDiff.Dual for automatic differentiation.","category":"page"},{"location":"#Smoothing-using-KF-1","page":"Home","title":"Smoothing using KF","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Kalman filters can also be used for smoothing ","category":"page"},{"location":"#","page":"Home","title":"Home","text":"kf = KalmanFilter(A, B, C, 0, cov(df), cov(dg), d0)\nxT,R,lls = smooth(kf, u, y, p) # Smoothed state, smoothed cov, loglik\nnothing # hide","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Plot and compare PF and KF","category":"page"},{"location":"#","page":"Home","title":"Home","text":"plot(vecvec_to_mat(xT), lab=\"Kalman smooth\", layout=2)\nplot!(xbm', lab=\"pf smooth\")\nplot!(vecvec_to_mat(x), lab=\"true\")","category":"page"},{"location":"#Unscented-Kalman-Filter-1","page":"Home","title":"Unscented Kalman Filter","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"The UnscentedKalmanFilter represents posterior distributions over x as Gaussian distributions, but propagate them through a nonlinear function f by a deterministic sampling of a small number of particles called sigma points (this is referred to as the unscented transform). This UKF thus handles nonlinear functions fg, but only Gaussian disturbances and unimodal posteriors.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The UKF takes the same arguments as a regular KalmanFilter, but the matrices defining the dynamics are replaced by two functions, dynamics and measurement, working in the same way as for the ParticleFilter above.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"ukf = UnscentedKalmanFilter(dynamics, measurement, cov(df), cov(dg), MvNormal([1.,1.]), nu=m, ny=p)","category":"page"},{"location":"#UKF-for-DAE-systems-1","page":"Home","title":"UKF for DAE systems","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"See the docstring for DAEUnscentedKalmanFilter or the test file. This filter is modeled after","category":"page"},{"location":"#","page":"Home","title":"Home","text":"\"Nonlinear State Estimation of Differential Algebraic Systems\" Ravi K. Mandela, Raghunathan Rengaswamy, Shankar Narasimhan","category":"page"},{"location":"#","page":"Home","title":"Home","text":"warning: Warning\nThis filter is still considered experimental and subject to change without respecting semantic versioning. Use at your own risk.","category":"page"},{"location":"#Extended-Kalman-Filter-1","page":"Home","title":"Extended Kalman Filter","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"The ExtendedKalmanFilter (EKF)is similar to the UKF, ubt propagates Gaussian distributions by linearizing the dynamics and using the formulas for linear systems similar to the standard Kalman filter. This can be slightly faster than the UKF (not always), but also less accurate for strongly nonlinear systems. The linearization is performed automatically using ForwardDiff.jl. In general, the UKF is recommended over the EKF unless the EKF is faster and computational performance is the top priority.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The EKF has the following signature","category":"page"},{"location":"#","page":"Home","title":"Home","text":"ExtendedKalmanFilter(kf, dynamics, measurement)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"where kf is a standard KalmanFilter from which the covariance properties are taken.","category":"page"},{"location":"#AdvancedParticleFilter-1","page":"Home","title":"AdvancedParticleFilter","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"The AdvancedParticleFilter works very much like the ParticleFilter, but admits more flexibility in its noise models.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The AdvancedParticleFilter type requires you to implement the same functions as the regular ParticleFilter, but in this case you also need to handle sampling from the noise distributions yourself. The function dynamics must have a method signature like below. It must provide one method that accepts state vector, control vector, parameter, time and noise::Bool that indicates whether or not to add noise to the state. If noise should be added, this should be done inside dynamics An example is given below","category":"page"},{"location":"#","page":"Home","title":"Home","text":"using Random\nconst rng = Random.Xoshiro()\nfunction dynamics(x, u, p, t, noise=false) # It's important that this defaults to false\n    x = A*x .+ B*u # A simple dynamics model\n    if noise\n        x += rand(rng, df) # it's faster to supply your own rng\n    end\n    x\nend\nnothing # hide","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The measurement_likelihood function must have a method accepting state, measurement, parameter and time, and returning the log-likelihood of the measurement given the state, a simple example below:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"function measurement_likelihood(x, u, y, p, t)\n    logpdf(dg, C*x-y) # A simple linear measurement model with normal additive noise\nend\nnothing # hide","category":"page"},{"location":"#","page":"Home","title":"Home","text":"This gives you very high flexibility. The noise model in either function can, for instance, be a function of the state, something that is not possible for the simple ParticleFilter. To be able to simulate the AdvancedParticleFilter like we did with the simple filter above, the measurement method with the signature measurement(x,u,p,t,noise=false) must be available and return a sample measurement given state (and possibly time). For our example measurement model above, this would look like this","category":"page"},{"location":"#","page":"Home","title":"Home","text":"measurement(x, u, p, t, noise=false) = C*x + noise*rand(rng, dg)\nnothing # hide","category":"page"},{"location":"#","page":"Home","title":"Home","text":"We now create the AdvancedParticleFilter and use it in the same way as the other filters:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"apf = AdvancedParticleFilter(N, dynamics, measurement, measurement_likelihood, df, d0)\nsol = forward_trajectory(apf, u, y, p)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"plot(sol, xreal=x)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"We can even use this type as an AuxiliaryParticleFilter","category":"page"},{"location":"#","page":"Home","title":"Home","text":"apfa = AuxiliaryParticleFilter(apf)\nsol = forward_trajectory(apfa, u, y, p)\nplot(sol, dim=1, xreal=x) # Same as above, but only plots a single dimension","category":"page"},{"location":"#Troubleshooting-and-tuning-1","page":"Home","title":"Troubleshooting and tuning","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Tuning a particle filter can be quite the challenge. To assist with this, we provide som visualization tools","category":"page"},{"location":"#","page":"Home","title":"Home","text":"debugplot(pf,u[1:20],y[1:20], runall=true, xreal=x[1:20])","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The plot displays all states and all measurements. The heatmap in the background represents the weighted particle distributions per time step. For the measurement sequences, the heatmap represent the distributions of predicted measurements. The blue dots corresponds to measured values. In this case, we simulated the data and we had access to states as well, if we do not have that, just omit xreal. You can also manually step through the time-series using","category":"page"},{"location":"#","page":"Home","title":"Home","text":"commandplot(pf,u,y; kwargs...)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"For options to the debug plots, see ?pplot.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Something seems to be off with this figure as the hottest spot is not really where we would expect it","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Optimization of the log likelihood can be done by, e.g., global/black box methods, see BlackBoxOptim.jl, see examples in Parameter Estimation. Standard tricks apply, such as performing the parameter search in log-space etc.","category":"page"},{"location":"#Discretization-1","page":"Home","title":"Discretization","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Continuous-time dynamics functions on the form (x,u,p,t) -> ẋ can be discretized (integrated) using the function LowLevelParticleFilters.rk4, e.g.,","category":"page"},{"location":"#","page":"Home","title":"Home","text":"discrete_dynamics = LowLevelParticleFilters.rk4(continuous_dynamics, sampletime; supersample=1)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"where the integer supersample determines the number of RK4 steps that is taken internally for each change of the control signal (1 is often sufficient and is the default). The returned function discrete_dynamics is on the form (x,u,p,t) -> x⁺.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"When solving state-estimation problems, accurate integration is often less important than during simulation. The motivations for this are several","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The dynamics model is often inaccurate, and solving an inaccurate model to high accuracy can be a waste of effort.\nThe performance is often dictated by the disturbances acting on the system.\nState-estimation enjoys feedback from measurements that corrects for slight errors due to integration.","category":"page"}]
}
